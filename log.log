nohup: ignoring input
2018-04-09 09:29:22.952820: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
2018-04-09 09:29:29.338564: step 0, loss = 4.66 (197.5 examples/sec; 0.648 sec/batch)
2018-04-09 09:29:39.586761: step 10, loss = 4.09 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 09:29:49.760298: step 20, loss = 3.82 (125.8 examples/sec; 1.017 sec/batch)
2018-04-09 09:29:59.945171: step 30, loss = 3.86 (125.7 examples/sec; 1.018 sec/batch)
2018-04-09 09:30:10.218714: step 40, loss = 3.94 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 09:30:20.405911: step 50, loss = 3.82 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 09:30:30.631666: step 60, loss = 3.94 (125.2 examples/sec; 1.023 sec/batch)
2018-04-09 09:30:40.823003: step 70, loss = 3.75 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 09:30:51.003041: step 80, loss = 3.79 (125.7 examples/sec; 1.018 sec/batch)
2018-04-09 09:31:01.244998: step 90, loss = 3.73 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 09:31:11.815860: step 100, loss = 3.69 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 09:31:22.013172: step 110, loss = 3.75 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 09:31:32.190090: step 120, loss = 3.67 (125.8 examples/sec; 1.018 sec/batch)
2018-04-09 09:31:42.345355: step 130, loss = 3.73 (126.0 examples/sec; 1.016 sec/batch)
2018-04-09 09:31:52.466188: step 140, loss = 3.43 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 09:32:02.659737: step 150, loss = 3.29 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 09:32:12.777078: step 160, loss = 3.37 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 09:32:22.942371: step 170, loss = 3.52 (125.9 examples/sec; 1.017 sec/batch)
2018-04-09 09:32:33.055970: step 180, loss = 3.50 (126.6 examples/sec; 1.011 sec/batch)
2018-04-09 09:32:43.175129: step 190, loss = 3.46 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 09:32:53.642228: step 200, loss = 3.18 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 09:33:03.785754: step 210, loss = 3.29 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:33:13.947721: step 220, loss = 2.98 (126.0 examples/sec; 1.016 sec/batch)
2018-04-09 09:33:24.113758: step 230, loss = 3.15 (125.9 examples/sec; 1.017 sec/batch)
2018-04-09 09:33:34.263041: step 240, loss = 3.08 (126.1 examples/sec; 1.015 sec/batch)
2018-04-09 09:33:44.388084: step 250, loss = 3.13 (126.4 examples/sec; 1.013 sec/batch)
2018-04-09 09:33:54.534142: step 260, loss = 3.47 (126.2 examples/sec; 1.015 sec/batch)
2018-04-09 09:34:04.688089: step 270, loss = 3.18 (126.1 examples/sec; 1.015 sec/batch)
2018-04-09 09:34:14.829200: step 280, loss = 2.98 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:34:24.906903: step 290, loss = 3.23 (127.0 examples/sec; 1.008 sec/batch)
2018-04-09 09:34:35.354254: step 300, loss = 2.88 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 09:34:45.484545: step 310, loss = 3.14 (126.4 examples/sec; 1.013 sec/batch)
2018-04-09 09:34:55.582039: step 320, loss = 3.00 (126.8 examples/sec; 1.010 sec/batch)
2018-04-09 09:35:05.766078: step 330, loss = 2.99 (125.7 examples/sec; 1.018 sec/batch)
2018-04-09 09:35:16.007800: step 340, loss = 2.97 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 09:35:26.098187: step 350, loss = 2.80 (126.9 examples/sec; 1.009 sec/batch)
2018-04-09 09:35:36.233720: step 360, loss = 2.98 (126.3 examples/sec; 1.014 sec/batch)
2018-04-09 09:35:46.315078: step 370, loss = 2.76 (127.0 examples/sec; 1.008 sec/batch)
2018-04-09 09:35:56.415061: step 380, loss = 3.02 (126.7 examples/sec; 1.010 sec/batch)
2018-04-09 09:36:06.592636: step 390, loss = 2.78 (125.8 examples/sec; 1.018 sec/batch)
2018-04-09 09:36:17.033203: step 400, loss = 2.80 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 09:36:27.101380: step 410, loss = 2.70 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 09:36:37.245322: step 420, loss = 2.72 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:36:47.357644: step 430, loss = 2.89 (126.6 examples/sec; 1.011 sec/batch)
2018-04-09 09:36:57.412601: step 440, loss = 2.98 (127.3 examples/sec; 1.005 sec/batch)
2018-04-09 09:37:07.555094: step 450, loss = 2.89 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:37:17.698465: step 460, loss = 2.58 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:37:27.887904: step 470, loss = 2.65 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 09:37:38.029738: step 480, loss = 2.49 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:37:48.152465: step 490, loss = 2.50 (126.4 examples/sec; 1.012 sec/batch)
2018-04-09 09:37:58.581606: step 500, loss = 2.51 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 09:38:08.749800: step 510, loss = 2.67 (125.9 examples/sec; 1.017 sec/batch)
2018-04-09 09:38:18.910382: step 520, loss = 2.85 (126.0 examples/sec; 1.016 sec/batch)
2018-04-09 09:38:29.027128: step 530, loss = 2.44 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 09:38:39.189214: step 540, loss = 2.49 (126.0 examples/sec; 1.016 sec/batch)
2018-04-09 09:38:49.326835: step 550, loss = 2.69 (126.3 examples/sec; 1.014 sec/batch)
2018-04-09 09:38:59.466711: step 560, loss = 2.51 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 09:39:09.754629: step 570, loss = 2.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 09:39:19.930630: step 580, loss = 2.57 (125.8 examples/sec; 1.018 sec/batch)
2018-04-09 09:39:30.189258: step 590, loss = 2.55 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 09:39:40.625230: step 600, loss = 2.35 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 09:39:50.731270: step 610, loss = 2.23 (126.7 examples/sec; 1.011 sec/batch)
2018-04-09 09:40:00.863128: step 620, loss = 2.52 (126.3 examples/sec; 1.013 sec/batch)
2018-04-09 09:40:11.070337: step 630, loss = 2.34 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 09:40:21.198803: step 640, loss = 2.35 (126.4 examples/sec; 1.013 sec/batch)
2018-04-09 09:40:31.283545: step 650, loss = 2.39 (126.9 examples/sec; 1.008 sec/batch)
2018-04-09 09:40:41.404997: step 660, loss = 2.62 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 09:40:51.458094: step 670, loss = 2.25 (127.3 examples/sec; 1.005 sec/batch)
2018-04-09 09:41:01.593028: step 680, loss = 2.33 (126.3 examples/sec; 1.013 sec/batch)
2018-04-09 09:41:11.679256: step 690, loss = 2.48 (126.9 examples/sec; 1.009 sec/batch)
2018-04-09 09:41:22.009990: step 700, loss = 2.22 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 09:41:32.052714: step 710, loss = 2.29 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 09:41:42.164267: step 720, loss = 2.10 (126.6 examples/sec; 1.011 sec/batch)
2018-04-09 09:41:52.220632: step 730, loss = 2.46 (127.3 examples/sec; 1.006 sec/batch)
2018-04-09 09:42:02.291810: step 740, loss = 2.36 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 09:42:12.397307: step 750, loss = 2.45 (126.7 examples/sec; 1.011 sec/batch)
2018-04-09 09:42:22.430519: step 760, loss = 2.19 (127.6 examples/sec; 1.003 sec/batch)
2018-04-09 09:42:32.459553: step 770, loss = 2.10 (127.6 examples/sec; 1.003 sec/batch)
2018-04-09 09:42:42.495619: step 780, loss = 2.24 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 09:42:52.476517: step 790, loss = 2.07 (128.2 examples/sec; 0.998 sec/batch)
2018-04-09 09:43:02.911931: step 800, loss = 2.10 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 09:43:12.980571: step 810, loss = 2.16 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 09:43:23.004709: step 820, loss = 2.21 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 09:43:33.044061: step 830, loss = 2.03 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 09:43:43.088183: step 840, loss = 1.90 (127.4 examples/sec; 1.004 sec/batch)
2018-04-09 09:43:53.117698: step 850, loss = 1.98 (127.6 examples/sec; 1.003 sec/batch)
2018-04-09 09:44:03.216813: step 860, loss = 1.95 (126.7 examples/sec; 1.010 sec/batch)
2018-04-09 09:44:13.287547: step 870, loss = 2.03 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 09:44:23.311292: step 880, loss = 1.80 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 09:44:33.365996: step 890, loss = 2.08 (127.3 examples/sec; 1.005 sec/batch)
2018-04-09 09:44:43.761010: step 900, loss = 2.04 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 09:44:53.733244: step 910, loss = 1.87 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 09:45:03.789028: step 920, loss = 2.17 (127.3 examples/sec; 1.006 sec/batch)
2018-04-09 09:45:13.813973: step 930, loss = 2.00 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 09:45:23.863697: step 940, loss = 2.02 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 09:45:33.886997: step 950, loss = 2.03 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 09:45:43.977668: step 960, loss = 1.80 (126.8 examples/sec; 1.009 sec/batch)
2018-04-09 09:45:53.964026: step 970, loss = 1.76 (128.2 examples/sec; 0.999 sec/batch)
2018-04-09 09:46:04.012542: step 980, loss = 1.79 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 09:46:14.033926: step 990, loss = 2.09 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 09:46:24.313039: step 1000, loss = 1.89 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 09:46:34.315675: step 1010, loss = 1.76 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 09:46:44.329134: step 1020, loss = 1.78 (127.8 examples/sec; 1.001 sec/batch)
2018-04-09 09:46:54.290270: step 1030, loss = 1.72 (128.5 examples/sec; 0.996 sec/batch)
2018-04-09 09:47:04.328862: step 1040, loss = 1.86 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 09:47:14.376043: step 1050, loss = 2.05 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 09:47:24.419121: step 1060, loss = 1.94 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 09:47:34.453002: step 1070, loss = 1.84 (127.6 examples/sec; 1.003 sec/batch)
2018-04-09 09:47:44.517411: step 1080, loss = 2.04 (127.2 examples/sec; 1.006 sec/batch)
2018-04-09 09:47:54.546105: step 1090, loss = 1.74 (127.6 examples/sec; 1.003 sec/batch)
2018-04-09 09:48:04.906488: step 1100, loss = 1.78 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 09:48:14.950347: step 1110, loss = 1.67 (127.4 examples/sec; 1.004 sec/batch)
2018-04-09 09:48:24.924653: step 1120, loss = 1.84 (128.3 examples/sec; 0.997 sec/batch)
2018-04-09 09:48:34.908360: step 1130, loss = 1.63 (128.2 examples/sec; 0.998 sec/batch)
2018-04-09 09:48:44.884448: step 1140, loss = 1.69 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 09:48:54.881447: step 1150, loss = 1.78 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 09:49:04.901819: step 1160, loss = 1.58 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 09:49:14.896357: step 1170, loss = 1.95 (128.1 examples/sec; 0.999 sec/batch)
2018-04-09 09:49:24.833455: step 1180, loss = 1.71 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 09:49:34.881801: step 1190, loss = 1.64 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 09:49:45.251284: step 1200, loss = 1.51 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 09:49:55.305656: step 1210, loss = 1.74 (127.3 examples/sec; 1.005 sec/batch)
2018-04-09 09:50:05.368355: step 1220, loss = 1.55 (127.2 examples/sec; 1.006 sec/batch)
2018-04-09 09:50:15.432150: step 1230, loss = 1.46 (127.2 examples/sec; 1.006 sec/batch)
2018-04-09 09:50:25.425481: step 1240, loss = 1.60 (128.1 examples/sec; 0.999 sec/batch)
2018-04-09 09:50:35.396264: step 1250, loss = 1.50 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 09:50:45.373613: step 1260, loss = 1.49 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 09:50:55.365080: step 1270, loss = 1.45 (128.1 examples/sec; 0.999 sec/batch)
2018-04-09 09:51:05.405815: step 1280, loss = 1.58 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 09:51:15.377167: step 1290, loss = 1.49 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 09:51:25.628719: step 1300, loss = 1.63 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 09:51:35.609593: step 1310, loss = 1.50 (128.2 examples/sec; 0.998 sec/batch)
2018-04-09 09:51:45.563335: step 1320, loss = 1.31 (128.6 examples/sec; 0.995 sec/batch)
2018-04-09 09:51:55.512843: step 1330, loss = 1.47 (128.6 examples/sec; 0.995 sec/batch)
2018-04-09 09:52:05.510261: step 1340, loss = 1.51 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 09:52:15.498238: step 1350, loss = 1.36 (128.2 examples/sec; 0.999 sec/batch)
2018-04-09 09:52:25.431505: step 1360, loss = 1.42 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 09:52:35.361002: step 1370, loss = 1.32 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 09:52:45.302063: step 1380, loss = 1.40 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 09:52:55.250400: step 1390, loss = 1.51 (128.7 examples/sec; 0.995 sec/batch)
2018-04-09 09:53:05.562002: step 1400, loss = 1.49 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 09:53:15.537744: step 1410, loss = 1.40 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 09:53:25.426794: step 1420, loss = 1.38 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 09:53:35.318353: step 1430, loss = 1.47 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 09:53:45.242858: step 1440, loss = 1.35 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 09:53:55.148167: step 1450, loss = 1.27 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 09:54:05.145732: step 1460, loss = 1.34 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 09:54:15.095704: step 1470, loss = 1.27 (128.6 examples/sec; 0.995 sec/batch)
2018-04-09 09:54:24.999323: step 1480, loss = 1.52 (129.2 examples/sec; 0.990 sec/batch)
2018-04-09 09:54:34.899144: step 1490, loss = 1.33 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 09:54:45.141367: step 1500, loss = 1.36 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 09:54:55.072990: step 1510, loss = 1.36 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 09:55:05.041263: step 1520, loss = 1.40 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 09:55:14.978281: step 1530, loss = 1.22 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 09:55:25.022871: step 1540, loss = 1.54 (127.4 examples/sec; 1.004 sec/batch)
2018-04-09 09:55:34.932179: step 1550, loss = 1.28 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 09:55:44.870433: step 1560, loss = 1.23 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 09:55:54.778730: step 1570, loss = 1.36 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 09:56:04.788038: step 1580, loss = 1.34 (127.9 examples/sec; 1.001 sec/batch)
2018-04-09 09:56:14.704945: step 1590, loss = 1.31 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 09:56:24.900299: step 1600, loss = 1.16 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 09:56:34.818433: step 1610, loss = 1.49 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 09:56:44.705735: step 1620, loss = 1.27 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 09:56:54.600486: step 1630, loss = 1.30 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 09:57:04.571606: step 1640, loss = 1.12 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 09:57:14.541260: step 1650, loss = 1.15 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 09:57:24.458602: step 1660, loss = 1.14 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 09:57:34.388159: step 1670, loss = 1.24 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 09:57:44.325820: step 1680, loss = 1.26 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 09:57:54.244302: step 1690, loss = 1.13 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 09:58:04.521334: step 1700, loss = 1.44 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 09:58:14.480674: step 1710, loss = 1.10 (128.5 examples/sec; 0.996 sec/batch)
2018-04-09 09:58:24.374129: step 1720, loss = 1.03 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 09:58:34.292168: step 1730, loss = 1.15 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 09:58:44.224232: step 1740, loss = 1.05 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 09:58:54.169356: step 1750, loss = 1.31 (128.7 examples/sec; 0.995 sec/batch)
2018-04-09 09:59:04.163636: step 1760, loss = 1.04 (128.1 examples/sec; 0.999 sec/batch)
2018-04-09 09:59:14.292804: step 1770, loss = 1.11 (126.4 examples/sec; 1.013 sec/batch)
2018-04-09 09:59:24.200755: step 1780, loss = 1.07 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 09:59:34.212607: step 1790, loss = 0.98 (127.8 examples/sec; 1.001 sec/batch)
2018-04-09 09:59:44.455109: step 1800, loss = 1.18 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 09:59:54.391733: step 1810, loss = 1.08 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:00:04.407301: step 1820, loss = 1.82 (127.8 examples/sec; 1.002 sec/batch)
2018-04-09 10:00:14.350940: step 1830, loss = 1.17 (128.7 examples/sec; 0.994 sec/batch)
2018-04-09 10:00:24.291644: step 1840, loss = 0.97 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:00:34.259555: step 1850, loss = 1.86 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:00:44.173898: step 1860, loss = 1.18 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:00:54.033358: step 1870, loss = 1.02 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:01:04.034605: step 1880, loss = 1.26 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 10:01:13.997245: step 1890, loss = 1.08 (128.5 examples/sec; 0.996 sec/batch)
2018-04-09 10:01:24.202059: step 1900, loss = 1.11 (125.4 examples/sec; 1.020 sec/batch)
2018-04-09 10:01:34.085944: step 1910, loss = 1.03 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:01:43.963983: step 1920, loss = 1.06 (129.6 examples/sec; 0.988 sec/batch)
2018-04-09 10:01:53.849130: step 1930, loss = 1.17 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:02:03.785692: step 1940, loss = 0.84 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:02:13.741322: step 1950, loss = 1.09 (128.6 examples/sec; 0.996 sec/batch)
2018-04-09 10:02:23.650484: step 1960, loss = 1.07 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:02:33.602764: step 1970, loss = 1.02 (128.6 examples/sec; 0.995 sec/batch)
2018-04-09 10:02:43.562756: step 1980, loss = 0.95 (128.5 examples/sec; 0.996 sec/batch)
2018-04-09 10:02:53.486794: step 1990, loss = 1.05 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:03:03.744639: step 2000, loss = 0.97 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 10:03:13.732647: step 2010, loss = 0.99 (128.2 examples/sec; 0.999 sec/batch)
2018-04-09 10:03:23.641923: step 2020, loss = 1.29 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:03:33.529193: step 2030, loss = 0.84 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:03:43.505191: step 2040, loss = 1.08 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 10:03:53.439243: step 2050, loss = 0.97 (128.8 examples/sec; 0.993 sec/batch)
2018-04-09 10:04:03.419722: step 2060, loss = 0.94 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 10:04:13.385170: step 2070, loss = 1.00 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:04:23.316477: step 2080, loss = 0.94 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:04:33.251825: step 2090, loss = 0.87 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:04:43.484478: step 2100, loss = 1.02 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 10:04:53.428540: step 2110, loss = 1.04 (128.7 examples/sec; 0.994 sec/batch)
2018-04-09 10:05:03.405809: step 2120, loss = 0.87 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 10:05:13.370523: step 2130, loss = 0.95 (128.5 examples/sec; 0.996 sec/batch)
2018-04-09 10:05:23.385456: step 2140, loss = 0.89 (127.8 examples/sec; 1.001 sec/batch)
2018-04-09 10:05:33.310775: step 2150, loss = 1.13 (129.0 examples/sec; 0.993 sec/batch)
2018-04-09 10:05:43.301738: step 2160, loss = 1.23 (128.1 examples/sec; 0.999 sec/batch)
2018-04-09 10:05:53.210256: step 2170, loss = 1.04 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:06:03.181864: step 2180, loss = 1.10 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:06:13.119693: step 2190, loss = 0.83 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:06:23.399181: step 2200, loss = 1.17 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 10:06:33.305335: step 2210, loss = 0.98 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:06:43.176456: step 2220, loss = 0.92 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:06:53.039214: step 2230, loss = 0.86 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:07:02.915479: step 2240, loss = 0.75 (129.6 examples/sec; 0.988 sec/batch)
2018-04-09 10:07:12.835507: step 2250, loss = 0.77 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:07:22.702018: step 2260, loss = 1.11 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:07:32.472975: step 2270, loss = 0.87 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 10:07:42.288412: step 2280, loss = 0.83 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:07:52.131675: step 2290, loss = 0.76 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:08:02.315134: step 2300, loss = 1.17 (125.7 examples/sec; 1.018 sec/batch)
2018-04-09 10:08:12.214052: step 2310, loss = 0.81 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:08:22.058508: step 2320, loss = 0.84 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:08:31.897623: step 2330, loss = 0.82 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:08:41.798376: step 2340, loss = 0.78 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:08:51.664766: step 2350, loss = 0.84 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:09:01.521410: step 2360, loss = 0.90 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 10:09:11.564965: step 2370, loss = 0.75 (127.4 examples/sec; 1.004 sec/batch)
2018-04-09 10:09:21.480196: step 2380, loss = 0.75 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:09:31.526968: step 2390, loss = 0.98 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 10:09:41.646439: step 2400, loss = 0.69 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 10:09:51.516498: step 2410, loss = 0.79 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:10:01.388931: step 2420, loss = 0.85 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:10:11.334217: step 2430, loss = 0.74 (128.7 examples/sec; 0.995 sec/batch)
2018-04-09 10:10:21.241161: step 2440, loss = 0.63 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:10:31.139901: step 2450, loss = 0.95 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:10:41.067798: step 2460, loss = 0.75 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:10:50.984574: step 2470, loss = 0.77 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:11:00.939847: step 2480, loss = 0.84 (128.6 examples/sec; 0.996 sec/batch)
2018-04-09 10:11:10.871107: step 2490, loss = 0.75 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:11:21.021520: step 2500, loss = 0.70 (126.1 examples/sec; 1.015 sec/batch)
2018-04-09 10:11:30.918384: step 2510, loss = 0.94 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:11:40.829537: step 2520, loss = 1.02 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:11:50.765561: step 2530, loss = 0.69 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:12:00.693640: step 2540, loss = 0.72 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:12:10.620014: step 2550, loss = 0.68 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:12:24.243688: step 2560, loss = 0.71 (94.0 examples/sec; 1.362 sec/batch)
2018-04-09 10:12:38.355859: step 2570, loss = 0.89 (90.7 examples/sec; 1.411 sec/batch)
2018-04-09 10:12:48.211898: step 2580, loss = 0.72 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 10:12:58.055325: step 2590, loss = 0.69 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:13:08.249001: step 2600, loss = 0.68 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 10:13:18.146238: step 2610, loss = 1.18 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:13:27.930818: step 2620, loss = 0.81 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 10:13:37.717277: step 2630, loss = 0.67 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 10:13:47.523966: step 2640, loss = 0.62 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:13:57.620477: step 2650, loss = 0.64 (126.8 examples/sec; 1.010 sec/batch)
2018-04-09 10:14:07.616800: step 2660, loss = 0.63 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 10:14:17.553923: step 2670, loss = 0.61 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:14:27.438726: step 2680, loss = 0.71 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:14:37.299143: step 2690, loss = 0.69 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:14:47.448046: step 2700, loss = 0.65 (126.1 examples/sec; 1.015 sec/batch)
2018-04-09 10:14:57.257324: step 2710, loss = 0.70 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:15:07.116370: step 2720, loss = 0.69 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:15:16.990473: step 2730, loss = 0.59 (129.6 examples/sec; 0.987 sec/batch)
2018-04-09 10:15:26.844505: step 2740, loss = 0.81 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:15:36.702575: step 2750, loss = 0.64 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:15:46.548384: step 2760, loss = 0.68 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:15:56.356183: step 2770, loss = 0.76 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:16:06.209765: step 2780, loss = 0.77 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:16:16.075149: step 2790, loss = 0.54 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:16:26.230984: step 2800, loss = 0.73 (126.0 examples/sec; 1.016 sec/batch)
2018-04-09 10:16:36.083623: step 2810, loss = 0.79 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:16:45.883123: step 2820, loss = 0.57 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:16:55.697908: step 2830, loss = 0.63 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 10:17:05.569397: step 2840, loss = 0.60 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:17:15.450041: step 2850, loss = 0.79 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:17:25.265410: step 2860, loss = 0.60 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:17:35.069214: step 2870, loss = 0.56 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:17:44.916326: step 2880, loss = 0.54 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:17:54.749528: step 2890, loss = 0.61 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:18:04.967911: step 2900, loss = 0.53 (125.3 examples/sec; 1.022 sec/batch)
2018-04-09 10:18:14.885674: step 2910, loss = 0.71 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:18:24.806888: step 2920, loss = 0.61 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:18:34.702698: step 2930, loss = 0.52 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:18:44.565021: step 2940, loss = 0.68 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:18:54.452700: step 2950, loss = 0.68 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:19:04.384459: step 2960, loss = 0.59 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:19:14.310682: step 2970, loss = 0.69 (129.0 examples/sec; 0.993 sec/batch)
2018-04-09 10:19:24.212115: step 2980, loss = 0.73 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:19:34.249643: step 2990, loss = 0.48 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 10:19:44.444200: step 3000, loss = 0.56 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 10:19:54.271552: step 3010, loss = 0.64 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:20:04.186345: step 3020, loss = 0.69 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:20:14.130393: step 3030, loss = 0.50 (128.7 examples/sec; 0.994 sec/batch)
2018-04-09 10:20:24.032293: step 3040, loss = 0.59 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:20:33.950324: step 3050, loss = 0.59 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:20:43.893368: step 3060, loss = 0.52 (128.7 examples/sec; 0.994 sec/batch)
2018-04-09 10:20:53.864757: step 3070, loss = 0.52 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:21:03.832266: step 3080, loss = 0.51 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:21:13.776870: step 3090, loss = 0.73 (128.7 examples/sec; 0.994 sec/batch)
2018-04-09 10:21:23.977778: step 3100, loss = 0.61 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 10:21:33.859155: step 3110, loss = 0.56 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:21:43.754210: step 3120, loss = 0.59 (129.4 examples/sec; 0.990 sec/batch)
2018-04-09 10:21:53.650768: step 3130, loss = 0.51 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:22:03.575755: step 3140, loss = 0.79 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:22:13.499596: step 3150, loss = 0.52 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:22:23.413769: step 3160, loss = 0.55 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:22:33.325046: step 3170, loss = 0.61 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:22:43.220638: step 3180, loss = 0.61 (129.4 examples/sec; 0.990 sec/batch)
2018-04-09 10:22:53.132884: step 3190, loss = 0.47 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:23:03.378421: step 3200, loss = 0.46 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 10:23:13.307177: step 3210, loss = 0.66 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:23:23.212042: step 3220, loss = 0.55 (129.2 examples/sec; 0.990 sec/batch)
2018-04-09 10:23:33.115351: step 3230, loss = 0.58 (129.2 examples/sec; 0.990 sec/batch)
2018-04-09 10:23:42.996034: step 3240, loss = 0.52 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:23:52.887819: step 3250, loss = 0.48 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:24:02.805539: step 3260, loss = 0.48 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:24:12.750301: step 3270, loss = 0.56 (128.7 examples/sec; 0.994 sec/batch)
2018-04-09 10:24:22.713018: step 3280, loss = 0.42 (128.5 examples/sec; 0.996 sec/batch)
2018-04-09 10:24:32.616105: step 3290, loss = 0.51 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:24:42.854116: step 3300, loss = 0.73 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 10:24:52.746022: step 3310, loss = 0.44 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:25:02.666965: step 3320, loss = 0.54 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:25:12.594250: step 3330, loss = 0.47 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:25:22.485030: step 3340, loss = 0.52 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:25:32.337144: step 3350, loss = 0.52 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:25:42.149833: step 3360, loss = 0.45 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 10:25:51.948837: step 3370, loss = 0.52 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:26:01.834357: step 3380, loss = 0.44 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:26:11.868922: step 3390, loss = 0.55 (127.6 examples/sec; 1.003 sec/batch)
2018-04-09 10:26:21.992546: step 3400, loss = 0.49 (126.4 examples/sec; 1.012 sec/batch)
2018-04-09 10:26:31.806970: step 3410, loss = 0.53 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 10:26:41.660360: step 3420, loss = 0.39 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:26:51.495008: step 3430, loss = 0.38 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:27:01.314074: step 3440, loss = 0.64 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:27:11.205397: step 3450, loss = 0.42 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:27:21.088532: step 3460, loss = 0.47 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:27:30.903830: step 3470, loss = 0.49 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:27:40.741346: step 3480, loss = 0.60 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:27:50.584825: step 3490, loss = 0.44 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:28:00.719151: step 3500, loss = 0.40 (126.3 examples/sec; 1.013 sec/batch)
2018-04-09 10:28:10.609982: step 3510, loss = 0.44 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:28:20.458078: step 3520, loss = 0.55 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:28:30.266411: step 3530, loss = 0.52 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:28:40.115793: step 3540, loss = 0.44 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:28:49.992707: step 3550, loss = 0.50 (129.6 examples/sec; 0.988 sec/batch)
2018-04-09 10:28:59.843357: step 3560, loss = 0.44 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:29:09.757910: step 3570, loss = 0.43 (129.1 examples/sec; 0.991 sec/batch)
2018-04-09 10:29:19.595695: step 3580, loss = 0.38 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:29:29.415551: step 3590, loss = 1.07 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:29:39.567889: step 3600, loss = 0.51 (126.1 examples/sec; 1.015 sec/batch)
2018-04-09 10:29:49.410379: step 3610, loss = 0.50 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:29:59.256494: step 3620, loss = 0.58 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:30:09.223233: step 3630, loss = 0.41 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:30:19.121313: step 3640, loss = 0.53 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:30:28.987744: step 3650, loss = 0.40 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:30:38.849230: step 3660, loss = 0.61 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:30:48.674995: step 3670, loss = 0.40 (130.3 examples/sec; 0.983 sec/batch)
2018-04-09 10:30:58.541008: step 3680, loss = 0.41 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:31:08.456690: step 3690, loss = 0.47 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:31:18.620488: step 3700, loss = 0.47 (125.9 examples/sec; 1.016 sec/batch)
2018-04-09 10:31:28.454273: step 3710, loss = 0.48 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:31:38.254809: step 3720, loss = 0.73 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:31:48.039135: step 3730, loss = 0.58 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 10:31:57.829886: step 3740, loss = 0.33 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 10:32:07.711557: step 3750, loss = 0.34 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:32:17.605284: step 3760, loss = 1.38 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:32:27.444412: step 3770, loss = 0.50 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:32:37.292592: step 3780, loss = 0.36 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:32:47.134561: step 3790, loss = 0.44 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:32:57.299184: step 3800, loss = 0.46 (125.9 examples/sec; 1.016 sec/batch)
2018-04-09 10:33:07.232305: step 3810, loss = 0.38 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:33:17.139883: step 3820, loss = 0.52 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:33:27.002554: step 3830, loss = 0.43 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:33:36.871003: step 3840, loss = 0.39 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:33:46.772551: step 3850, loss = 0.40 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:33:56.675933: step 3860, loss = 0.35 (129.2 examples/sec; 0.990 sec/batch)
2018-04-09 10:34:06.612342: step 3870, loss = 0.32 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:34:16.544551: step 3880, loss = 1.04 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:34:26.393389: step 3890, loss = 0.36 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:34:36.582268: step 3900, loss = 0.38 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 10:34:46.443360: step 3910, loss = 0.39 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:34:56.260837: step 3920, loss = 0.50 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:35:06.145451: step 3930, loss = 0.42 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:35:15.995192: step 3940, loss = 0.52 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:35:25.799119: step 3950, loss = 0.41 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:35:35.638048: step 3960, loss = 0.42 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:35:45.500210: step 3970, loss = 0.39 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:35:55.343603: step 3980, loss = 0.34 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:36:05.283970: step 3990, loss = 0.60 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:36:15.466261: step 4000, loss = 0.41 (125.7 examples/sec; 1.018 sec/batch)
2018-04-09 10:36:25.331251: step 4010, loss = 0.42 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:36:35.174613: step 4020, loss = 0.42 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 10:36:45.043256: step 4030, loss = 0.36 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:36:54.862650: step 4040, loss = 0.32 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:37:04.748212: step 4050, loss = 0.35 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:37:14.609976: step 4060, loss = 0.42 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:37:24.431189: step 4070, loss = 0.45 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:37:34.266614: step 4080, loss = 0.32 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:37:44.091959: step 4090, loss = 0.31 (130.3 examples/sec; 0.983 sec/batch)
2018-04-09 10:37:54.217344: step 4100, loss = 0.73 (126.4 examples/sec; 1.013 sec/batch)
2018-04-09 10:38:04.037665: step 4110, loss = 0.40 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:38:13.865674: step 4120, loss = 0.40 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:38:23.660695: step 4130, loss = 0.36 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 10:38:33.491746: step 4140, loss = 0.40 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:38:43.313073: step 4150, loss = 0.35 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:38:53.148520: step 4160, loss = 0.36 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:39:03.029403: step 4170, loss = 0.35 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:39:12.877157: step 4180, loss = 0.41 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:39:22.688900: step 4190, loss = 0.32 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:39:32.982619: step 4200, loss = 0.30 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 10:39:42.742784: step 4210, loss = 0.33 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 10:39:52.564308: step 4220, loss = 0.32 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:40:02.405030: step 4230, loss = 0.53 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:40:12.215727: step 4240, loss = 0.36 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:40:22.002946: step 4250, loss = 0.38 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 10:40:31.804057: step 4260, loss = 0.32 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:40:41.588388: step 4270, loss = 0.43 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 10:40:51.374243: step 4280, loss = 0.37 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 10:41:01.197831: step 4290, loss = 0.32 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:41:11.399884: step 4300, loss = 0.27 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 10:41:21.215683: step 4310, loss = 0.41 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:41:30.996002: step 4320, loss = 0.36 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 10:41:40.790706: step 4330, loss = 0.47 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 10:41:50.575484: step 4340, loss = 0.33 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 10:42:00.373440: step 4350, loss = 0.34 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:42:10.259966: step 4360, loss = 0.33 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:42:20.061331: step 4370, loss = 0.35 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:42:29.853660: step 4380, loss = 0.34 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 10:42:39.660245: step 4390, loss = 1.59 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:42:49.707848: step 4400, loss = 0.44 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 10:42:59.433308: step 4410, loss = 0.33 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 10:43:09.214036: step 4420, loss = 0.71 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 10:43:18.966830: step 4430, loss = 0.34 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 10:43:28.707483: step 4440, loss = 0.38 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 10:43:38.492898: step 4450, loss = 0.29 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 10:43:48.272394: step 4460, loss = 0.30 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 10:43:58.052008: step 4470, loss = 0.32 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 10:44:07.885596: step 4480, loss = 0.37 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:44:17.691145: step 4490, loss = 0.30 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:44:27.759689: step 4500, loss = 0.43 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 10:44:37.576381: step 4510, loss = 0.36 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 10:44:47.380190: step 4520, loss = 0.34 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:44:57.290562: step 4530, loss = 0.41 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:45:07.084699: step 4540, loss = 0.34 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 10:45:16.880479: step 4550, loss = 0.32 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 10:45:26.642781: step 4560, loss = 0.38 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 10:45:36.399713: step 4570, loss = 0.27 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 10:45:46.164836: step 4580, loss = 0.51 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 10:45:55.928453: step 4590, loss = 0.28 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 10:46:06.074241: step 4600, loss = 0.33 (126.2 examples/sec; 1.015 sec/batch)
2018-04-09 10:46:15.903970: step 4610, loss = 0.30 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:46:25.701886: step 4620, loss = 0.29 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:46:35.510140: step 4630, loss = 0.37 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:46:45.499660: step 4640, loss = 0.31 (128.1 examples/sec; 0.999 sec/batch)
2018-04-09 10:46:55.330373: step 4650, loss = 0.29 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:47:05.228221: step 4660, loss = 0.33 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:47:15.063220: step 4670, loss = 0.34 (130.1 examples/sec; 0.983 sec/batch)
2018-04-09 10:47:24.897133: step 4680, loss = 0.32 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:47:34.691537: step 4690, loss = 0.30 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 10:47:44.797709: step 4700, loss = 0.34 (126.7 examples/sec; 1.011 sec/batch)
2018-04-09 10:47:54.643564: step 4710, loss = 0.30 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:48:04.651191: step 4720, loss = 0.28 (127.9 examples/sec; 1.001 sec/batch)
2018-04-09 10:48:14.531941: step 4730, loss = 0.32 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:48:24.339996: step 4740, loss = 0.45 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:48:34.123334: step 4750, loss = 0.39 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 10:48:43.880911: step 4760, loss = 0.38 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 10:48:53.690661: step 4770, loss = 0.29 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:49:03.585435: step 4780, loss = 0.33 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:49:13.523641: step 4790, loss = 0.30 (128.8 examples/sec; 0.994 sec/batch)
2018-04-09 10:49:23.662984: step 4800, loss = 0.29 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 10:49:33.664668: step 4810, loss = 0.30 (128.0 examples/sec; 1.000 sec/batch)
2018-04-09 10:49:43.478703: step 4820, loss = 0.25 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 10:49:53.348563: step 4830, loss = 0.28 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:50:03.296455: step 4840, loss = 0.33 (128.7 examples/sec; 0.995 sec/batch)
2018-04-09 10:50:13.274280: step 4850, loss = 0.36 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 10:50:23.180381: step 4860, loss = 0.32 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:50:33.130329: step 4870, loss = 0.30 (128.6 examples/sec; 0.995 sec/batch)
2018-04-09 10:50:43.018266: step 4880, loss = 0.27 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 10:50:52.917715: step 4890, loss = 0.27 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:51:03.124831: step 4900, loss = 0.28 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 10:51:13.040069: step 4910, loss = 0.30 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:51:22.966055: step 4920, loss = 0.38 (129.0 examples/sec; 0.993 sec/batch)
2018-04-09 10:51:32.867251: step 4930, loss = 0.26 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 10:51:42.788885: step 4940, loss = 0.35 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:51:52.681358: step 4950, loss = 0.57 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:52:02.611605: step 4960, loss = 0.29 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:52:12.544715: step 4970, loss = 0.34 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:52:22.425857: step 4980, loss = 0.30 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:52:32.287531: step 4990, loss = 0.27 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:52:42.466520: step 5000, loss = 0.26 (125.7 examples/sec; 1.018 sec/batch)
2018-04-09 10:52:52.339580: step 5010, loss = 0.29 (129.6 examples/sec; 0.987 sec/batch)
2018-04-09 10:53:02.269895: step 5020, loss = 0.26 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:53:12.201812: step 5030, loss = 0.29 (128.9 examples/sec; 0.993 sec/batch)
2018-04-09 10:53:22.075928: step 5040, loss = 0.29 (129.6 examples/sec; 0.987 sec/batch)
2018-04-09 10:53:31.924222: step 5050, loss = 1.32 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:53:41.684207: step 5060, loss = 0.71 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 10:53:51.462792: step 5070, loss = 0.46 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 10:54:01.268353: step 5080, loss = 0.45 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:54:11.193070: step 5090, loss = 0.37 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:54:21.345084: step 5100, loss = 0.42 (126.1 examples/sec; 1.015 sec/batch)
2018-04-09 10:54:31.168724: step 5110, loss = 0.32 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 10:54:41.007063: step 5120, loss = 0.30 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:54:50.865309: step 5130, loss = 0.45 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:55:00.727589: step 5140, loss = 0.40 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:55:10.634562: step 5150, loss = 0.31 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 10:55:20.528031: step 5160, loss = 0.27 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:55:30.358675: step 5170, loss = 0.32 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 10:55:40.237001: step 5180, loss = 0.24 (129.6 examples/sec; 0.988 sec/batch)
2018-04-09 10:55:50.077299: step 5190, loss = 0.35 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 10:56:00.272022: step 5200, loss = 0.31 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 10:56:10.222228: step 5210, loss = 0.31 (128.6 examples/sec; 0.995 sec/batch)
2018-04-09 10:56:20.104933: step 5220, loss = 0.30 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 10:56:29.982767: step 5230, loss = 0.38 (129.6 examples/sec; 0.988 sec/batch)
2018-04-09 10:56:39.949161: step 5240, loss = 0.23 (128.4 examples/sec; 0.997 sec/batch)
2018-04-09 10:56:49.813337: step 5250, loss = 0.42 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:56:59.671126: step 5260, loss = 0.36 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:57:09.565729: step 5270, loss = 0.36 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:57:19.432354: step 5280, loss = 0.26 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 10:57:29.352621: step 5290, loss = 0.29 (129.0 examples/sec; 0.992 sec/batch)
2018-04-09 10:57:39.510395: step 5300, loss = 0.32 (126.0 examples/sec; 1.016 sec/batch)
2018-04-09 10:57:49.405045: step 5310, loss = 0.39 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 10:57:59.259253: step 5320, loss = 0.23 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:58:09.174582: step 5330, loss = 0.31 (129.1 examples/sec; 0.992 sec/batch)
2018-04-09 10:58:19.026828: step 5340, loss = 0.32 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:58:28.811649: step 5350, loss = 0.34 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 10:58:38.666965: step 5360, loss = 0.25 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 10:58:48.527436: step 5370, loss = 0.25 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:58:58.373336: step 5380, loss = 0.36 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 10:59:08.224042: step 5390, loss = 0.27 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 10:59:18.345200: step 5400, loss = 0.41 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 10:59:28.153031: step 5410, loss = 0.29 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 10:59:38.015220: step 5420, loss = 0.29 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 10:59:47.818677: step 5430, loss = 0.28 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 10:59:57.637415: step 5440, loss = 0.30 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:00:07.532258: step 5450, loss = 0.25 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 11:00:17.385832: step 5460, loss = 0.28 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 11:00:27.153375: step 5470, loss = 0.27 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:00:36.958022: step 5480, loss = 0.32 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:00:46.779913: step 5490, loss = 0.34 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:00:56.920354: step 5500, loss = 0.28 (126.2 examples/sec; 1.014 sec/batch)
2018-04-09 11:01:06.820322: step 5510, loss = 0.35 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 11:01:16.598039: step 5520, loss = 0.28 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:01:26.408871: step 5530, loss = 0.27 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:01:36.232823: step 5540, loss = 0.25 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:01:46.034414: step 5550, loss = 0.20 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:01:55.853680: step 5560, loss = 0.28 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:02:05.751550: step 5570, loss = 0.25 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 11:02:15.621627: step 5580, loss = 0.29 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 11:02:25.425327: step 5590, loss = 0.62 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:02:35.490935: step 5600, loss = 0.36 (127.2 examples/sec; 1.007 sec/batch)
2018-04-09 11:02:45.275870: step 5610, loss = 0.32 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:02:55.053494: step 5620, loss = 0.22 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:03:04.870026: step 5630, loss = 0.24 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:03:14.717829: step 5640, loss = 0.27 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 11:03:24.525697: step 5650, loss = 0.22 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:03:34.329608: step 5660, loss = 0.31 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:03:44.124976: step 5670, loss = 0.30 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:03:53.916530: step 5680, loss = 0.28 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:04:03.780048: step 5690, loss = 0.22 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 11:04:13.953755: step 5700, loss = 0.23 (125.8 examples/sec; 1.017 sec/batch)
2018-04-09 11:04:23.971613: step 5710, loss = 0.28 (127.8 examples/sec; 1.002 sec/batch)
2018-04-09 11:04:33.789216: step 5720, loss = 0.30 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:04:43.644898: step 5730, loss = 0.24 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 11:04:53.503249: step 5740, loss = 0.27 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 11:05:03.401159: step 5750, loss = 0.27 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 11:05:13.297481: step 5760, loss = 0.46 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 11:05:23.133377: step 5770, loss = 0.27 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 11:05:32.967205: step 5780, loss = 0.27 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 11:05:42.806804: step 5790, loss = 0.24 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 11:05:52.993591: step 5800, loss = 0.33 (125.7 examples/sec; 1.019 sec/batch)
2018-04-09 11:06:02.867662: step 5810, loss = 0.26 (129.6 examples/sec; 0.987 sec/batch)
2018-04-09 11:06:12.713731: step 5820, loss = 0.24 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 11:06:22.497080: step 5830, loss = 0.27 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:06:32.292949: step 5840, loss = 0.31 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:06:42.091111: step 5850, loss = 0.22 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:06:51.893487: step 5860, loss = 0.43 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:07:01.613411: step 5870, loss = 0.55 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:07:11.354467: step 5880, loss = 0.41 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:07:21.074334: step 5890, loss = 0.39 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:07:31.096929: step 5900, loss = 0.31 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 11:07:40.838068: step 5910, loss = 0.22 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:07:50.615918: step 5920, loss = 0.25 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:08:00.389102: step 5930, loss = 0.23 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:08:10.238962: step 5940, loss = 0.29 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 11:08:20.022998: step 5950, loss = 0.27 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:08:29.801598: step 5960, loss = 0.37 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:08:39.583739: step 5970, loss = 0.24 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:08:49.363448: step 5980, loss = 0.26 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:08:59.149221: step 5990, loss = 0.22 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 11:09:09.350862: step 6000, loss = 0.24 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 11:09:19.175198: step 6010, loss = 0.33 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:09:28.936543: step 6020, loss = 0.29 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:09:38.764805: step 6030, loss = 0.26 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 11:09:48.532451: step 6040, loss = 0.34 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:09:58.317183: step 6050, loss = 0.28 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:10:08.199333: step 6060, loss = 0.25 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 11:10:18.044175: step 6070, loss = 0.23 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 11:10:27.848666: step 6080, loss = 0.25 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:10:37.652394: step 6090, loss = 0.29 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:10:47.724426: step 6100, loss = 0.27 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 11:10:57.537224: step 6110, loss = 0.25 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 11:11:07.391630: step 6120, loss = 0.24 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 11:11:17.224006: step 6130, loss = 0.23 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 11:11:27.019126: step 6140, loss = 0.38 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:11:36.885345: step 6150, loss = 0.23 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 11:11:46.717749: step 6160, loss = 2.77 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 11:11:56.337885: step 6170, loss = 1.32 (133.1 examples/sec; 0.962 sec/batch)
2018-04-09 11:12:05.903876: step 6180, loss = 0.90 (133.8 examples/sec; 0.957 sec/batch)
2018-04-09 11:12:15.539783: step 6190, loss = 0.69 (132.8 examples/sec; 0.964 sec/batch)
2018-04-09 11:12:25.438965: step 6200, loss = 0.61 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 11:12:35.086863: step 6210, loss = 0.56 (132.7 examples/sec; 0.965 sec/batch)
2018-04-09 11:12:44.705033: step 6220, loss = 0.46 (133.1 examples/sec; 0.962 sec/batch)
2018-04-09 11:12:54.343824: step 6230, loss = 0.53 (132.8 examples/sec; 0.964 sec/batch)
2018-04-09 11:13:04.033584: step 6240, loss = 0.50 (132.1 examples/sec; 0.969 sec/batch)
2018-04-09 11:13:13.733909: step 6250, loss = 0.38 (132.0 examples/sec; 0.970 sec/batch)
2018-04-09 11:13:23.424775: step 6260, loss = 0.35 (132.1 examples/sec; 0.969 sec/batch)
2018-04-09 11:13:33.116338: step 6270, loss = 0.41 (132.1 examples/sec; 0.969 sec/batch)
2018-04-09 11:13:42.793862: step 6280, loss = 0.45 (132.3 examples/sec; 0.968 sec/batch)
2018-04-09 11:13:52.494572: step 6290, loss = 0.51 (131.9 examples/sec; 0.970 sec/batch)
2018-04-09 11:14:02.532158: step 6300, loss = 0.44 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 11:14:12.274083: step 6310, loss = 0.43 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:14:21.928719: step 6320, loss = 0.40 (132.6 examples/sec; 0.965 sec/batch)
2018-04-09 11:14:31.606565: step 6330, loss = 0.31 (132.3 examples/sec; 0.968 sec/batch)
2018-04-09 11:14:41.318947: step 6340, loss = 0.38 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:14:51.046727: step 6350, loss = 0.40 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:15:00.768512: step 6360, loss = 0.50 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:15:10.527031: step 6370, loss = 0.36 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:15:20.288455: step 6380, loss = 0.37 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:15:30.008719: step 6390, loss = 0.39 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:15:40.108588: step 6400, loss = 0.29 (126.7 examples/sec; 1.010 sec/batch)
2018-04-09 11:15:49.877388: step 6410, loss = 0.35 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:15:59.667162: step 6420, loss = 0.48 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:16:09.421452: step 6430, loss = 0.34 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:16:19.209182: step 6440, loss = 0.45 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 11:16:28.965026: step 6450, loss = 0.43 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:16:38.710948: step 6460, loss = 0.32 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:16:48.461182: step 6470, loss = 0.30 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:16:58.210097: step 6480, loss = 0.28 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:17:07.990879: step 6490, loss = 0.28 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:17:18.083443: step 6500, loss = 0.45 (126.8 examples/sec; 1.009 sec/batch)
2018-04-09 11:17:27.797941: step 6510, loss = 0.44 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:17:37.503228: step 6520, loss = 0.39 (131.9 examples/sec; 0.971 sec/batch)
2018-04-09 11:17:47.204795: step 6530, loss = 0.36 (131.9 examples/sec; 0.970 sec/batch)
2018-04-09 11:17:56.889377: step 6540, loss = 0.37 (132.2 examples/sec; 0.968 sec/batch)
2018-04-09 11:18:06.649012: step 6550, loss = 0.27 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:18:16.422352: step 6560, loss = 0.33 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:18:26.115768: step 6570, loss = 0.33 (132.0 examples/sec; 0.969 sec/batch)
2018-04-09 11:18:35.801279: step 6580, loss = 0.34 (132.2 examples/sec; 0.969 sec/batch)
2018-04-09 11:18:45.512740: step 6590, loss = 0.28 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:18:55.530529: step 6600, loss = 0.29 (127.8 examples/sec; 1.002 sec/batch)
2018-04-09 11:19:05.305446: step 6610, loss = 0.31 (130.9 examples/sec; 0.977 sec/batch)
2018-04-09 11:19:15.049736: step 6620, loss = 0.33 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:19:24.759394: step 6630, loss = 0.35 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:19:34.466960: step 6640, loss = 0.33 (131.9 examples/sec; 0.971 sec/batch)
2018-04-09 11:19:44.361423: step 6650, loss = 0.29 (129.4 examples/sec; 0.989 sec/batch)
2018-04-09 11:19:54.114007: step 6660, loss = 0.28 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:20:03.886127: step 6670, loss = 0.40 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:20:13.643124: step 6680, loss = 0.36 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:20:23.392812: step 6690, loss = 0.46 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:20:33.438254: step 6700, loss = 0.37 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 11:20:43.208415: step 6710, loss = 0.30 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:20:52.950278: step 6720, loss = 0.33 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:21:02.741976: step 6730, loss = 0.41 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:21:12.514974: step 6740, loss = 0.70 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:21:22.249530: step 6750, loss = 0.30 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:21:31.998417: step 6760, loss = 0.32 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:21:41.729637: step 6770, loss = 0.83 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:21:51.521739: step 6780, loss = 0.30 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:22:01.226358: step 6790, loss = 0.39 (131.9 examples/sec; 0.970 sec/batch)
2018-04-09 11:22:11.309711: step 6800, loss = 0.35 (126.9 examples/sec; 1.008 sec/batch)
2018-04-09 11:22:21.054007: step 6810, loss = 0.31 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:22:30.736685: step 6820, loss = 0.26 (132.2 examples/sec; 0.968 sec/batch)
2018-04-09 11:22:40.448470: step 6830, loss = 0.32 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:22:50.160895: step 6840, loss = 0.30 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:22:59.919079: step 6850, loss = 0.37 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:23:09.724614: step 6860, loss = 0.26 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:23:19.488266: step 6870, loss = 0.36 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:23:29.231133: step 6880, loss = 0.27 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:23:38.965323: step 6890, loss = 0.29 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:23:49.060053: step 6900, loss = 0.37 (126.8 examples/sec; 1.009 sec/batch)
2018-04-09 11:23:58.849843: step 6910, loss = 0.28 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:24:08.705599: step 6920, loss = 0.28 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 11:24:18.452564: step 6930, loss = 0.31 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:24:28.203209: step 6940, loss = 0.41 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:24:37.960729: step 6950, loss = 0.41 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:24:47.728560: step 6960, loss = 0.27 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:24:57.540215: step 6970, loss = 0.28 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:25:07.354926: step 6980, loss = 0.28 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 11:25:17.134846: step 6990, loss = 0.26 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:25:27.176554: step 7000, loss = 0.23 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 11:25:36.917371: step 7010, loss = 0.78 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:25:46.644225: step 7020, loss = 0.26 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:25:56.369512: step 7030, loss = 0.24 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:26:06.172125: step 7040, loss = 0.27 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:26:15.928134: step 7050, loss = 0.26 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:26:25.656470: step 7060, loss = 0.33 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:26:35.386762: step 7070, loss = 0.28 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:26:45.107008: step 7080, loss = 0.29 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:26:54.992700: step 7090, loss = 0.39 (129.5 examples/sec; 0.989 sec/batch)
2018-04-09 11:27:05.058763: step 7100, loss = 0.26 (127.2 examples/sec; 1.007 sec/batch)
2018-04-09 11:27:14.855577: step 7110, loss = 0.35 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:27:24.589583: step 7120, loss = 0.30 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:27:34.297048: step 7130, loss = 0.23 (131.9 examples/sec; 0.971 sec/batch)
2018-04-09 11:27:44.015629: step 7140, loss = 0.23 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:27:53.755440: step 7150, loss = 0.26 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:28:03.513285: step 7160, loss = 0.29 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:28:13.282707: step 7170, loss = 0.27 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:28:22.998568: step 7180, loss = 0.25 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:28:32.734429: step 7190, loss = 0.28 (131.5 examples/sec; 0.974 sec/batch)
2018-04-09 11:28:42.777288: step 7200, loss = 0.24 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 11:28:52.534554: step 7210, loss = 0.30 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:29:02.307755: step 7220, loss = 0.23 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:29:12.091547: step 7230, loss = 0.28 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:29:21.856830: step 7240, loss = 0.28 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 11:29:31.636411: step 7250, loss = 0.25 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:29:41.498447: step 7260, loss = 0.25 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 11:29:51.242428: step 7270, loss = 0.71 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:30:00.951385: step 7280, loss = 0.33 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:30:10.769853: step 7290, loss = 0.29 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:30:20.812477: step 7300, loss = 0.28 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 11:30:30.566403: step 7310, loss = 0.29 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:30:40.293289: step 7320, loss = 0.24 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:30:49.994853: step 7330, loss = 0.26 (131.9 examples/sec; 0.970 sec/batch)
2018-04-09 11:30:59.766719: step 7340, loss = 0.27 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:31:09.589786: step 7350, loss = 0.30 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:31:19.368388: step 7360, loss = 0.40 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:31:29.119468: step 7370, loss = 0.31 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:31:38.830011: step 7380, loss = 0.33 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:31:48.574862: step 7390, loss = 0.21 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:31:58.710328: step 7400, loss = 0.29 (126.3 examples/sec; 1.014 sec/batch)
2018-04-09 11:32:08.550058: step 7410, loss = 0.28 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 11:32:18.339598: step 7420, loss = 0.27 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 11:32:28.115757: step 7430, loss = 0.35 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:32:37.886046: step 7440, loss = 0.32 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:32:47.680735: step 7450, loss = 0.37 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:32:57.491639: step 7460, loss = 0.22 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:33:07.294767: step 7470, loss = 0.26 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:33:17.094547: step 7480, loss = 0.25 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:33:26.858358: step 7490, loss = 0.27 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:33:36.937956: step 7500, loss = 0.31 (127.0 examples/sec; 1.008 sec/batch)
2018-04-09 11:33:46.680629: step 7510, loss = 0.27 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:33:56.417053: step 7520, loss = 0.26 (131.5 examples/sec; 0.974 sec/batch)
2018-04-09 11:34:06.209560: step 7530, loss = 0.24 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:34:15.992654: step 7540, loss = 0.34 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:34:25.766500: step 7550, loss = 0.24 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:34:35.556362: step 7560, loss = 0.22 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:34:45.324304: step 7570, loss = 0.23 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:34:55.082311: step 7580, loss = 0.31 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:35:04.930822: step 7590, loss = 0.30 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 11:35:15.054401: step 7600, loss = 0.22 (126.4 examples/sec; 1.012 sec/batch)
2018-04-09 11:35:24.874077: step 7610, loss = 0.26 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:35:34.676065: step 7620, loss = 0.25 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:35:44.420186: step 7630, loss = 0.27 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:35:54.181287: step 7640, loss = 0.33 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:36:03.999157: step 7650, loss = 0.23 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:36:13.784200: step 7660, loss = 0.27 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 11:36:23.557713: step 7670, loss = 0.20 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:36:33.325461: step 7680, loss = 0.40 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:36:43.072344: step 7690, loss = 0.28 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:36:53.119769: step 7700, loss = 0.27 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 11:37:02.898336: step 7710, loss = 0.26 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:37:12.663451: step 7720, loss = 0.31 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 11:37:22.391027: step 7730, loss = 0.26 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:37:32.150925: step 7740, loss = 0.22 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:37:41.905429: step 7750, loss = 0.30 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:37:51.676746: step 7760, loss = 0.25 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:38:01.469260: step 7770, loss = 0.26 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:38:11.280121: step 7780, loss = 0.19 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:38:21.178556: step 7790, loss = 0.21 (129.3 examples/sec; 0.990 sec/batch)
2018-04-09 11:38:31.307740: step 7800, loss = 0.31 (126.4 examples/sec; 1.013 sec/batch)
2018-04-09 11:38:41.061953: step 7810, loss = 0.29 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:38:50.819456: step 7820, loss = 0.24 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:39:00.572962: step 7830, loss = 0.20 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:39:10.427338: step 7840, loss = 0.21 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 11:39:20.223542: step 7850, loss = 0.19 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:39:29.999116: step 7860, loss = 0.20 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:39:39.865729: step 7870, loss = 0.22 (129.7 examples/sec; 0.987 sec/batch)
2018-04-09 11:39:49.640144: step 7880, loss = 0.25 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:39:59.409738: step 7890, loss = 0.27 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:40:09.578228: step 7900, loss = 0.21 (125.9 examples/sec; 1.017 sec/batch)
2018-04-09 11:40:19.377727: step 7910, loss = 0.21 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:40:29.151247: step 7920, loss = 0.27 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:40:38.873226: step 7930, loss = 0.27 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:40:48.574909: step 7940, loss = 0.29 (131.9 examples/sec; 0.970 sec/batch)
2018-04-09 11:40:58.327095: step 7950, loss = 0.29 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:41:08.141682: step 7960, loss = 0.21 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 11:41:17.901788: step 7970, loss = 0.22 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:41:27.640235: step 7980, loss = 0.32 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:41:37.409829: step 7990, loss = 0.22 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:41:47.474035: step 8000, loss = 0.45 (127.2 examples/sec; 1.006 sec/batch)
2018-04-09 11:41:57.235399: step 8010, loss = 0.24 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:42:07.076770: step 8020, loss = 0.30 (130.1 examples/sec; 0.984 sec/batch)
2018-04-09 11:42:16.842054: step 8030, loss = 0.24 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 11:42:26.586282: step 8040, loss = 0.26 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:42:36.382558: step 8050, loss = 0.23 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:42:46.167684: step 8060, loss = 0.20 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 11:42:55.938638: step 8070, loss = 0.24 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:43:05.746076: step 8080, loss = 0.23 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:43:15.599563: step 8090, loss = 0.26 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 11:43:25.688363: step 8100, loss = 0.21 (126.9 examples/sec; 1.009 sec/batch)
2018-04-09 11:43:35.480939: step 8110, loss = 0.25 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:43:45.273988: step 8120, loss = 0.19 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:43:55.046986: step 8130, loss = 0.25 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:44:04.867069: step 8140, loss = 0.31 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:44:14.670852: step 8150, loss = 0.23 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:44:24.445137: step 8160, loss = 0.29 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:44:34.211123: step 8170, loss = 0.20 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 11:44:44.006512: step 8180, loss = 0.29 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:44:53.765171: step 8190, loss = 0.23 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:45:03.955453: step 8200, loss = 0.23 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 11:45:13.779075: step 8210, loss = 0.23 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:45:23.535659: step 8220, loss = 0.19 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:45:33.286980: step 8230, loss = 0.23 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:45:43.097239: step 8240, loss = 0.20 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:45:52.898933: step 8250, loss = 0.27 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:46:02.706008: step 8260, loss = 0.24 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:46:12.475335: step 8270, loss = 0.24 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:46:22.207534: step 8280, loss = 0.19 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:46:31.936554: step 8290, loss = 0.25 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:46:42.057596: step 8300, loss = 0.23 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 11:46:51.826161: step 8310, loss = 0.61 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:47:01.621229: step 8320, loss = 0.30 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 11:47:11.473239: step 8330, loss = 0.26 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 11:47:21.267664: step 8340, loss = 0.20 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:47:31.057336: step 8350, loss = 0.22 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 11:47:40.804250: step 8360, loss = 0.21 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:47:50.572840: step 8370, loss = 0.77 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:48:00.329847: step 8380, loss = 0.29 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:48:10.068909: step 8390, loss = 0.21 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:48:20.109767: step 8400, loss = 0.27 (127.5 examples/sec; 1.004 sec/batch)
2018-04-09 11:48:29.876156: step 8410, loss = 0.22 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 11:48:39.732262: step 8420, loss = 0.23 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 11:48:49.531582: step 8430, loss = 0.21 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:48:59.292912: step 8440, loss = 0.24 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:49:09.090688: step 8450, loss = 0.39 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:49:18.839779: step 8460, loss = 0.27 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:49:28.559250: step 8470, loss = 0.24 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:49:38.413269: step 8480, loss = 0.19 (129.9 examples/sec; 0.985 sec/batch)
2018-04-09 11:49:48.123912: step 8490, loss = 0.29 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:49:58.171220: step 8500, loss = 0.20 (127.4 examples/sec; 1.005 sec/batch)
2018-04-09 11:50:07.993495: step 8510, loss = 0.18 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:50:17.740761: step 8520, loss = 0.24 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:50:27.464392: step 8530, loss = 0.19 (131.6 examples/sec; 0.972 sec/batch)
2018-04-09 11:50:37.197935: step 8540, loss = 0.18 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:50:46.942151: step 8550, loss = 0.22 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:50:56.706488: step 8560, loss = 0.21 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:51:06.516068: step 8570, loss = 0.22 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:51:16.285700: step 8580, loss = 0.21 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:51:26.033004: step 8590, loss = 0.22 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:51:36.096698: step 8600, loss = 0.27 (127.2 examples/sec; 1.006 sec/batch)
2018-04-09 11:51:45.847122: step 8610, loss = 0.20 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:51:55.600079: step 8620, loss = 0.18 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:52:05.367895: step 8630, loss = 0.24 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:52:15.165889: step 8640, loss = 0.25 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:52:24.898589: step 8650, loss = 0.21 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:52:34.625545: step 8660, loss = 0.23 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:52:44.368729: step 8670, loss = 0.27 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:52:54.106456: step 8680, loss = 0.32 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:53:03.880250: step 8690, loss = 0.20 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:53:13.936790: step 8700, loss = 0.19 (127.3 examples/sec; 1.006 sec/batch)
2018-04-09 11:53:23.664356: step 8710, loss = 0.18 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:53:33.392236: step 8720, loss = 0.20 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:53:43.127204: step 8730, loss = 0.20 (131.5 examples/sec; 0.973 sec/batch)
2018-04-09 11:53:52.875422: step 8740, loss = 0.22 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:54:02.677900: step 8750, loss = 0.25 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:54:12.522468: step 8760, loss = 0.22 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 11:54:22.251608: step 8770, loss = 0.20 (131.6 examples/sec; 0.973 sec/batch)
2018-04-09 11:54:31.996347: step 8780, loss = 0.26 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:54:41.772224: step 8790, loss = 0.22 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 11:54:51.881036: step 8800, loss = 0.21 (126.6 examples/sec; 1.011 sec/batch)
2018-04-09 11:55:01.688912: step 8810, loss = 0.19 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 11:55:11.546448: step 8820, loss = 0.20 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 11:55:21.349701: step 8830, loss = 0.17 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 11:55:31.113753: step 8840, loss = 0.22 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:55:40.864582: step 8850, loss = 0.20 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:55:50.618437: step 8860, loss = 0.21 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:56:00.410612: step 8870, loss = 0.19 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 11:56:10.227186: step 8880, loss = 0.26 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 11:56:19.966646: step 8890, loss = 0.24 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:56:29.991138: step 8900, loss = 0.20 (127.7 examples/sec; 1.002 sec/batch)
2018-04-09 11:56:39.729362: step 8910, loss = 0.25 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:56:49.499082: step 8920, loss = 0.20 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 11:56:59.249651: step 8930, loss = 0.24 (131.3 examples/sec; 0.975 sec/batch)
2018-04-09 11:57:09.157707: step 8940, loss = 0.21 (129.2 examples/sec; 0.991 sec/batch)
2018-04-09 11:57:18.981512: step 8950, loss = 0.24 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 11:57:28.719169: step 8960, loss = 0.22 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 11:57:38.483749: step 8970, loss = 0.18 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 11:57:48.248843: step 8980, loss = 0.22 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 11:57:58.106912: step 8990, loss = 0.18 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 11:58:08.208439: step 9000, loss = 0.22 (126.7 examples/sec; 1.010 sec/batch)
2018-04-09 11:58:17.992655: step 9010, loss = 0.24 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 11:58:27.713595: step 9020, loss = 0.22 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:58:37.427512: step 9030, loss = 0.23 (131.8 examples/sec; 0.971 sec/batch)
2018-04-09 11:58:47.146492: step 9040, loss = 0.17 (131.7 examples/sec; 0.972 sec/batch)
2018-04-09 11:58:56.900638: step 9050, loss = 0.25 (131.2 examples/sec; 0.975 sec/batch)
2018-04-09 11:59:06.756759: step 9060, loss = 0.19 (129.9 examples/sec; 0.986 sec/batch)
2018-04-09 11:59:16.736471: step 9070, loss = 0.24 (128.3 examples/sec; 0.998 sec/batch)
2018-04-09 11:59:26.473935: step 9080, loss = 0.23 (131.5 examples/sec; 0.974 sec/batch)
2018-04-09 11:59:36.230646: step 9090, loss = 0.20 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 11:59:46.398686: step 9100, loss = 0.18 (125.9 examples/sec; 1.017 sec/batch)
2018-04-09 11:59:56.171415: step 9110, loss = 0.19 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 12:00:05.965014: step 9120, loss = 0.19 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 12:00:15.780256: step 9130, loss = 0.17 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 12:00:25.552732: step 9140, loss = 0.21 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 12:00:35.309437: step 9150, loss = 0.17 (131.2 examples/sec; 0.976 sec/batch)
2018-04-09 12:00:45.098575: step 9160, loss = 0.19 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 12:00:54.867219: step 9170, loss = 0.28 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 12:01:04.725400: step 9180, loss = 0.22 (129.8 examples/sec; 0.986 sec/batch)
2018-04-09 12:01:14.529975: step 9190, loss = 0.19 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 12:01:24.587749: step 9200, loss = 0.19 (127.3 examples/sec; 1.006 sec/batch)
2018-04-09 12:01:34.403571: step 9210, loss = 0.17 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 12:01:44.228374: step 9220, loss = 0.22 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 12:01:54.041903: step 9230, loss = 0.18 (130.4 examples/sec; 0.981 sec/batch)
2018-04-09 12:02:03.884916: step 9240, loss = 0.28 (130.0 examples/sec; 0.984 sec/batch)
2018-04-09 12:02:13.716724: step 9250, loss = 0.21 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 12:02:23.512316: step 9260, loss = 0.27 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 12:02:33.288888: step 9270, loss = 0.34 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 12:02:43.071286: step 9280, loss = 0.21 (130.8 examples/sec; 0.978 sec/batch)
2018-04-09 12:02:52.808952: step 9290, loss = 0.17 (131.4 examples/sec; 0.974 sec/batch)
2018-04-09 12:03:02.920417: step 9300, loss = 0.20 (126.6 examples/sec; 1.011 sec/batch)
2018-04-09 12:03:12.752931: step 9310, loss = 0.18 (130.2 examples/sec; 0.983 sec/batch)
2018-04-09 12:03:22.579449: step 9320, loss = 0.22 (130.3 examples/sec; 0.983 sec/batch)
2018-04-09 12:03:32.344668: step 9330, loss = 0.23 (131.1 examples/sec; 0.977 sec/batch)
2018-04-09 12:03:42.118920: step 9340, loss = 0.17 (131.0 examples/sec; 0.977 sec/batch)
2018-04-09 12:03:51.900984: step 9350, loss = 0.17 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 12:04:01.680528: step 9360, loss = 0.20 (130.9 examples/sec; 0.978 sec/batch)
2018-04-09 12:04:11.500553: step 9370, loss = 0.20 (130.3 examples/sec; 0.982 sec/batch)
2018-04-09 12:04:21.319751: step 9380, loss = 0.20 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 12:04:31.112205: step 9390, loss = 0.25 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 12:04:41.183703: step 9400, loss = 0.24 (127.1 examples/sec; 1.007 sec/batch)
2018-04-09 12:04:50.982566: step 9410, loss = 0.19 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 12:05:00.783474: step 9420, loss = 0.22 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 12:05:10.608819: step 9430, loss = 0.21 (130.3 examples/sec; 0.983 sec/batch)
2018-04-09 12:05:20.424004: step 9440, loss = 0.23 (130.4 examples/sec; 0.982 sec/batch)
2018-04-09 12:05:30.220748: step 9450, loss = 0.21 (130.7 examples/sec; 0.980 sec/batch)
2018-04-09 12:05:40.011612: step 9460, loss = 0.20 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 12:05:49.806440: step 9470, loss = 0.16 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 12:05:59.601198: step 9480, loss = 0.20 (130.7 examples/sec; 0.979 sec/batch)
2018-04-09 12:06:09.447791: step 9490, loss = 0.20 (130.0 examples/sec; 0.985 sec/batch)
2018-04-09 12:06:19.568792: step 9500, loss = 0.24 (126.5 examples/sec; 1.012 sec/batch)
2018-04-09 12:06:29.371914: step 9510, loss = 0.22 (130.6 examples/sec; 0.980 sec/batch)
2018-04-09 12:06:39.134616: step 9520, loss = 0.21 (131.1 examples/sec; 0.976 sec/batch)
2018-04-09 12:06:48.940313: step 9530, loss = 0.23 (130.5 examples/sec; 0.981 sec/batch)
2018-04-09 12:06:58.728698: step 9540, loss = 0.17 (130.8 examples/sec; 0.979 sec/batch)
2018-04-09 12:07:08.613226: step 9550, loss = 83.22 (129.5 examples/sec; 0.988 sec/batch)
2018-04-09 12:07:18.750577: step 9560, loss = 1437995047583744.00 (126.3 examples/sec; 1.014 sec/batch)
2018-04-09 12:07:29.142679: step 9570, loss = 5229515895725883392.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:07:39.497620: step 9580, loss = 5187838357719220224.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:07:49.850004: step 9590, loss = 5146492872224145408.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:08:00.718889: step 9600, loss = 5105476690461589504.00 (117.8 examples/sec; 1.087 sec/batch)
2018-04-09 12:08:11.350886: step 9610, loss = 5064787063652483072.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 12:08:21.895018: step 9620, loss = 5024422342529384448.00 (121.4 examples/sec; 1.054 sec/batch)
2018-04-09 12:08:32.382462: step 9630, loss = 4984380328069038080.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 12:08:42.928512: step 9640, loss = 4944656072469118976.00 (121.4 examples/sec; 1.055 sec/batch)
2018-04-09 12:08:53.431591: step 9650, loss = 4905249575729627136.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 12:09:04.021160: step 9660, loss = 4866155340292423680.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 12:09:14.638921: step 9670, loss = 4827373915913322496.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 12:09:25.155136: step 9680, loss = 4788901454301626368.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 12:09:35.621035: step 9690, loss = 4750734107166638080.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 12:09:46.350591: step 9700, loss = 4712872424264171520.00 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 12:09:56.670814: step 9710, loss = 4675313107059343360.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:10:07.007185: step 9720, loss = 4638052307261456384.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:10:17.346327: step 9730, loss = 4601088375603068928.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:10:27.655531: step 9740, loss = 4564419387938832384.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:10:37.993200: step 9750, loss = 4528042870367584256.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:10:48.352100: step 9760, loss = 4491954974598627328.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:10:58.721984: step 9770, loss = 4456155700631961600.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:11:09.150775: step 9780, loss = 4420641475054796800.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:11:19.558411: step 9790, loss = 4385410098843877376.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 12:11:30.216690: step 9800, loss = 4350460197609668608.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 12:11:40.593261: step 9810, loss = 4315789297451008000.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:11:50.898224: step 9820, loss = 4281394374710919168.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 12:12:01.215376: step 9830, loss = 4247272955488239616.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:12:11.572998: step 9840, loss = 4213423940271341568.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:12:21.896612: step 9850, loss = 4179843480769527808.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:12:32.196933: step 9860, loss = 4146531576982798336.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 12:12:42.514541: step 9870, loss = 4113484655498362880.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:12:52.839634: step 9880, loss = 4080701616804593664.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 12:13:03.172709: step 9890, loss = 4048179437244514304.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:13:13.849740: step 9900, loss = 4015916467550683136.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 12:13:24.154566: step 9910, loss = 3983911608211472384.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 12:13:34.465314: step 9920, loss = 3952161560691998720.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:13:44.773184: step 9930, loss = 3920664125969006592.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:13:55.112860: step 9940, loss = 3889417654775054336.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:14:05.540129: step 9950, loss = 3858420222964793344.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 12:14:15.955760: step 9960, loss = 3827669356637061120.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 12:14:26.373156: step 9970, loss = 3797163956280229888.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 12:14:36.792339: step 9980, loss = 3766902097748951040.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 12:14:47.191345: step 9990, loss = 3736881856897875968.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:14:57.891339: step 10000, loss = 3707099110558400512.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 12:15:08.243517: step 10010, loss = 3677554683364245504.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:15:18.579708: step 10020, loss = 3648246101414248448.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:15:28.877369: step 10030, loss = 3619170341051432960.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 12:15:39.170202: step 10040, loss = 3590327127397892096.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 12:15:49.470755: step 10050, loss = 3561712612162928640.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 12:15:59.830563: step 10060, loss = 3533327894858170368.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:16:10.185736: step 10070, loss = 3505169402070827008.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:16:20.572433: step 10080, loss = 3477235209655549952.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:16:30.958217: step 10090, loss = 3449523118589083648.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:16:41.677882: step 10100, loss = 3422030654970265600.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 12:16:52.017553: step 10110, loss = 3394758368554909696.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:17:02.375597: step 10120, loss = 3367703235686039552.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:17:12.753543: step 10130, loss = 3340864156852027392.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:17:23.065162: step 10140, loss = 3314238383273803776.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:17:33.371896: step 10150, loss = 3287825090317647872.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:17:43.707747: step 10160, loss = 3261621804082397184.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:17:54.044272: step 10170, loss = 3235627699934330880.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:18:04.393660: step 10180, loss = 3209841403483914240.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:18:14.894280: step 10190, loss = 3184259341318356992.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 12:18:25.566201: step 10200, loss = 3158881788315566080.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 12:18:35.865674: step 10210, loss = 3133706270574379008.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 12:18:46.192092: step 10220, loss = 3108731688583168000.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 12:18:56.504968: step 10230, loss = 3083956118196584448.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:19:06.850770: step 10240, loss = 3059377635269279744.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:19:17.210134: step 10250, loss = 3034995964923346944.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:19:27.522762: step 10260, loss = 3010807258868088832.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:19:37.979861: step 10270, loss = 2986812616615133184.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 12:19:48.249816: step 10280, loss = 2963008464751689728.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 12:19:58.611202: step 10290, loss = 2939393703766130688.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:20:09.396998: step 10300, loss = 2915967783902642176.00 (118.7 examples/sec; 1.079 sec/batch)
2018-04-09 12:20:19.822400: step 10310, loss = 2892728506137968640.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 12:20:30.233561: step 10320, loss = 2869673946326761472.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 12:20:40.563793: step 10330, loss = 2846804104469020672.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:20:50.879424: step 10340, loss = 2824115407151955968.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:21:01.255064: step 10350, loss = 2801608679009288192.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:21:11.684246: step 10360, loss = 2779280621506134016.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:21:22.114260: step 10370, loss = 2757130959764586496.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:21:32.470880: step 10380, loss = 2735157769639297024.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:21:42.779560: step 10390, loss = 2713359401862823936.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:21:53.439270: step 10400, loss = 2691734482045632512.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 12:22:03.786988: step 10410, loss = 2670282735309815808.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:22:14.184319: step 10420, loss = 2649000863120490496.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:22:24.572334: step 10430, loss = 2627889415233470464.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:22:34.865867: step 10440, loss = 2606945917747593216.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 12:22:45.204574: step 10450, loss = 2586169546029137920.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:22:55.592853: step 10460, loss = 2565558650810662912.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:23:06.090871: step 10470, loss = 2545111582824726528.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 12:23:16.528158: step 10480, loss = 2524828342071328768.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 12:23:26.848501: step 10490, loss = 2504705630015586304.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:23:37.467949: step 10500, loss = 2484744271291219968.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 12:23:47.782280: step 10510, loss = 2464941791997067264.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:23:58.085378: step 10520, loss = 2445296817743593472.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 12:24:08.434930: step 10530, loss = 2425809348530798592.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:24:18.772548: step 10540, loss = 2406476635579613184.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:24:29.118907: step 10550, loss = 2387297579378409472.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:24:39.437186: step 10560, loss = 2368271630171373568.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:24:49.777121: step 10570, loss = 2349397688446877696.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:25:00.122469: step 10580, loss = 2330673280303759360.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:25:10.498659: step 10590, loss = 2312098680619925504.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:25:21.160600: step 10600, loss = 2293672240127934464.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 12:25:31.536770: step 10610, loss = 2275392584438251520.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:25:41.842847: step 10620, loss = 2257258064283435008.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:25:52.177426: step 10630, loss = 2239268817102438400.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:26:02.614492: step 10640, loss = 2221422368994099200.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 12:26:12.984531: step 10650, loss = 2203718307641556992.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:26:23.368218: step 10660, loss = 2186155533533184000.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:26:33.684136: step 10670, loss = 2168732397401538560.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:26:44.036880: step 10680, loss = 2151448349490806784.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:26:54.539194: step 10690, loss = 2134301877972500480.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 12:27:05.340689: step 10700, loss = 2117292433090805760.00 (118.5 examples/sec; 1.080 sec/batch)
2018-04-09 12:27:15.949383: step 10710, loss = 2100418365578280960.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 12:27:26.512448: step 10720, loss = 2083678575923298304.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 12:27:36.947937: step 10730, loss = 2067072239492136960.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 12:27:47.427340: step 10740, loss = 2050598669090029568.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 12:27:57.935710: step 10750, loss = 2034255940571627520.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 12:28:08.532566: step 10760, loss = 2018043779059023872.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 12:28:18.984011: step 10770, loss = 2001960947601637376.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 12:28:29.317391: step 10780, loss = 1986005522054119424.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:28:39.842505: step 10790, loss = 1970177777294376960.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 12:28:50.630036: step 10800, loss = 1954475789177061376.00 (118.7 examples/sec; 1.079 sec/batch)
2018-04-09 12:29:01.001858: step 10810, loss = 1938899282824265728.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:29:11.430790: step 10820, loss = 1923446471529594880.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:29:21.792434: step 10830, loss = 1908117492732002304.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:29:32.179216: step 10840, loss = 1892910422286139392.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:29:42.611467: step 10850, loss = 1877824710436192256.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:29:52.995789: step 10860, loss = 1862859120231579648.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:30:03.361454: step 10870, loss = 1848012689599627264.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:30:13.734331: step 10880, loss = 1833284593906614272.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:30:24.054867: step 10890, loss = 1818673596201959424.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:30:34.728820: step 10900, loss = 1804179559046709248.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 12:30:45.083621: step 10910, loss = 1789800558295515136.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:30:55.401926: step 10920, loss = 1775536868826284032.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:31:05.777341: step 10930, loss = 1761386016737853440.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:31:16.160870: step 10940, loss = 1747348002030223360.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:31:26.513653: step 10950, loss = 1733422412386533376.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:31:36.879577: step 10960, loss = 1719607735978295296.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:31:47.192039: step 10970, loss = 1705903010732834816.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:31:57.518742: step 10980, loss = 1692307412016431104.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 12:32:07.893923: step 10990, loss = 1678820252634316800.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:32:18.578974: step 11000, loss = 1665440845391724544.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 12:32:28.946581: step 11010, loss = 1652167678460166144.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:32:39.260172: step 11020, loss = 1639000614400688128.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:32:49.563381: step 11030, loss = 1625938141384802304.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 12:32:59.905414: step 11040, loss = 1612980121973555200.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:33:10.290115: step 11050, loss = 1600124494582644736.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:33:20.652184: step 11060, loss = 1587372496162652160.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:33:31.020191: step 11070, loss = 1574721927690321920.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:33:41.356018: step 11080, loss = 1562172376848793600.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:33:51.670332: step 11090, loss = 1549722744126439424.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:34:02.349534: step 11100, loss = 1537372479767445504.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 12:34:12.721090: step 11110, loss = 1525120759138091008.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:34:23.055040: step 11120, loss = 1512965795531980800.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:34:33.385128: step 11130, loss = 1500907588949114880.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:34:43.710652: step 11140, loss = 1488946414267400192.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 12:34:54.071995: step 11150, loss = 1477079660146720768.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:35:04.449003: step 11160, loss = 1465308151220797440.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:35:14.816336: step 11170, loss = 1453629825905328128.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:35:25.143817: step 11180, loss = 1442044821639266304.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:35:35.477084: step 11190, loss = 1430552176349937664.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:35:46.147954: step 11200, loss = 1419151477720481792.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 12:35:56.488926: step 11210, loss = 1407841076483457024.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:36:06.868837: step 11220, loss = 1396621110077816832.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:36:17.222918: step 11230, loss = 1385490341552979968.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:36:27.547702: step 11240, loss = 1374448496031039488.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:36:37.883734: step 11250, loss = 1363494748878274560.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:36:48.188623: step 11260, loss = 1352628138022010880.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 12:36:58.498525: step 11270, loss = 1341848113706434560.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:37:08.859870: step 11280, loss = 1331153988736778240.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:37:19.175373: step 11290, loss = 1320545213357228032.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 12:37:29.823319: step 11300, loss = 1310020825495109632.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 12:37:40.170582: step 11310, loss = 1299580275394609152.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:37:50.490982: step 11320, loss = 1289223013299912704.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:38:00.872228: step 11330, loss = 1278948489455206400.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:38:11.254749: step 11340, loss = 1268755604348862464.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:38:21.653896: step 11350, loss = 1258643945664020480.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:38:32.021369: step 11360, loss = 1248612963644866560.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:38:42.356050: step 11370, loss = 1238661696218726400.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:38:52.693975: step 11380, loss = 1228790005946646528.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:39:03.079000: step 11390, loss = 1218996930755952640.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 12:39:13.757535: step 11400, loss = 1209281920890830848.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 12:39:24.093951: step 11410, loss = 1199644289156513792.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:39:34.422096: step 11420, loss = 1190083760675094528.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:39:44.828608: step 11430, loss = 1180599235934945280.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 12:39:55.177212: step 11440, loss = 1171190302619205632.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:40:05.556278: step 11450, loss = 1161856136094154752.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:40:16.174462: step 11460, loss = 1152596392762408960.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 12:40:26.578352: step 11470, loss = 1143410591587631104.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 12:40:36.906917: step 11480, loss = 1134297976655577088.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:40:47.229407: step 11490, loss = 1125258066929909760.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 12:40:57.890212: step 11500, loss = 1116290037776908288.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 12:41:08.329070: step 11510, loss = 1107393545599188992.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 12:41:18.694675: step 11520, loss = 1098567903201984512.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:41:29.127066: step 11530, loss = 1089812973146341376.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:41:39.512132: step 11540, loss = 1081127312323248128.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 12:41:49.973567: step 11550, loss = 1072511126891134976.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 12:42:00.384340: step 11560, loss = 1063963729655234560.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 12:42:10.871574: step 11570, loss = 1055484227262349312.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 12:42:21.266025: step 11580, loss = 1047072413554049024.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 12:42:31.666537: step 11590, loss = 1038727532616089600.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:42:42.322853: step 11600, loss = 1030449240851087360.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 12:42:52.666042: step 11610, loss = 1022236988503228416.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:43:03.028494: step 11620, loss = 1014090019658268672.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:43:13.395325: step 11630, loss = 1006008196877254656.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:43:23.777845: step 11640, loss = 997990489368035328.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:43:34.213689: step 11650, loss = 990036622252703744.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 12:43:44.619692: step 11660, loss = 982146320653352960.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 12:43:54.986992: step 11670, loss = 974318897375215616.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:44:05.387705: step 11680, loss = 966553940101431296.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:44:15.810917: step 11690, loss = 958850830356709376.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 12:44:26.504043: step 11700, loss = 951209155824189440.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 12:44:36.899014: step 11710, loss = 943628435467534336.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 12:44:47.292132: step 11720, loss = 936108050811453440.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:44:57.690271: step 11730, loss = 928647658258563072.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:45:08.121872: step 11740, loss = 921246639333572608.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:45:18.558646: step 11750, loss = 913904375561191424.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 12:45:28.931862: step 11760, loss = 906620935660896256.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:45:39.305343: step 11770, loss = 899395357560012800.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:45:49.701636: step 11780, loss = 892227778697494528.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:46:00.119665: step 11790, loss = 885116962122760192.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 12:46:10.880794: step 11800, loss = 878062770396856320.00 (118.9 examples/sec; 1.076 sec/batch)
2018-04-09 12:46:21.316263: step 11810, loss = 871064859922399232.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 12:46:31.691379: step 11820, loss = 864122749663051776.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:46:42.056354: step 11830, loss = 857236164740907008.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:46:52.428811: step 11840, loss = 850404280522244096.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:47:02.842376: step 11850, loss = 843626822129156096.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 12:47:13.204203: step 11860, loss = 836903445964259328.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:47:23.549510: step 11870, loss = 830233670991216640.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:47:33.901774: step 11880, loss = 823616947454214144.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 12:47:44.208051: step 11890, loss = 817052931755868160.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:47:54.973450: step 11900, loss = 810541280298795008.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 12:48:05.423060: step 11910, loss = 804081580766134272.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 12:48:15.792373: step 11920, loss = 797673352121548800.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:48:26.099462: step 11930, loss = 791316113328701440.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:48:36.395197: step 11940, loss = 785009589509685248.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 12:48:46.722422: step 11950, loss = 778753162189209600.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:48:57.060965: step 11960, loss = 772546831367274496.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:49:07.404491: step 11970, loss = 766389978568589312.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 12:49:17.745433: step 11980, loss = 760282054037340160.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:49:28.053879: step 11990, loss = 754222920334573568.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:49:38.841102: step 12000, loss = 748211890265522176.00 (118.7 examples/sec; 1.079 sec/batch)
2018-04-09 12:49:49.105018: step 12010, loss = 742248963830185984.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 12:49:59.471776: step 12020, loss = 736333591272751104.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:50:09.836332: step 12030, loss = 730465291556880384.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:50:20.178275: step 12040, loss = 724643583646236672.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:50:30.520598: step 12050, loss = 718868604979773440.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 12:50:40.835272: step 12060, loss = 713139462204293120.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 12:50:51.165357: step 12070, loss = 707455743002935296.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:51:01.553715: step 12080, loss = 701817653534130176.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:51:11.915122: step 12090, loss = 696224437883633664.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:51:22.568650: step 12100, loss = 690675889893015552.00 (120.1 examples/sec; 1.065 sec/batch)
2018-04-09 12:51:32.901257: step 12110, loss = 685171459806461952.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 12:51:43.208745: step 12120, loss = 679710872746065920.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 12:51:53.582142: step 12130, loss = 674293716394967040.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:52:03.979042: step 12140, loss = 668919853314211840.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 12:52:14.370097: step 12150, loss = 663588802467463168.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:52:24.777733: step 12160, loss = 658300151537860608.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 12:52:35.136915: step 12170, loss = 653053694366973952.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:52:45.510039: step 12180, loss = 647849087357419520.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:52:55.883160: step 12190, loss = 642685918192336896.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:53:06.641906: step 12200, loss = 637563911993819136.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 12:53:17.113591: step 12210, loss = 632482725164482560.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 12:53:27.485028: step 12220, loss = 627442082826420224.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 12:53:37.830323: step 12230, loss = 622441503943294976.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 12:53:48.187452: step 12240, loss = 617480988515106816.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 12:53:58.577428: step 12250, loss = 612559780627611648.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:54:08.953346: step 12260, loss = 607677742841856000.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 12:54:19.315516: step 12270, loss = 602834806438363136.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 12:54:29.619824: step 12280, loss = 598030421661319168.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 12:54:39.921600: step 12290, loss = 593264244913340416.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 12:54:50.569782: step 12300, loss = 588536138755473408.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 12:55:00.959791: step 12310, loss = 583845759590334464.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 12:55:11.366355: step 12320, loss = 579192626381586432.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 12:55:21.760456: step 12330, loss = 574576704769490944.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 12:55:32.125700: step 12340, loss = 569997548077449216.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 12:55:42.504715: step 12350, loss = 565454847067815936.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 12:55:52.937202: step 12360, loss = 560948326862684160.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 12:56:03.488411: step 12370, loss = 556477712584146944.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 12:56:13.988633: step 12380, loss = 552042763714035712.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 12:56:24.465918: step 12390, loss = 547643136654966784.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 12:56:35.223061: step 12400, loss = 543278625248509952.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 12:56:45.683745: step 12410, loss = 538948817177804800.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 12:56:56.157941: step 12420, loss = 534653643723374592.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 12:57:06.678409: step 12430, loss = 530392589489143808.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 12:57:17.213093: step 12440, loss = 526165654475112448.00 (121.5 examples/sec; 1.053 sec/batch)
2018-04-09 12:57:27.677159: step 12450, loss = 521972254565728256.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 12:57:38.142855: step 12460, loss = 517812321041514496.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 12:57:48.632765: step 12470, loss = 513685579024564224.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 12:57:59.140654: step 12480, loss = 509591684917493760.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 12:58:09.693559: step 12490, loss = 505530398202134528.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 12:58:20.589879: step 12500, loss = 501501615799271424.00 (117.5 examples/sec; 1.090 sec/batch)
2018-04-09 12:58:31.085814: step 12510, loss = 497504684873875456.00 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 12:58:41.548148: step 12520, loss = 493539777224638464.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 12:58:52.019072: step 12530, loss = 489606480534700032.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 12:59:02.613324: step 12540, loss = 485704451206676480.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 12:59:13.301869: step 12550, loss = 481833517441875968.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 12:59:23.794211: step 12560, loss = 477993541801345024.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 12:59:34.242603: step 12570, loss = 474184146327961600.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 12:59:44.810790: step 12580, loss = 470405021784080384.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 12:59:55.312949: step 12590, loss = 466656030730747904.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 13:00:06.165036: step 12600, loss = 462936932649795584.00 (117.9 examples/sec; 1.085 sec/batch)
2018-04-09 13:00:16.699847: step 12610, loss = 459247452663316480.00 (121.5 examples/sec; 1.053 sec/batch)
2018-04-09 13:00:27.119567: step 12620, loss = 455587453332357120.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 13:00:37.481810: step 12630, loss = 451956556699795456.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:00:47.857740: step 12640, loss = 448354659686416384.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 13:00:58.262897: step 12650, loss = 444781384335097856.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 13:01:08.743062: step 12660, loss = 441236696286101504.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 13:01:19.203462: step 12670, loss = 437720114503090176.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 13:01:29.680881: step 12680, loss = 434231604626325504.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 13:01:40.159629: step 12690, loss = 430770891777900544.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 13:01:50.979218: step 12700, loss = 427337838518861824.00 (118.3 examples/sec; 1.082 sec/batch)
2018-04-09 13:02:01.500983: step 12710, loss = 423932066892087296.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 13:02:12.051997: step 12720, loss = 420553542537838592.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 13:02:22.547209: step 12730, loss = 417201818779516928.00 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 13:02:33.021628: step 12740, loss = 413876758178168832.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 13:02:43.489087: step 12750, loss = 410578463813009408.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 13:02:53.977266: step 12760, loss = 407306145410056192.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 13:03:04.460523: step 12770, loss = 404060180926431232.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 13:03:14.945766: step 12780, loss = 400839883167367168.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 13:03:25.307808: step 12790, loss = 397645149053648896.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:03:35.941565: step 12800, loss = 394476287822921728.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 13:03:46.262884: step 12810, loss = 391332543560941568.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:03:56.603039: step 12820, loss = 388213847548231680.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:04:06.974576: step 12830, loss = 385120062345838592.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:04:17.549599: step 12840, loss = 382050913075855360.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 13:04:27.931288: step 12850, loss = 379005987421421568.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:04:38.223241: step 12860, loss = 375985319742275584.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:04:48.531050: step 12870, loss = 372988978757894144.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:04:58.854317: step 12880, loss = 370016311633248256.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:05:09.209428: step 12890, loss = 367067455807291392.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:05:19.856870: step 12900, loss = 364141998963163136.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 13:05:30.137343: step 12910, loss = 361239906741125120.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:05:40.491782: step 12920, loss = 358361007342485504.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 13:05:50.785936: step 12930, loss = 355504922810122240.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 13:06:01.115744: step 12940, loss = 352671687503773696.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:06:11.499114: step 12950, loss = 349861026545532928.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:06:21.848907: step 12960, loss = 347072768136708096.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:06:32.169184: step 12970, loss = 344306740478607360.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:06:42.470562: step 12980, loss = 341562703053062144.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:06:52.795196: step 12990, loss = 338840552780857344.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:07:03.453139: step 13000, loss = 336140117863301120.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 13:07:13.792911: step 13010, loss = 333461192141963264.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:07:24.100077: step 13020, loss = 330803638177890304.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:07:34.378821: step 13030, loss = 328167181093175296.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:07:44.669045: step 13040, loss = 325551923967033344.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:07:55.002032: step 13050, loss = 322957351403388928.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:08:05.353048: step 13060, loss = 320383463402242048.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:08:15.716784: step 13070, loss = 317830122524639232.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:08:26.058136: step 13080, loss = 315297088252411904.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:08:36.388018: step 13090, loss = 312784188786868224.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:08:47.064263: step 13100, loss = 310291389768269824.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 13:08:57.502728: step 13110, loss = 307818519397924864.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 13:09:07.898397: step 13120, loss = 305365302797926400.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 13:09:18.249006: step 13130, loss = 302931636889059328.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:09:28.547666: step 13140, loss = 300517384232370176.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:09:38.866414: step 13150, loss = 298122373029167104.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:09:49.273696: step 13160, loss = 295746431480758272.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 13:09:59.556432: step 13170, loss = 293389456507928576.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:10:10.023844: step 13180, loss = 291051276311986176.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 13:10:20.505670: step 13190, loss = 288731684734500864.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 13:10:31.166235: step 13200, loss = 286430595876126720.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 13:10:41.522798: step 13210, loss = 284147803578433536.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:10:51.799789: step 13220, loss = 281883239121944576.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 13:11:02.158137: step 13230, loss = 279636730707968000.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:11:12.582875: step 13240, loss = 277408123717681152.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 13:11:22.887526: step 13250, loss = 275197315071868928.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:11:33.236135: step 13260, loss = 273004064252362752.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:11:43.507982: step 13270, loss = 270828233820209152.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:11:53.808246: step 13280, loss = 268669772235800576.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:12:04.107196: step 13290, loss = 266528490520576000.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:12:14.755355: step 13300, loss = 264404354314797056.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 13:12:25.085317: step 13310, loss = 262297140280164352.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:12:35.411005: step 13320, loss = 260206728157593600.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 13:12:45.722552: step 13330, loss = 258132963328262144.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:12:56.029435: step 13340, loss = 256075759892824064.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:13:06.386389: step 13350, loss = 254034911692849152.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:13:16.776853: step 13360, loss = 252010384368599040.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 13:13:27.088884: step 13370, loss = 250001937401905152.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:13:37.445156: step 13380, loss = 248009484893421568.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:13:47.772914: step 13390, loss = 246032923763933184.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:13:58.452087: step 13400, loss = 244072116574486528.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 13:14:08.893111: step 13410, loss = 242126943065997312.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 13:14:19.275974: step 13420, loss = 240197282979381248.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:14:29.623975: step 13430, loss = 238283016055554048.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:14:39.927580: step 13440, loss = 236383970495823872.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:14:50.261777: step 13450, loss = 234500060400844800.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:15:00.588633: step 13460, loss = 232631165511532544.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:15:10.931111: step 13470, loss = 230777182748672000.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:15:21.276651: step 13480, loss = 228937923133702144.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:15:31.593657: step 13490, loss = 227113352306884608.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 13:15:42.221841: step 13500, loss = 225303350009135104.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 13:15:52.545185: step 13510, loss = 223507761621630976.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:16:02.920538: step 13520, loss = 221726449705418752.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 13:16:13.265849: step 13530, loss = 219959311181283328.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:16:23.636715: step 13540, loss = 218206363229093888.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:16:34.008193: step 13550, loss = 216467296611205120.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:16:44.582305: step 13560, loss = 214742059788009472.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 13:16:54.986841: step 13570, loss = 213030652759506944.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 13:17:05.508294: step 13580, loss = 211332869367267328.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 13:17:16.175392: step 13590, loss = 209648623711944704.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 13:17:26.798032: step 13600, loss = 207977795534454784.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 13:17:37.111623: step 13610, loss = 206320281755582464.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:17:47.423585: step 13620, loss = 204675979296112640.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:17:57.760691: step 13630, loss = 203044785076830208.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:18:08.157968: step 13640, loss = 201426613198389248.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 13:18:18.526284: step 13650, loss = 199821291862097920.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 13:18:29.168325: step 13660, loss = 198228769528348672.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 13:18:39.507571: step 13670, loss = 196648977477664768.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:18:49.819243: step 13680, loss = 195081743911354368.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:19:00.114704: step 13690, loss = 193526982930071552.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:19:10.767108: step 13700, loss = 191984625814339584.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 13:19:21.074222: step 13710, loss = 190454569484943360.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:19:31.358952: step 13720, loss = 188936710862667776.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:19:41.762328: step 13730, loss = 187430929688428544.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 13:19:52.003211: step 13740, loss = 185937157242748928.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:20:02.420965: step 13750, loss = 184455307626283008.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 13:20:12.776227: step 13760, loss = 182985277759815680.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:20:23.111012: step 13770, loss = 181526964564131840.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:20:33.421336: step 13780, loss = 180080247780147200.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:20:43.749362: step 13790, loss = 178645041508515840.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:20:54.402725: step 13800, loss = 177221328569368576.00 (120.1 examples/sec; 1.065 sec/batch)
2018-04-09 13:21:04.787369: step 13810, loss = 175808902804275200.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:21:15.169305: step 13820, loss = 174407781393104896.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:21:25.500932: step 13830, loss = 173017826896904192.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:21:35.834578: step 13840, loss = 171638919056588800.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:21:46.227871: step 13850, loss = 170271006332551168.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 13:21:56.569671: step 13860, loss = 168913985645576192.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:22:06.905660: step 13870, loss = 167567805456056320.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:22:17.246620: step 13880, loss = 166232362684776448.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:22:27.600410: step 13890, loss = 164907537072652288.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 13:22:38.215219: step 13900, loss = 163593328619683840.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 13:22:48.527371: step 13910, loss = 162289513987571712.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:22:58.851537: step 13920, loss = 160996144715923456.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:23:09.203335: step 13930, loss = 159713066185916416.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:23:19.524126: step 13940, loss = 158440175318335488.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:23:29.823547: step 13950, loss = 157177472113180672.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:23:40.174250: step 13960, loss = 155924819131498496.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:23:50.509910: step 13970, loss = 154682164833681408.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:24:00.820249: step 13980, loss = 153449354600906752.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:24:11.146522: step 13990, loss = 152226474332520448.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 13:24:21.773903: step 14000, loss = 151013283510353920.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 13:24:32.077361: step 14010, loss = 149809764954537984.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:24:42.412715: step 14020, loss = 148615815585857536.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:24:52.715876: step 14030, loss = 147431401044574208.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:25:03.061130: step 14040, loss = 146256401071603712.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:25:13.392547: step 14050, loss = 145090798487076864.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:25:23.742015: step 14060, loss = 143934473031909376.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:25:34.069004: step 14070, loss = 142787381756428288.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:25:44.362646: step 14080, loss = 141649387221680128.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 13:25:54.722350: step 14090, loss = 140520506607534080.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:26:05.479434: step 14100, loss = 139400585295167488.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 13:26:15.880959: step 14110, loss = 138289614694645760.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 13:26:26.237230: step 14120, loss = 137187500316688384.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:26:36.585679: step 14130, loss = 136094173441818624.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:26:46.895067: step 14140, loss = 135009513810952192.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:26:57.251656: step 14150, loss = 133933555783827456.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:27:07.652072: step 14160, loss = 132866127561752576.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 13:27:18.018383: step 14170, loss = 131807220554792960.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 13:27:28.377954: step 14180, loss = 130756774633406464.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:27:38.669304: step 14190, loss = 129714695308312576.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:27:49.311016: step 14200, loss = 128680913860034560.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 13:27:59.720618: step 14210, loss = 127655370159030272.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 13:28:10.101066: step 14220, loss = 126638004075757568.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:28:20.471204: step 14230, loss = 125628746890739712.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:28:30.858463: step 14240, loss = 124627521294565376.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 13:28:41.233301: step 14250, loss = 123634284337561600.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:28:51.617124: step 14260, loss = 122648975890186240.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 13:29:02.018886: step 14270, loss = 121671484283289600.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 13:29:12.393325: step 14280, loss = 120701809516871680.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:29:22.730108: step 14290, loss = 119739857101651968.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:29:33.424922: step 14300, loss = 118785558318153728.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 13:29:43.869366: step 14310, loss = 117838878806638592.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 13:29:54.213245: step 14320, loss = 116899732667760640.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 13:30:04.549629: step 14330, loss = 115968085541781504.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:30:14.887773: step 14340, loss = 115043877299159040.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:30:25.207455: step 14350, loss = 114127013450612736.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:30:35.523338: step 14360, loss = 113217459636404224.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 13:30:45.818237: step 14370, loss = 112315155726991360.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 13:30:56.142828: step 14380, loss = 111420033002897408.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:31:06.480921: step 14390, loss = 110532048514449408.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:31:17.134751: step 14400, loss = 109651159311974400.00 (120.1 examples/sec; 1.065 sec/batch)
2018-04-09 13:31:27.466882: step 14410, loss = 108777270906191872.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:31:37.774088: step 14420, loss = 107910348937363456.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:31:48.039748: step 14430, loss = 107050341865881600.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 13:31:58.519586: step 14440, loss = 106197180972269568.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 13:32:09.017853: step 14450, loss = 105350823306854400.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 13:32:19.496870: step 14460, loss = 104511191560224768.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 13:32:29.786636: step 14470, loss = 103678285732380672.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:32:40.215375: step 14480, loss = 102852011334041600.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 13:32:50.485143: step 14490, loss = 102032291055796224.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:33:01.476364: step 14500, loss = 101219159257382912.00 (116.5 examples/sec; 1.099 sec/batch)
2018-04-09 13:33:11.843378: step 14510, loss = 100412469909913600.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 13:33:22.163505: step 14520, loss = 99612180063715328.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:33:32.682692: step 14530, loss = 98818341258395648.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 13:33:43.182297: step 14540, loss = 98030824644935680.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 13:33:53.683084: step 14550, loss = 97249570093793280.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 13:34:04.160780: step 14560, loss = 96474560425099264.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 13:34:14.478893: step 14570, loss = 95705726919376896.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 13:34:24.775517: step 14580, loss = 94942992267214848.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:34:35.094433: step 14590, loss = 94186296339070976.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:34:45.870479: step 14600, loss = 93435673494683648.00 (118.8 examples/sec; 1.078 sec/batch)
2018-04-09 13:34:56.207096: step 14610, loss = 92691029244772352.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:35:06.576030: step 14620, loss = 91952320639664128.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 13:35:16.904154: step 14630, loss = 91219478959882240.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:35:27.218564: step 14640, loss = 90492512795361280.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:35:37.558550: step 14650, loss = 89771310476951552.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:35:47.868346: step 14660, loss = 89055837644914688.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:35:58.190413: step 14670, loss = 88346094299250688.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:36:08.542710: step 14680, loss = 87642003130548224.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 13:36:18.850678: step 14690, loss = 86943521189134336.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:36:29.473398: step 14700, loss = 86250614115270656.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 13:36:39.781985: step 14710, loss = 85563238959284224.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:36:50.078897: step 14720, loss = 84881318411763712.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:37:00.359758: step 14730, loss = 84204835292839936.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:37:10.713619: step 14740, loss = 83533746652839936.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 13:37:21.052911: step 14750, loss = 82868000952156160.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:37:31.401222: step 14760, loss = 82207581010919424.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:37:41.760138: step 14770, loss = 81552409519718400.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 13:37:52.080962: step 14780, loss = 80902469298683904.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:38:02.466605: step 14790, loss = 80257708808208384.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 13:38:13.135185: step 14800, loss = 79618067918749696.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 13:38:23.440751: step 14810, loss = 78983546630307840.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:38:33.782338: step 14820, loss = 78354067633471488.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:38:44.109938: step 14830, loss = 77729622338306048.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:38:54.400657: step 14840, loss = 77110142025334784.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:39:04.791281: step 14850, loss = 76495600924753920.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 13:39:15.097709: step 14860, loss = 75885973266759680.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:39:25.392754: step 14870, loss = 75281181741940736.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:39:35.681682: step 14880, loss = 74681217760362496.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:39:46.083820: step 14890, loss = 74086012602548224.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 13:39:56.729439: step 14900, loss = 73495566268497920.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 13:40:07.063457: step 14910, loss = 72909835808538624.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:40:17.361201: step 14920, loss = 72328743913259008.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:40:27.640180: step 14930, loss = 71752316352462848.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:40:37.908428: step 14940, loss = 71180471521771520.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 13:40:48.165255: step 14950, loss = 70613196536283136.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:40:58.402750: step 14960, loss = 70050431266455552.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:41:08.658850: step 14970, loss = 69492149942484992.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:41:18.889154: step 14980, loss = 68938318204633088.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 13:41:29.104142: step 14990, loss = 68388905988128768.00 (125.3 examples/sec; 1.021 sec/batch)
2018-04-09 13:41:39.632219: step 15000, loss = 67843878933233664.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 13:41:49.833313: step 15010, loss = 67303198385242112.00 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 13:42:00.035023: step 15020, loss = 66766817099513856.00 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 13:42:10.283222: step 15030, loss = 66234705011277824.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 13:42:20.507576: step 15040, loss = 65706844940664832.00 (125.2 examples/sec; 1.022 sec/batch)
2018-04-09 13:42:30.729710: step 15050, loss = 65183176758132736.00 (125.2 examples/sec; 1.022 sec/batch)
2018-04-09 13:42:40.964029: step 15060, loss = 64663678988845056.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 13:42:51.173718: step 15070, loss = 64148330157965312.00 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 13:43:01.421948: step 15080, loss = 63637095905755136.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 13:43:11.666908: step 15090, loss = 63129928987574272.00 (124.9 examples/sec; 1.024 sec/batch)
2018-04-09 13:43:22.220842: step 15100, loss = 62626803633618944.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 13:43:32.432124: step 15110, loss = 62127689779118080.00 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 13:43:42.645306: step 15120, loss = 61632553064333312.00 (125.3 examples/sec; 1.021 sec/batch)
2018-04-09 13:43:52.855975: step 15130, loss = 61141354834558976.00 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 13:44:03.167644: step 15140, loss = 60654082204893184.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:44:13.437411: step 15150, loss = 60170692225662976.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:44:23.679951: step 15160, loss = 59691159127064576.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:44:33.871405: step 15170, loss = 59215435664457728.00 (125.6 examples/sec; 1.019 sec/batch)
2018-04-09 13:44:44.075554: step 15180, loss = 58743508952940544.00 (125.4 examples/sec; 1.020 sec/batch)
2018-04-09 13:44:54.296549: step 15190, loss = 58275340337807360.00 (125.2 examples/sec; 1.022 sec/batch)
2018-04-09 13:45:04.872432: step 15200, loss = 57810904049254400.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 13:45:15.128100: step 15210, loss = 57350174317477888.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:45:25.376160: step 15220, loss = 56893103897837568.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 13:45:35.651167: step 15230, loss = 56439692790333440.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 13:45:45.889607: step 15240, loss = 55989867980521472.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:45:56.130058: step 15250, loss = 55543659533172736.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:46:06.743360: step 15260, loss = 55101003023777792.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 13:46:17.046313: step 15270, loss = 54661876977500160.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:46:27.350115: step 15280, loss = 54226238444666880.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:46:37.592562: step 15290, loss = 53794083130310656.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:46:48.245599: step 15300, loss = 53365372379725824.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 13:46:58.533022: step 15310, loss = 52940054653304832.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:47:08.843564: step 15320, loss = 52518147130916864.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:47:19.153611: step 15330, loss = 52099598272954368.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:47:29.473063: step 15340, loss = 51684378014646272.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:47:39.757105: step 15350, loss = 51272460586188800.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:47:50.056539: step 15360, loss = 50863841692614656.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:48:00.365983: step 15370, loss = 50458461204381696.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:48:10.646242: step 15380, loss = 50056332006391808.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:48:20.875632: step 15390, loss = 49657406854004736.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 13:48:31.433832: step 15400, loss = 49261651387482112.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 13:48:41.630942: step 15410, loss = 48869048426954752.00 (125.5 examples/sec; 1.020 sec/batch)
2018-04-09 13:48:51.888691: step 15420, loss = 48479580792553472.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:49:02.205663: step 15430, loss = 48093205534605312.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 13:49:12.493873: step 15440, loss = 47709926948077568.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:49:22.760901: step 15450, loss = 47329693493362688.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 13:49:33.016088: step 15460, loss = 46952487990591488.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:49:43.360144: step 15470, loss = 46578301849829376.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 13:49:53.605622: step 15480, loss = 46207087826436096.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 13:50:03.948270: step 15490, loss = 45838828740542464.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 13:50:14.544195: step 15500, loss = 45473507412279296.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 13:50:24.794687: step 15510, loss = 45111098071842816.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 13:50:35.062558: step 15520, loss = 44751583539363840.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 13:50:45.335292: step 15530, loss = 44394929455104000.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:50:55.624114: step 15540, loss = 44041110049259520.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:51:05.986721: step 15550, loss = 43690116731895808.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:51:16.289145: step 15560, loss = 43341919438241792.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:51:26.552651: step 15570, loss = 42996488103526400.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 13:51:36.822732: step 15580, loss = 42653822727749632.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:51:47.096761: step 15590, loss = 42313884656205824.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:51:57.715806: step 15600, loss = 41976665298960384.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 13:52:08.005199: step 15610, loss = 41642121706340352.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:52:18.260286: step 15620, loss = 41310249583378432.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:52:28.520540: step 15630, loss = 40981023160270848.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:52:38.752288: step 15640, loss = 40654412372246528.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 13:52:48.993935: step 15650, loss = 40330412924338176.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 13:52:59.314421: step 15660, loss = 40008990456807424.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 13:53:09.676983: step 15670, loss = 39690127789785088.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:53:19.955420: step 15680, loss = 39373812038369280.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:53:30.202879: step 15690, loss = 39060017432756224.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 13:53:40.828832: step 15700, loss = 38748722498109440.00 (120.5 examples/sec; 1.063 sec/batch)
2018-04-09 13:53:51.163403: step 15710, loss = 38439910054559744.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:54:01.490929: step 15720, loss = 38133554332303360.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 13:54:11.858459: step 15730, loss = 37829646741405696.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 13:54:22.154009: step 15740, loss = 37528152922128384.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:54:32.437274: step 15750, loss = 37229072874471424.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:54:42.732711: step 15760, loss = 36932363648761856.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 13:54:53.001727: step 15770, loss = 36638025244999680.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:55:03.353026: step 15780, loss = 36346031893381120.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 13:55:13.629574: step 15790, loss = 36056366414036992.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 13:55:24.198838: step 15800, loss = 35769009479614464.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 13:55:34.428819: step 15810, loss = 35483941762760704.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 13:55:44.718640: step 15820, loss = 35201148231090176.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:55:54.989779: step 15830, loss = 34920607409766400.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:56:05.380381: step 15840, loss = 34642302118920192.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 13:56:15.691227: step 15850, loss = 34366217326166016.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 13:56:25.979115: step 15860, loss = 34092327261700096.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:56:36.288107: step 15870, loss = 33820621188104192.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:56:46.571261: step 15880, loss = 33551086220476416.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:56:56.865871: step 15890, loss = 33283690146562048.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 13:57:07.497024: step 15900, loss = 33018435113844736.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 13:57:17.752965: step 15910, loss = 32755288910069760.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 13:57:28.062895: step 15920, loss = 32494240797818880.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 13:57:38.332874: step 15930, loss = 32235275744706560.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:57:48.603076: step 15940, loss = 31978370128412672.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 13:57:58.907256: step 15950, loss = 31723515359002624.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:58:09.270388: step 15960, loss = 31470694256607232.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:58:19.587195: step 15970, loss = 31219881051422720.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 13:58:29.879790: step 15980, loss = 30971065006030848.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 13:58:40.148311: step 15990, loss = 30724233235529728.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 13:58:50.764839: step 16000, loss = 30479370707533824.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 13:59:01.043391: step 16010, loss = 30236464537141248.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 13:59:11.407008: step 16020, loss = 29995484659580928.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 13:59:21.710651: step 16030, loss = 29756433222336512.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 13:59:31.995774: step 16040, loss = 29519282308120576.00 (124.5 examples/sec; 1.029 sec/batch)
2018-04-09 13:59:42.407286: step 16050, loss = 29284021179514880.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 13:59:52.695601: step 16060, loss = 29050634804133888.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:00:03.061616: step 16070, loss = 28819108149592064.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:00:13.423705: step 16080, loss = 28589430478471168.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:00:23.693027: step 16090, loss = 28361582463418368.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:00:34.271675: step 16100, loss = 28135551219531776.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 14:00:44.477489: step 16110, loss = 27911319566942208.00 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 14:00:54.731852: step 16120, loss = 27688876768231424.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 14:01:05.028031: step 16130, loss = 27468201348562944.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:01:15.335147: step 16140, loss = 27249291160453120.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:01:25.659465: step 16150, loss = 27032122581581824.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:01:35.909937: step 16160, loss = 26816687022014464.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 14:01:46.151165: step 16170, loss = 26602965154398208.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 14:01:56.437853: step 16180, loss = 26390944093831168.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:02:06.797405: step 16190, loss = 26180619545346048.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:02:17.465875: step 16200, loss = 25971970034106368.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 14:02:27.764256: step 16210, loss = 25764982675210240.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:02:38.125157: step 16220, loss = 25559642436272128.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:02:48.450389: step 16230, loss = 25355938579873792.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 14:02:58.763444: step 16240, loss = 25153866811047936.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:03:09.148274: step 16250, loss = 24953388475088896.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:03:19.478913: step 16260, loss = 24754514309414912.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:03:29.811237: step 16270, loss = 24557237871575040.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:03:40.102467: step 16280, loss = 24361535539249152.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:03:50.387629: step 16290, loss = 24167392280051712.00 (124.5 examples/sec; 1.029 sec/batch)
2018-04-09 14:04:01.031718: step 16300, loss = 23974799504048128.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 14:04:11.366912: step 16310, loss = 23783722851500032.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:04:21.704120: step 16320, loss = 23594164469891072.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:04:31.989903: step 16330, loss = 23406135096639488.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:04:42.299064: step 16340, loss = 23219593929555968.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:04:52.620706: step 16350, loss = 23034540968640512.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:05:03.003352: step 16360, loss = 22850963328991232.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:05:13.357843: step 16370, loss = 22668848125706240.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 14:05:23.691361: step 16380, loss = 22488182473883648.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:05:33.981681: step 16390, loss = 22308962078556160.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:05:44.617488: step 16400, loss = 22131169759854592.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 14:05:54.948308: step 16410, loss = 21954792632877056.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:06:05.338024: step 16420, loss = 21779817812721664.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:06:15.701610: step 16430, loss = 21606238856937472.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:06:26.054534: step 16440, loss = 21434045028106240.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 14:06:36.367345: step 16450, loss = 21263225588809728.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:06:46.707109: step 16460, loss = 21093759064211456.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:06:57.025072: step 16470, loss = 20925654044246016.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 14:07:07.385171: step 16480, loss = 20758880464142336.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:07:17.733809: step 16490, loss = 20593440471384064.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:07:28.382403: step 16500, loss = 20429316886102016.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 14:07:38.694160: step 16510, loss = 20266503265845248.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:07:49.016625: step 16520, loss = 20104988873195520.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:07:59.356611: step 16530, loss = 19944756528283648.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:08:09.709847: step 16540, loss = 19785801936142336.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 14:08:20.045927: step 16550, loss = 19628114359353344.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:08:30.392007: step 16560, loss = 19471685207982080.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:08:40.723187: step 16570, loss = 19316503744610304.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:08:51.063664: step 16580, loss = 19162557084336128.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:09:01.395263: step 16590, loss = 19009836637224960.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:09:12.152484: step 16600, loss = 18858333813342208.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 14:09:22.489178: step 16610, loss = 18708040022753280.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:09:32.795734: step 16620, loss = 18558946675523584.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:09:43.172530: step 16630, loss = 18411036591783936.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 14:09:53.418434: step 16640, loss = 18264307624050688.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 14:10:03.787640: step 16650, loss = 18118749034905600.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:10:14.170463: step 16660, loss = 17974345791963136.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:10:24.518951: step 16670, loss = 17831100042706944.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:10:34.843254: step 16680, loss = 17688990312300544.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:10:45.122765: step 16690, loss = 17548014453260288.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:10:55.749117: step 16700, loss = 17408162801909760.00 (120.5 examples/sec; 1.063 sec/batch)
2018-04-09 14:11:06.111631: step 16710, loss = 17269424620830720.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:11:16.473095: step 16720, loss = 17131795615055872.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:11:26.770948: step 16730, loss = 16995259678457856.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:11:37.054261: step 16740, loss = 16859811442327552.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:11:47.313232: step 16750, loss = 16725441242988544.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 14:11:57.610824: step 16760, loss = 16592149080440832.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:12:07.961604: step 16770, loss = 16459913479847936.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:12:18.268787: step 16780, loss = 16328733367468032.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:12:28.610978: step 16790, loss = 16198601227108352.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:12:39.203014: step 16800, loss = 16069500952641536.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 14:12:49.438010: step 16810, loss = 15941431470325760.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 14:12:59.777860: step 16820, loss = 15814383116484608.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:13:10.104827: step 16830, loss = 15688351596150784.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:13:20.471219: step 16840, loss = 15563320803196928.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:13:30.785689: step 16850, loss = 15439287516397568.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:13:41.104040: step 16860, loss = 15316239924592640.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 14:13:51.393084: step 16870, loss = 15194174806556672.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:14:01.825794: step 16880, loss = 15073081424871424.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 14:14:12.204055: step 16890, loss = 14952954410827776.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:14:22.852459: step 16900, loss = 14833784100749312.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 14:14:33.201093: step 16910, loss = 14715564052185088.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:14:43.529921: step 16920, loss = 14598284601458688.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:14:53.858803: step 16930, loss = 14481943601086464.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:15:04.226193: step 16940, loss = 14366523871199232.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:15:14.591132: step 16950, loss = 14252027559280640.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:15:24.941377: step 16960, loss = 14138442854170624.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:15:35.282489: step 16970, loss = 14025764387160064.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:15:45.638517: step 16980, loss = 13913985715798016.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:15:55.983320: step 16990, loss = 13803091807698944.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 14:16:06.711080: step 17000, loss = 13693080515379200.00 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 14:16:17.072214: step 17010, loss = 13583946470129664.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:16:27.473546: step 17020, loss = 13475684303241216.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 14:16:37.872279: step 17030, loss = 13368286498521088.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 14:16:48.259928: step 17040, loss = 13261749834743808.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:16:58.592273: step 17050, loss = 13156053910814720.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:17:09.013532: step 17060, loss = 13051206242926592.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 14:17:19.319881: step 17070, loss = 12947192872435712.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:17:29.606143: step 17080, loss = 12844008430632960.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:17:39.870500: step 17090, loss = 12741644327583744.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 14:17:50.484079: step 17100, loss = 12640098415804416.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 14:18:00.815719: step 17110, loss = 12539361031618560.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:18:11.154862: step 17120, loss = 12439425732575232.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:18:21.501980: step 17130, loss = 12340286076223488.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:18:31.843338: step 17140, loss = 12241935620112384.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:18:42.162819: step 17150, loss = 12144373290500096.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:18:52.497969: step 17160, loss = 12047586202484736.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:19:02.878862: step 17170, loss = 11951571134840832.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:19:13.282568: step 17180, loss = 11856320571375616.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 14:19:23.649795: step 17190, loss = 11761832364605440.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:19:34.324770: step 17200, loss = 11668093629628416.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 14:19:44.668532: step 17210, loss = 11575102218960896.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 14:19:54.942647: step 17220, loss = 11482851690151936.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:20:05.246709: step 17230, loss = 11391337748234240.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 14:20:15.549177: step 17240, loss = 11300551803273216.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 14:20:25.836540: step 17250, loss = 11210488486559744.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:20:36.110139: step 17260, loss = 11121144576868352.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:20:46.365082: step 17270, loss = 11032513631748096.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 14:20:56.654167: step 17280, loss = 10944589208748032.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:21:06.988206: step 17290, loss = 10857363791675392.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:21:17.604741: step 17300, loss = 10770833085562880.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 14:21:27.899241: step 17310, loss = 10684993869185024.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 14:21:38.177420: step 17320, loss = 10599837552607232.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:21:48.444007: step 17330, loss = 10515359840862208.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 14:21:58.728773: step 17340, loss = 10431556438982656.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:22:09.069205: step 17350, loss = 10348419830775808.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:22:19.382276: step 17360, loss = 10265946795016192.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:22:29.676322: step 17370, loss = 10184130889252864.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 14:22:39.968661: step 17380, loss = 10102966744776704.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:22:50.255541: step 17390, loss = 10022448992878592.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:23:00.907538: step 17400, loss = 9942572264849408.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 14:23:11.243212: step 17410, loss = 9863334413205504.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:23:21.585235: step 17420, loss = 9784724700528640.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:23:31.919063: step 17430, loss = 9706745274302464.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:23:42.215163: step 17440, loss = 9629385397108736.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:23:52.538650: step 17450, loss = 9552641847721984.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:24:02.920268: step 17460, loss = 9476506036207616.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:24:13.293047: step 17470, loss = 9400980110049280.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:24:23.632701: step 17480, loss = 9326057626796032.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:24:33.949145: step 17490, loss = 9251731070255104.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 14:24:44.603817: step 17500, loss = 9178000440426496.00 (120.1 examples/sec; 1.065 sec/batch)
2018-04-09 14:24:54.898683: step 17510, loss = 9104852852408320.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 14:25:05.264200: step 17520, loss = 9032290453684224.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:25:15.592174: step 17530, loss = 8960306264932352.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:25:25.928597: step 17540, loss = 8888895991185408.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:25:36.251757: step 17550, loss = 8818054263734272.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:25:46.579007: step 17560, loss = 8747777324482560.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:25:56.914704: step 17570, loss = 8678060878462976.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:26:07.373051: step 17580, loss = 8608899556966400.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 14:26:17.680920: step 17590, loss = 8540289601896448.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:26:28.284017: step 17600, loss = 8472226718285824.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 14:26:38.521520: step 17610, loss = 8404705000554496.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 14:26:48.782947: step 17620, loss = 8337722301218816.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 14:26:59.073991: step 17630, loss = 8271273788440576.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:27:09.385547: step 17640, loss = 8205355704123392.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:27:19.687286: step 17650, loss = 8139961068945408.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:27:30.023699: step 17660, loss = 8075089346035712.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:27:40.308903: step 17670, loss = 8010733556072448.00 (124.5 examples/sec; 1.029 sec/batch)
2018-04-09 14:27:50.695391: step 17680, loss = 7946891551571968.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:28:01.039619: step 17690, loss = 7883557963825152.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 14:28:11.730990: step 17700, loss = 7820729034735616.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 14:28:22.007216: step 17710, loss = 7758398858723328.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:28:32.337462: step 17720, loss = 7696569046401024.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:28:42.611292: step 17730, loss = 7635230470963200.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:28:52.885121: step 17740, loss = 7574379911184384.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:29:03.212090: step 17750, loss = 7514013608968192.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:29:13.662408: step 17760, loss = 7454129953701888.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 14:29:23.959946: step 17770, loss = 7394723576676352.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:29:34.231875: step 17780, loss = 7335790719795200.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:29:44.652483: step 17790, loss = 7277326551220224.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 14:29:55.237764: step 17800, loss = 7219329460338688.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 14:30:05.575587: step 17810, loss = 7161792467828736.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:30:15.899451: step 17820, loss = 7104715036819456.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:30:26.169979: step 17830, loss = 7048093409214464.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:30:36.508832: step 17840, loss = 6991922216304640.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:30:46.877559: step 17850, loss = 6936198773735424.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:30:57.207287: step 17860, loss = 6880919323410432.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:31:07.529077: step 17870, loss = 6826081180975104.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:31:17.846499: step 17880, loss = 6771677903978496.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 14:31:28.121265: step 17890, loss = 6717710566162432.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:31:38.724240: step 17900, loss = 6664172725075968.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 14:31:48.979809: step 17910, loss = 6611061696364544.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 14:31:59.304243: step 17920, loss = 6558374258802688.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:32:09.641004: step 17930, loss = 6506105580552192.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:32:19.934307: step 17940, loss = 6454252977258496.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:32:30.256690: step 17950, loss = 6402815912050688.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:32:40.566973: step 17960, loss = 6351785794994176.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:32:50.856173: step 17970, loss = 6301166384185344.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:33:01.181296: step 17980, loss = 6250947479076864.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 14:33:11.485745: step 17990, loss = 6201125858443264.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 14:33:22.099546: step 18000, loss = 6151706890993664.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 14:33:32.374965: step 18010, loss = 6102683060535296.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:33:42.650315: step 18020, loss = 6054050072100864.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:33:52.903910: step 18030, loss = 6005804167593984.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 14:34:03.235774: step 18040, loss = 5957938367692800.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:34:13.590975: step 18050, loss = 5910453746139136.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:34:23.918712: step 18060, loss = 5863350302932992.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:34:34.256228: step 18070, loss = 5816623206236160.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:34:44.598486: step 18080, loss = 5770264402984960.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:34:54.965929: step 18090, loss = 5724278188146688.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:35:05.650001: step 18100, loss = 5678657045528576.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 14:35:15.977603: step 18110, loss = 5633400438259712.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:35:26.315141: step 18120, loss = 5588504071372800.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:35:36.661471: step 18130, loss = 5543965260513280.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:35:47.004876: step 18140, loss = 5499781858197504.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:35:57.364451: step 18150, loss = 5455950106329088.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:36:07.792299: step 18160, loss = 5412467857424384.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 14:36:18.132464: step 18170, loss = 5369331890257920.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:36:28.470359: step 18180, loss = 5326539520475136.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:36:38.810238: step 18190, loss = 5284089674334208.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:36:49.463000: step 18200, loss = 5241976446255104.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 14:36:59.769658: step 18210, loss = 5200200373108736.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:37:10.155911: step 18220, loss = 5158755549315072.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:37:20.510045: step 18230, loss = 5117641974874112.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 14:37:30.858747: step 18240, loss = 5076855891689472.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:37:41.206403: step 18250, loss = 5036394615406592.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:37:51.526336: step 18260, loss = 4996255998541824.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:38:01.881755: step 18270, loss = 4956437356740608.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:38:12.285644: step 18280, loss = 4916936005648384.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 14:38:22.609654: step 18290, loss = 4877748724039680.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:38:33.307561: step 18300, loss = 4838874975043584.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 14:38:43.651470: step 18310, loss = 4800311000563712.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 14:38:53.988313: step 18320, loss = 4762054116245504.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:39:04.372471: step 18330, loss = 4724102174605312.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:39:14.720769: step 18340, loss = 4686452491288576.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:39:25.041083: step 18350, loss = 4649102918811648.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:39:35.414059: step 18360, loss = 4612051309690880.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:39:45.807879: step 18370, loss = 4575294979571712.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:39:56.088308: step 18380, loss = 4538831780970496.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:40:06.398269: step 18390, loss = 4502658761097216.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:40:17.049687: step 18400, loss = 4466773772468224.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 14:40:27.345962: step 18410, loss = 4431174667599872.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:40:37.655548: step 18420, loss = 4395859567443968.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:40:47.901373: step 18430, loss = 4360826324516864.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 14:40:58.177698: step 18440, loss = 4326071449157632.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:41:08.515350: step 18450, loss = 4291594404495360.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:41:18.835067: step 18460, loss = 4257390895562752.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:41:29.107833: step 18470, loss = 4223461459230720.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:41:39.373290: step 18480, loss = 4189803142709248.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 14:41:49.629563: step 18490, loss = 4156410845724672.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 14:42:00.239969: step 18500, loss = 4123285642018816.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 14:42:10.595074: step 18510, loss = 4090424310366208.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:42:20.915227: step 18520, loss = 4057825240154112.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:42:31.201620: step 18530, loss = 4025485210157056.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:42:41.517497: step 18540, loss = 3993403146633216.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 14:42:51.812837: step 18550, loss = 3961577170534400.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:43:02.187095: step 18560, loss = 3930004597506048.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:43:12.522247: step 18570, loss = 3898684353806336.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:43:22.849408: step 18580, loss = 3867613755080704.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:43:33.160709: step 18590, loss = 3836789043232768.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:43:43.789453: step 18600, loss = 3806211828875264.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 14:43:54.065551: step 18610, loss = 3775877011734528.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:44:04.433735: step 18620, loss = 3745785397116928.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 14:44:14.764566: step 18630, loss = 3715931616313344.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:44:25.066612: step 18640, loss = 3686316743065600.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 14:44:35.374005: step 18650, loss = 3656937824583680.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:44:45.720778: step 18660, loss = 3627793250254848.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:44:56.279065: step 18670, loss = 3598880872595456.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 14:45:06.679634: step 18680, loss = 3570199617863680.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 14:45:16.993841: step 18690, loss = 3541745727963136.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:45:27.593591: step 18700, loss = 3513519202893824.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 14:45:37.881694: step 18710, loss = 3485517089865728.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:45:48.185384: step 18720, loss = 3457739657314304.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 14:45:58.524246: step 18730, loss = 3430183147143168.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:46:08.905522: step 18740, loss = 3402845948739584.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:46:19.206238: step 18750, loss = 3375727525232640.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:46:29.525806: step 18760, loss = 3348824386961408.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:46:39.805570: step 18770, loss = 3322134654877696.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:46:50.095034: step 18780, loss = 3295659134287872.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:47:00.423359: step 18790, loss = 3269393530224640.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:47:11.108705: step 18800, loss = 3243337037381632.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 14:47:21.376197: step 18810, loss = 3217489118887936.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 14:47:31.750667: step 18820, loss = 3191846821953536.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:47:42.110779: step 18830, loss = 3166409341272064.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:47:52.387226: step 18840, loss = 3141174260924416.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:48:02.746419: step 18850, loss = 3116139701862400.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:48:13.098879: step 18860, loss = 3091304858779648.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 14:48:23.391078: step 18870, loss = 3066668121063424.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:48:33.706750: step 18880, loss = 3042227878100992.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 14:48:43.986864: step 18890, loss = 3017983056150528.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:48:54.599642: step 18900, loss = 2993930433986560.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 14:49:04.933574: step 18910, loss = 2970068937867264.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:49:15.272097: step 18920, loss = 2946398299357184.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:49:25.552206: step 18930, loss = 2922917176279040.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:49:35.890456: step 18940, loss = 2899622615842816.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:49:46.300156: step 18950, loss = 2876514081177600.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 14:49:56.593047: step 18960, loss = 2853589424799744.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:50:06.997157: step 18970, loss = 2830847036096512.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 14:50:17.345519: step 18980, loss = 2808286109761536.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:50:27.658559: step 18990, loss = 2785904766746624.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:50:38.281748: step 19000, loss = 2763702738616320.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 14:50:48.591874: step 19010, loss = 2741676535709696.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:50:58.963064: step 19020, loss = 2719826694897664.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:51:09.350461: step 19030, loss = 2698150263390208.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:51:19.696645: step 19040, loss = 2676646972751872.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 14:51:30.028530: step 19050, loss = 2655315480805376.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:51:40.371127: step 19060, loss = 2634153103196160.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:51:50.692160: step 19070, loss = 2613159839924224.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:52:01.052541: step 19080, loss = 2592334348812288.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:52:11.408478: step 19090, loss = 2571673945505792.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:52:22.068916: step 19100, loss = 2551178630004736.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 14:52:32.372044: step 19110, loss = 2530846254825472.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 14:52:42.682534: step 19120, loss = 2510676819968000.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 14:52:52.954687: step 19130, loss = 2490666835771392.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:53:03.346690: step 19140, loss = 2470817107542016.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 14:53:13.687665: step 19150, loss = 2451125487796224.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:53:23.984945: step 19160, loss = 2431591439663104.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:53:34.283396: step 19170, loss = 2412211741917184.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:53:44.609868: step 19180, loss = 2392987468300288.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 14:53:54.936971: step 19190, loss = 2373916471328768.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:54:05.671206: step 19200, loss = 2354997677260800.00 (119.2 examples/sec; 1.073 sec/batch)
2018-04-09 14:54:16.030676: step 19210, loss = 2336229207048192.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 14:54:26.354604: step 19220, loss = 2317609986949120.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 14:54:36.719043: step 19230, loss = 2299139480092672.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 14:54:47.098749: step 19240, loss = 2280816075866112.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 14:54:57.513627: step 19250, loss = 2262638700527616.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 14:55:08.019854: step 19260, loss = 2244606414553088.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 14:55:18.414602: step 19270, loss = 2226717338894336.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 14:55:28.788606: step 19280, loss = 2208971205115904.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 14:55:39.046353: step 19290, loss = 2191366402605056.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 14:55:49.639915: step 19300, loss = 2173901857619968.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 14:55:59.928389: step 19310, loss = 2156576362201088.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:56:10.267333: step 19320, loss = 2139389647912960.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:56:20.553486: step 19330, loss = 2122339030401024.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:56:30.824131: step 19340, loss = 2105424643883008.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 14:56:41.100123: step 19350, loss = 2088645146181632.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 14:56:51.397885: step 19360, loss = 2071999195119616.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 14:57:01.725137: step 19370, loss = 2055485716955136.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:57:12.051594: step 19380, loss = 2039104309035008.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 14:57:22.319516: step 19390, loss = 2022853494964224.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 14:57:32.923481: step 19400, loss = 2006731932565504.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 14:57:43.171164: step 19410, loss = 1990739084967936.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 14:57:53.433493: step 19420, loss = 1974873609994240.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 14:58:03.738510: step 19430, loss = 1959134836555776.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 14:58:14.067155: step 19440, loss = 1943520617168896.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 14:58:24.319673: step 19450, loss = 1928031757139968.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 14:58:34.606693: step 19460, loss = 1912665974767616.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 14:58:44.885472: step 19470, loss = 1897422598963200.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 14:58:55.228543: step 19480, loss = 1882300555984896.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 14:59:05.580819: step 19490, loss = 1867299308961792.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 14:59:16.207281: step 19500, loss = 1852417649934336.00 (120.5 examples/sec; 1.063 sec/batch)
2018-04-09 14:59:26.442705: step 19510, loss = 1837654505160704.00 (125.1 examples/sec; 1.024 sec/batch)
2018-04-09 14:59:36.709961: step 19520, loss = 1823008935116800.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 14:59:47.117574: step 19530, loss = 1808480268713984.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 14:59:57.379349: step 19540, loss = 1794067432210432.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:00:07.745776: step 19550, loss = 1779768949211136.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 15:00:18.074247: step 19560, loss = 1765584417062912.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:00:28.359332: step 19570, loss = 1751513567330304.00 (124.5 examples/sec; 1.029 sec/batch)
2018-04-09 15:00:38.665029: step 19580, loss = 1737554252529664.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:00:48.915358: step 19590, loss = 1723706606878720.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:00:59.537218: step 19600, loss = 1709969288200192.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:01:09.834645: step 19610, loss = 1696341356969984.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:01:20.123150: step 19620, loss = 1682822276317184.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:01:30.387710: step 19630, loss = 1669410972499968.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:01:40.644915: step 19640, loss = 1656106237558784.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:01:50.912594: step 19650, loss = 1642907534622720.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:02:01.266475: step 19660, loss = 1629813655732224.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 15:02:11.643004: step 19670, loss = 1616824869322752.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 15:02:21.981301: step 19680, loss = 1603939430563840.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:02:32.261328: step 19690, loss = 1591156399931392.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:02:42.893441: step 19700, loss = 1578475643207680.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 15:02:53.171180: step 19710, loss = 1565895818215424.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:03:03.487223: step 19720, loss = 1553415582777344.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:03:13.797751: step 19730, loss = 1541034802675712.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:03:24.061559: step 19740, loss = 1528754148999168.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:03:34.322673: step 19750, loss = 1516571205828608.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:03:44.635172: step 19760, loss = 1504485570510848.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:03:54.921788: step 19770, loss = 1492495095562240.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:04:05.446770: step 19780, loss = 1480599646765056.00 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 15:04:15.767229: step 19790, loss = 1468799626772480.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:04:26.344344: step 19800, loss = 1457094230278144.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 15:04:36.576050: step 19810, loss = 1445481175580672.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 15:04:46.836013: step 19820, loss = 1433961536421888.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:04:57.112428: step 19830, loss = 1422533165318144.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 15:05:07.437413: step 19840, loss = 1411196062269440.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:05:17.811110: step 19850, loss = 1399949287751680.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:05:28.084399: step 19860, loss = 1388792573329408.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:05:38.338359: step 19870, loss = 1377724039954432.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 15:05:48.628949: step 19880, loss = 1366744090279936.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:05:58.928807: step 19890, loss = 1355851516346368.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:06:09.565097: step 19900, loss = 1345045781282816.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 15:06:19.895081: step 19910, loss = 1334325945565184.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:06:30.169106: step 19920, loss = 1323691874975744.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:06:40.444950: step 19930, loss = 1313142495772672.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 15:06:50.705014: step 19940, loss = 1302677002649600.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:07:00.973393: step 19950, loss = 1292295261388800.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:07:11.380931: step 19960, loss = 1281995929812992.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 15:07:21.830523: step 19970, loss = 1271779007922176.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 15:07:32.196206: step 19980, loss = 1261643421974528.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 15:07:42.555632: step 19990, loss = 1251588635099136.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:07:53.193938: step 20000, loss = 1241613707771904.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 15:08:03.572606: step 20010, loss = 1231718505775104.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:08:13.942411: step 20020, loss = 1221902223802368.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:08:24.300614: step 20030, loss = 1212164056547328.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:08:34.630242: step 20040, loss = 1202503601356800.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:08:44.952904: step 20050, loss = 1192919918706688.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:08:55.292228: step 20060, loss = 1183412874379264.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:09:05.671054: step 20070, loss = 1173981394632704.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:09:16.041531: step 20080, loss = 1164624942596096.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:09:26.351015: step 20090, loss = 1155343115616256.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:09:36.972004: step 20100, loss = 1146135511040000.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:09:47.460906: step 20110, loss = 1137000920907776.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 15:09:57.982513: step 20120, loss = 1127939747872768.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 15:10:08.272888: step 20130, loss = 1118950448431104.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:10:18.578792: step 20140, loss = 1110032754147328.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:10:28.841626: step 20150, loss = 1101185993932800.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:10:39.090897: step 20160, loss = 1092409966460928.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:10:49.327164: step 20170, loss = 1083703799316480.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 15:10:59.611868: step 20180, loss = 1075067022737408.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:11:09.900629: step 20190, loss = 1066499234070528.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:11:20.483621: step 20200, loss = 1057999560900608.00 (120.9 examples/sec; 1.058 sec/batch)
2018-04-09 15:11:30.688859: step 20210, loss = 1049567734792192.00 (125.4 examples/sec; 1.021 sec/batch)
2018-04-09 15:11:40.923032: step 20220, loss = 1041203084656640.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 15:11:51.161677: step 20230, loss = 1032905006514176.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 15:12:01.426262: step 20240, loss = 1024673030602752.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:12:11.715877: step 20250, loss = 1016506687160320.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:12:22.035194: step 20260, loss = 1008405506424832.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:12:32.286226: step 20270, loss = 1000369018634240.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:12:42.540255: step 20280, loss = 992396284264448.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 15:12:52.768532: step 20290, loss = 984487169097728.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 15:13:03.384311: step 20300, loss = 976641136263168.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 15:13:13.743399: step 20310, loss = 968857648889856.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:13:24.021839: step 20320, loss = 961136035889152.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:13:34.270952: step 20330, loss = 953476095934464.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:13:44.550242: step 20340, loss = 945877426372608.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:13:54.865130: step 20350, loss = 938339020570624.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:14:05.192050: step 20360, loss = 930860878528512.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:14:15.465769: step 20370, loss = 923442262048768.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:14:25.748717: step 20380, loss = 916082768478208.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:14:35.995568: step 20390, loss = 908781793837056.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:14:46.551455: step 20400, loss = 901539136798720.00 (121.3 examples/sec; 1.056 sec/batch)
2018-04-09 15:14:56.799263: step 20410, loss = 894354126274560.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:15:07.122626: step 20420, loss = 887226560937984.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:15:17.425290: step 20430, loss = 880155568373760.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:15:27.776391: step 20440, loss = 873141215690752.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:15:38.109946: step 20450, loss = 866182496256000.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:15:48.381743: step 20460, loss = 859279275851776.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:15:58.655524: step 20470, loss = 852430816280576.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:16:08.974561: step 20480, loss = 845637117542400.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:16:19.356778: step 20490, loss = 838897709875200.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:16:29.998965: step 20500, loss = 832211989299200.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 15:16:40.315694: step 20510, loss = 825579418943488.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:16:50.595803: step 20520, loss = 818999998808064.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:17:00.911702: step 20530, loss = 812472856477696.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:17:11.210830: step 20540, loss = 805997455081472.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:17:21.593166: step 20550, loss = 799573928837120.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:17:31.893425: step 20560, loss = 793201673764864.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:17:42.172164: step 20570, loss = 786880220102656.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:17:52.510438: step 20580, loss = 780609165197312.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:18:02.847575: step 20590, loss = 774387770851328.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:18:13.481738: step 20600, loss = 768216305500160.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 15:18:23.755362: step 20610, loss = 762093829619712.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:18:34.028692: step 20620, loss = 756020276101120.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:18:44.290274: step 20630, loss = 749995040964608.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:18:54.565730: step 20640, loss = 744017922883584.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 15:19:04.904753: step 20650, loss = 738088317878272.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:19:15.233535: step 20660, loss = 732205957513216.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:19:25.513067: step 20670, loss = 726370506244096.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:19:35.910354: step 20680, loss = 720581561417728.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 15:19:46.448689: step 20690, loss = 714838720380928.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 15:19:57.013154: step 20700, loss = 709141647589376.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 15:20:07.346976: step 20710, loss = 703490074607616.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:20:17.736299: step 20720, loss = 697883464564736.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:20:28.068903: step 20730, loss = 692321549025280.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:20:38.353409: step 20740, loss = 686804126662656.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:20:48.627823: step 20750, loss = 681330392170496.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:20:58.938870: step 20760, loss = 675900479766528.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:21:09.259339: step 20770, loss = 670513718362112.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:21:19.584233: step 20780, loss = 665169973739520.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:21:29.872775: step 20790, loss = 659868776136704.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:21:40.447585: step 20800, loss = 654609857118208.00 (121.0 examples/sec; 1.057 sec/batch)
2018-04-09 15:21:50.672523: step 20810, loss = 649392881139712.00 (125.2 examples/sec; 1.022 sec/batch)
2018-04-09 15:22:00.930219: step 20820, loss = 644217445548032.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:22:11.236108: step 20830, loss = 639083147689984.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:22:21.556942: step 20840, loss = 633989853347840.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:22:31.928772: step 20850, loss = 628937226977280.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:22:42.233314: step 20860, loss = 623924664598528.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:22:52.548524: step 20870, loss = 618952166211584.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:23:02.941354: step 20880, loss = 614019329163264.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:23:13.730584: step 20890, loss = 609125750800384.00 (118.6 examples/sec; 1.079 sec/batch)
2018-04-09 15:23:24.355215: step 20900, loss = 604271296905216.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:23:34.663484: step 20910, loss = 599455430606848.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:23:44.983627: step 20920, loss = 594677950578688.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:23:55.304766: step 20930, loss = 589938521276416.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:24:05.692916: step 20940, loss = 585236941373440.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:24:16.147268: step 20950, loss = 580572808216576.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 15:24:26.591610: step 20960, loss = 575945719152640.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 15:24:36.862219: step 20970, loss = 571355674181632.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:24:47.118214: step 20980, loss = 566802136432640.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:24:57.383261: step 20990, loss = 562284904579072.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:25:08.005639: step 21000, loss = 557803676631040.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:25:18.274356: step 21010, loss = 553358150598656.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:25:28.569061: step 21020, loss = 548948058046464.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 15:25:38.893890: step 21030, loss = 544573130539008.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:25:49.422296: step 21040, loss = 540233032531968.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 15:25:59.855728: step 21050, loss = 535927562698752.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 15:26:10.208829: step 21060, loss = 531656452603904.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 15:26:20.468810: step 21070, loss = 527419366703104.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:26:30.750910: step 21080, loss = 523215935897600.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:26:41.021722: step 21090, loss = 519046126632960.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:26:51.653728: step 21100, loss = 514909469147136.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 15:27:01.907049: step 21110, loss = 510805795667968.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 15:27:12.227933: step 21120, loss = 506734904868864.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:27:22.651470: step 21130, loss = 502696394096640.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 15:27:33.068663: step 21140, loss = 498690095579136.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 15:27:43.446667: step 21150, loss = 494715740880896.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:27:53.774588: step 21160, loss = 490773095120896.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:28:04.134309: step 21170, loss = 486861755645952.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:28:14.485658: step 21180, loss = 482981621792768.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:28:24.789207: step 21190, loss = 479132425125888.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:28:35.453422: step 21200, loss = 475313830100992.00 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 15:28:45.753940: step 21210, loss = 471525702500352.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:28:56.087277: step 21220, loss = 467767908106240.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:29:06.516024: step 21230, loss = 464039943602176.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 15:29:16.892557: step 21240, loss = 460341674770432.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 15:29:27.245007: step 21250, loss = 456672866729984.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 15:29:37.595392: step 21260, loss = 453033351708672.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:29:48.033397: step 21270, loss = 449422827716608.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 15:29:58.342673: step 21280, loss = 445841026318336.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:30:08.679715: step 21290, loss = 442287846850560.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:30:19.281593: step 21300, loss = 438762920214528.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 15:30:29.596663: step 21310, loss = 435266179301376.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:30:40.028106: step 21320, loss = 431797187903488.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 15:30:50.366118: step 21330, loss = 428355912466432.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:31:00.834090: step 21340, loss = 424942118109184.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 15:31:11.280384: step 21350, loss = 421555502841856.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 15:31:21.717941: step 21360, loss = 418195798228992.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 15:31:32.077981: step 21370, loss = 414862903607296.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:31:42.385020: step 21380, loss = 411556584095744.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:31:52.703527: step 21390, loss = 408276604813312.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:32:03.413749: step 21400, loss = 405022831542272.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 15:32:13.759427: step 21410, loss = 401794928738304.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:32:24.056965: step 21420, loss = 398592795738112.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:32:34.385786: step 21430, loss = 395416063442944.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:32:44.720282: step 21440, loss = 392264798961664.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:32:55.026598: step 21450, loss = 389138532532224.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:33:05.413514: step 21460, loss = 386037029273600.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:33:15.788521: step 21470, loss = 382960557621248.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 15:33:26.120563: step 21480, loss = 379908681367552.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:33:36.438376: step 21490, loss = 376881132077056.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:33:47.036984: step 21500, loss = 373877473542144.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 15:33:57.298592: step 21510, loss = 370897605099520.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:34:07.669523: step 21520, loss = 367941694521344.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:34:17.962829: step 21530, loss = 365009406263296.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:34:28.226667: step 21540, loss = 362100270563328.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:34:38.512793: step 21550, loss = 359214589411328.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:34:48.783669: step 21560, loss = 356351658164224.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:34:59.090457: step 21570, loss = 353511745257472.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:35:09.461110: step 21580, loss = 350694414483456.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:35:20.044725: step 21590, loss = 347899397406720.00 (120.9 examples/sec; 1.058 sec/batch)
2018-04-09 15:35:30.634271: step 21600, loss = 345126794690560.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 15:35:40.912672: step 21610, loss = 342376304345088.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:35:51.215221: step 21620, loss = 339647657934848.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:36:01.508418: step 21630, loss = 336940788350976.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:36:11.806638: step 21640, loss = 334255494266880.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:36:22.072502: step 21650, loss = 331591540801536.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:36:32.375430: step 21660, loss = 328948894400512.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:36:42.731012: step 21670, loss = 326327253073920.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:36:52.999280: step 21680, loss = 323726549712896.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:37:03.363803: step 21690, loss = 321146549436416.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 15:37:13.985285: step 21700, loss = 318587151581184.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:37:24.222170: step 21710, loss = 316048121266176.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 15:37:34.526518: step 21720, loss = 313529290719232.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:37:44.799644: step 21730, loss = 311030559277056.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:37:55.076147: step 21740, loss = 308551759167488.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 15:38:05.413250: step 21750, loss = 306092689063936.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:38:15.780154: step 21760, loss = 303653248303104.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 15:38:26.032208: step 21770, loss = 301233235558400.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 15:38:36.331695: step 21780, loss = 298832516612096.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:38:46.668296: step 21790, loss = 296450923692032.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:38:57.384960: step 21800, loss = 294088289026048.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 15:39:07.826084: step 21810, loss = 291744545505280.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 15:39:18.140120: step 21820, loss = 289419458248704.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:39:28.406310: step 21830, loss = 287112859484160.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:39:38.727645: step 21840, loss = 284824682102784.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:39:49.118992: step 21850, loss = 282554691223552.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:39:59.430736: step 21860, loss = 280302886846464.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:40:09.764902: step 21870, loss = 278068983758848.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:40:20.055638: step 21880, loss = 275852847742976.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:40:30.315551: step 21890, loss = 273654394912768.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:40:40.880906: step 21900, loss = 271473457496064.00 (121.2 examples/sec; 1.057 sec/batch)
2018-04-09 15:40:51.184985: step 21910, loss = 269309918052352.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:41:01.545495: step 21920, loss = 267163592032256.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 15:41:11.866340: step 21930, loss = 265034345218048.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:41:22.136896: step 21940, loss = 262922060169216.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:41:32.416170: step 21950, loss = 260826602668032.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:41:42.727749: step 21960, loss = 258747888828416.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:41:53.049629: step 21970, loss = 256685767655424.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:42:03.383594: step 21980, loss = 254640088154112.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:42:13.711443: step 21990, loss = 252610699329536.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:42:24.303921: step 22000, loss = 250597500518400.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 15:42:34.607989: step 22010, loss = 248600323948544.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:42:44.956300: step 22020, loss = 246619035402240.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:42:55.279356: step 22030, loss = 244653567770624.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:43:05.719291: step 22040, loss = 242703736504320.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 15:43:16.140652: step 22050, loss = 240769457717248.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 15:43:26.523356: step 22060, loss = 238850613968896.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:43:36.916940: step 22070, loss = 236947054264320.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:43:47.372275: step 22080, loss = 235058677940224.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 15:43:57.776219: step 22090, loss = 233185300447232.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 15:44:08.543611: step 22100, loss = 231326921785344.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 15:44:18.904443: step 22110, loss = 229483340627968.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 15:44:29.252109: step 22120, loss = 227654456311808.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:44:39.586512: step 22130, loss = 225840101064704.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:44:49.908598: step 22140, loss = 224040207777792.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:45:00.260782: step 22150, loss = 222254659010560.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 15:45:10.636127: step 22160, loss = 220483421208576.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 15:45:20.953994: step 22170, loss = 218726225936384.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:45:31.293262: step 22180, loss = 216982972530688.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:45:41.600296: step 22190, loss = 215253761654784.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:45:52.215477: step 22200, loss = 213538324873216.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 15:46:02.575570: step 22210, loss = 211836561522688.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:46:12.915986: step 22220, loss = 210148303831040.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:46:23.201189: step 22230, loss = 208473451134976.00 (124.5 examples/sec; 1.029 sec/batch)
2018-04-09 15:46:33.459079: step 22240, loss = 206812020211712.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 15:46:43.753562: step 22250, loss = 205163792957440.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 15:46:54.028516: step 22260, loss = 203528702263296.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:47:04.346687: step 22270, loss = 201906664243200.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:47:14.683201: step 22280, loss = 200297511124992.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:47:24.947204: step 22290, loss = 198701242908672.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 15:47:35.642979: step 22300, loss = 197117641490432.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 15:47:46.148904: step 22310, loss = 195546673315840.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 15:47:56.719413: step 22320, loss = 193988237721600.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 15:48:07.359486: step 22330, loss = 192442250821632.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 15:48:17.884560: step 22340, loss = 190908528066560.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 15:48:28.376960: step 22350, loss = 189387019124736.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 15:48:38.866288: step 22360, loss = 187877690441728.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 15:48:49.335163: step 22370, loss = 186380391022592.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 15:48:59.958050: step 22380, loss = 184895020204032.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:49:10.552190: step 22390, loss = 183421477322752.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 15:49:21.386626: step 22400, loss = 181959661715456.00 (118.1 examples/sec; 1.083 sec/batch)
2018-04-09 15:49:31.913966: step 22410, loss = 180509539827712.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 15:49:42.423574: step 22420, loss = 179070927110144.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 15:49:53.112948: step 22430, loss = 177643773231104.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 15:50:03.976376: step 22440, loss = 176227994304512.00 (117.8 examples/sec; 1.086 sec/batch)
2018-04-09 15:50:14.579933: step 22450, loss = 174823523221504.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 15:50:25.049734: step 22460, loss = 173430208987136.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 15:50:35.371890: step 22470, loss = 172048085155840.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:50:45.706804: step 22480, loss = 170676900069376.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:50:56.005567: step 22490, loss = 169316670504960.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:51:06.723080: step 22500, loss = 167967295799296.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 15:51:17.012424: step 22510, loss = 166628641734656.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:51:27.318588: step 22520, loss = 165300657979392.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:51:37.612194: step 22530, loss = 163983277424640.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 15:51:47.905163: step 22540, loss = 162676399407104.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:51:58.209691: step 22550, loss = 161379923263488.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:52:08.579881: step 22560, loss = 160093781884928.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 15:52:18.933803: step 22570, loss = 158817857830912.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 15:52:29.233713: step 22580, loss = 157552151101440.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 15:52:39.542021: step 22590, loss = 156296510701568.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:52:50.156865: step 22600, loss = 155050869522432.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 15:53:00.474351: step 22610, loss = 153815160455168.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:53:10.837848: step 22620, loss = 152589282836480.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 15:53:21.156487: step 22630, loss = 151373236666368.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:53:31.483491: step 22640, loss = 150166820618240.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 15:53:41.806784: step 22650, loss = 148970051469312.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:53:52.122766: step 22660, loss = 147782862110720.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 15:54:02.556161: step 22670, loss = 146605051215872.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 15:54:12.940126: step 22680, loss = 145436652339200.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:54:23.223568: step 22690, loss = 144277564817408.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:54:33.847986: step 22700, loss = 143127721541632.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 15:54:44.158718: step 22710, loss = 141987055403008.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:54:54.518751: step 22720, loss = 140855448961024.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 15:55:05.000887: step 22730, loss = 139732893827072.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 15:55:15.383569: step 22740, loss = 138619255783424.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 15:55:25.690254: step 22750, loss = 137514501275648.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 15:55:36.009537: step 22760, loss = 136418554806272.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:55:46.275589: step 22770, loss = 135331349266432.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:55:56.618161: step 22780, loss = 134252817547264.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:56:07.028932: step 22790, loss = 133182867374080.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 15:56:17.672180: step 22800, loss = 132121431638016.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 15:56:27.940007: step 22810, loss = 131068460007424.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 15:56:38.241939: step 22820, loss = 130023910539264.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:56:48.527716: step 22830, loss = 128987657404416.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:56:58.816332: step 22840, loss = 127959658659840.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:57:09.162025: step 22850, loss = 126939880751104.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 15:57:19.501752: step 22860, loss = 125928231403520.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:57:29.776125: step 22870, loss = 124924609953792.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 15:57:40.096795: step 22880, loss = 123929008013312.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 15:57:50.398777: step 22890, loss = 122941324918784.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 15:58:01.060200: step 22900, loss = 121961535504384.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 15:58:11.402593: step 22910, loss = 120989530718208.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:58:21.744910: step 22920, loss = 120025268617216.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:58:32.134342: step 22930, loss = 119068698869760.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:58:42.412166: step 22940, loss = 118119771144192.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 15:58:52.702119: step 22950, loss = 117178393165824.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 15:59:03.091584: step 22960, loss = 116244497825792.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 15:59:13.431400: step 22970, loss = 115318051569664.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 15:59:23.741611: step 22980, loss = 114399004065792.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 15:59:33.983515: step 22990, loss = 113487279816704.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 15:59:44.535569: step 23000, loss = 112582828490752.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 15:59:54.857457: step 23010, loss = 111685591367680.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:00:05.268853: step 23020, loss = 110795501338624.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 16:00:15.658913: step 23030, loss = 109912491294720.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:00:26.008653: step 23040, loss = 109036527681536.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:00:36.344677: step 23050, loss = 108167551778816.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:00:46.670338: step 23060, loss = 107305496477696.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 16:00:56.975991: step 23070, loss = 106450294669312.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:01:07.371752: step 23080, loss = 105601921187840.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 16:01:17.673606: step 23090, loss = 104760308924416.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 16:01:28.276471: step 23100, loss = 103925407547392.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 16:01:38.551406: step 23110, loss = 103097149947904.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:01:48.902548: step 23120, loss = 102275519348736.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:01:59.247093: step 23130, loss = 101460415086592.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 16:02:09.611634: step 23140, loss = 100651786829824.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:02:19.968059: step 23150, loss = 99849651355648.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:02:30.282127: step 23160, loss = 99053840891904.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:02:40.625808: step 23170, loss = 98264464490496.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 16:02:50.899203: step 23180, loss = 97481320824832.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:03:01.181465: step 23190, loss = 96704393117696.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:03:11.883349: step 23200, loss = 95933639426048.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 16:03:22.125251: step 23210, loss = 95169126858752.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 16:03:32.389021: step 23220, loss = 94410712809472.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 16:03:42.657333: step 23230, loss = 93658237894656.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 16:03:52.915846: step 23240, loss = 92911769223168.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:04:03.244043: step 23250, loss = 92171298406400.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:04:13.612301: step 23260, loss = 91436749946880.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:04:23.914575: step 23270, loss = 90707998015488.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 16:04:34.208081: step 23280, loss = 89985118109696.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:04:44.512372: step 23290, loss = 89267942457344.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 16:04:55.126583: step 23300, loss = 88556521390080.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 16:05:05.489123: step 23310, loss = 87850745856000.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:05:15.865206: step 23320, loss = 87150590689280.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 16:05:26.151266: step 23330, loss = 86456030724096.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:05:36.438875: step 23340, loss = 85767015628800.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:05:46.721220: step 23350, loss = 85083478294528.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:05:57.014733: step 23360, loss = 84405401944064.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:06:07.344283: step 23370, loss = 83732719468544.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:06:17.730896: step 23380, loss = 83065405702144.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:06:28.000909: step 23390, loss = 82403393536000.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:06:38.619507: step 23400, loss = 81746674581504.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 16:06:48.885306: step 23410, loss = 81095164952576.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 16:06:59.205175: step 23420, loss = 80448873037824.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:07:09.550800: step 23430, loss = 79807706562560.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:07:19.882215: step 23440, loss = 79171673915392.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:07:30.161806: step 23450, loss = 78540707987456.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:07:40.494832: step 23460, loss = 77914775224320.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:07:50.791830: step 23470, loss = 77293808517120.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:08:01.156977: step 23480, loss = 76677799477248.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:08:11.510424: step 23490, loss = 76066697773056.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:08:22.123258: step 23500, loss = 75460469850112.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 16:08:32.343452: step 23510, loss = 74859073765376.00 (125.2 examples/sec; 1.022 sec/batch)
2018-04-09 16:08:42.613066: step 23520, loss = 74262484353024.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:08:52.937464: step 23530, loss = 73670642892800.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:09:03.270931: step 23540, loss = 73083499053056.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:09:13.600803: step 23550, loss = 72501061222400.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:09:23.887105: step 23560, loss = 71923245514752.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:09:34.159067: step 23570, loss = 71350035152896.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:09:44.434631: step 23580, loss = 70781404971008.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 16:09:54.763530: step 23590, loss = 70217279471616.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:10:05.404657: step 23600, loss = 69657679626240.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 16:10:15.699458: step 23610, loss = 69102521548800.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 16:10:25.966762: step 23620, loss = 68551801044992.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 16:10:36.236707: step 23630, loss = 68005459394560.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:10:46.529678: step 23640, loss = 67463488208896.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:10:56.837008: step 23650, loss = 66925816184832.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:11:07.242058: step 23660, loss = 66392455905280.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 16:11:17.601187: step 23670, loss = 65863336067072.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:11:27.869897: step 23680, loss = 65338435698688.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 16:11:38.181775: step 23690, loss = 64817704468480.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:11:48.776659: step 23700, loss = 64301129793536.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 16:11:59.121070: step 23710, loss = 63788669730816.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 16:12:09.566430: step 23720, loss = 63280290725888.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 16:12:19.918845: step 23730, loss = 62775963418624.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:12:30.247170: step 23740, loss = 62275666837504.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:12:40.601329: step 23750, loss = 61779342262272.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:12:50.879573: step 23760, loss = 61286985498624.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:13:01.147044: step 23770, loss = 60798546214912.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 16:13:11.484293: step 23780, loss = 60313999245312.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:13:21.772282: step 23790, loss = 59833323618304.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:13:32.347961: step 23800, loss = 59356473196544.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 16:13:42.606482: step 23810, loss = 58883414425600.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:13:52.946358: step 23820, loss = 58414130528256.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:14:03.310463: step 23830, loss = 57948592144384.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:14:13.695289: step 23840, loss = 57486753136640.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 16:14:24.002699: step 23850, loss = 57028588339200.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:14:34.321962: step 23860, loss = 56574097752064.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:14:44.620586: step 23870, loss = 56123218460672.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:14:54.918004: step 23880, loss = 55675929493504.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:15:05.285769: step 23890, loss = 55232214073344.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:15:15.945621: step 23900, loss = 54792026062848.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 16:15:26.231839: step 23910, loss = 54355361267712.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:15:36.565094: step 23920, loss = 53922165161984.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:15:46.892176: step 23930, loss = 53492416774144.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:15:57.240557: step 23940, loss = 53066099326976.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:16:07.657808: step 23950, loss = 52643179266048.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 16:16:18.057232: step 23960, loss = 52223639814144.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 16:16:28.511239: step 23970, loss = 51807430639616.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 16:16:38.925069: step 23980, loss = 51394547548160.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 16:16:49.260773: step 23990, loss = 50984948596736.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:16:59.949230: step 24000, loss = 50578612813824.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 16:17:10.325506: step 24010, loss = 50175515033600.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 16:17:20.681978: step 24020, loss = 49775638478848.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:17:30.997029: step 24030, loss = 49378932817920.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:17:41.319403: step 24040, loss = 48985389662208.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:17:51.638623: step 24050, loss = 48594992234496.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:18:01.963472: step 24060, loss = 48207715368960.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:18:12.355104: step 24070, loss = 47823517122560.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:18:22.672266: step 24080, loss = 47442372329472.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:18:32.935305: step 24090, loss = 47064272601088.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 16:18:43.527844: step 24100, loss = 46689184382976.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 16:18:53.820004: step 24110, loss = 46317082509312.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:19:04.211579: step 24120, loss = 45947950202880.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:19:14.605451: step 24130, loss = 45581766492160.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 16:19:25.035747: step 24140, loss = 45218485239808.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 16:19:35.388666: step 24150, loss = 44858114834432.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:19:45.725248: step 24160, loss = 44500600750080.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:19:56.057781: step 24170, loss = 44145955569664.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:20:06.406960: step 24180, loss = 43794112184320.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:20:16.708143: step 24190, loss = 43445091565568.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:20:27.312601: step 24200, loss = 43098851770368.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 16:20:37.552566: step 24210, loss = 42755367632896.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 16:20:47.844741: step 24220, loss = 42414618181632.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:20:58.118648: step 24230, loss = 42076582445056.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:21:08.469233: step 24240, loss = 41741239451648.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:21:18.775375: step 24250, loss = 41408580812800.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:21:29.124741: step 24260, loss = 41078564585472.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:21:39.408545: step 24270, loss = 40751186575360.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:21:49.715832: step 24280, loss = 40426413228032.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:22:00.001453: step 24290, loss = 40104227766272.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:22:10.666270: step 24300, loss = 39784613412864.00 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 16:22:20.921935: step 24310, loss = 39467540807680.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:22:31.208360: step 24320, loss = 39152993173504.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:22:41.485681: step 24330, loss = 38840953733120.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:22:52.092601: step 24340, loss = 38531405709312.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 16:23:02.455016: step 24350, loss = 38224319741952.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:23:12.778438: step 24360, loss = 37919691636736.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:23:23.003578: step 24370, loss = 37617479450624.00 (125.2 examples/sec; 1.023 sec/batch)
2018-04-09 16:23:33.295582: step 24380, loss = 37317691572224.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:23:43.563560: step 24390, loss = 37020269281280.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 16:23:54.128976: step 24400, loss = 36725237743616.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 16:24:04.412318: step 24410, loss = 36432550821888.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:24:14.711057: step 24420, loss = 36142200127488.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:24:24.980489: step 24430, loss = 35854156300288.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:24:35.229201: step 24440, loss = 35568406757376.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 16:24:45.512041: step 24450, loss = 35284938915840.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:24:55.784351: step 24460, loss = 35003735998464.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:25:06.168201: step 24470, loss = 34724766547968.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 16:25:16.553831: step 24480, loss = 34448017981440.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:25:26.810131: step 24490, loss = 34173477715968.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:25:37.380200: step 24500, loss = 33901122682880.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 16:25:47.635125: step 24510, loss = 33630946590720.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 16:25:57.936022: step 24520, loss = 33362913787904.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:26:08.293007: step 24530, loss = 33097026371584.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:26:18.606249: step 24540, loss = 32833252884480.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:26:28.927005: step 24550, loss = 32571584937984.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:26:39.224711: step 24560, loss = 32311991074816.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:26:49.509336: step 24570, loss = 32054471294976.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:26:59.847658: step 24580, loss = 31799013015552.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:27:10.217584: step 24590, loss = 31545584779264.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 16:27:20.826382: step 24600, loss = 31294178197504.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 16:27:31.105229: step 24610, loss = 31044778590208.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:27:41.391615: step 24620, loss = 30797362888704.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:27:51.648545: step 24630, loss = 30551912218624.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:28:01.984214: step 24640, loss = 30308426579968.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:28:12.292065: step 24650, loss = 30066880806912.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:28:22.564250: step 24660, loss = 29827258122240.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:28:32.821350: step 24670, loss = 29589545943040.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:28:43.062171: step 24680, loss = 29353725394944.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 16:28:53.297038: step 24690, loss = 29119785992192.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 16:29:03.936596: step 24700, loss = 28887713054720.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 16:29:14.175727: step 24710, loss = 28657489805312.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 16:29:24.408955: step 24720, loss = 28429097369600.00 (125.1 examples/sec; 1.023 sec/batch)
2018-04-09 16:29:34.693006: step 24730, loss = 28202529456128.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:29:44.951381: step 24740, loss = 27977765093376.00 (124.8 examples/sec; 1.026 sec/batch)
2018-04-09 16:29:55.317592: step 24750, loss = 27754787504128.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:30:05.692732: step 24760, loss = 27533592494080.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 16:30:15.999998: step 24770, loss = 27314159091712.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:30:26.296450: step 24780, loss = 27096474714112.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:30:36.643092: step 24790, loss = 26880526778368.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:30:47.232305: step 24800, loss = 26666296410112.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 16:30:57.502528: step 24810, loss = 26453771026432.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:31:07.832308: step 24820, loss = 26242946433024.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:31:18.122748: step 24830, loss = 26033799561216.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:31:28.366855: step 24840, loss = 25826319925248.00 (124.9 examples/sec; 1.024 sec/batch)
2018-04-09 16:31:38.673403: step 24850, loss = 25620490747904.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:31:48.976203: step 24860, loss = 25416305737728.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 16:31:59.268328: step 24870, loss = 25213743923200.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:32:09.623121: step 24880, loss = 25012799012864.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:32:19.899388: step 24890, loss = 24813450035200.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 16:32:30.477192: step 24900, loss = 24615701184512.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 16:32:40.740350: step 24910, loss = 24419525197824.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 16:32:50.982855: step 24920, loss = 24224905297920.00 (125.0 examples/sec; 1.024 sec/batch)
2018-04-09 16:33:01.312673: step 24930, loss = 24031826804736.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:33:11.723064: step 24940, loss = 23840287621120.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 16:33:22.051014: step 24950, loss = 23650279358464.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:33:32.416964: step 24960, loss = 23461778948096.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:33:42.764046: step 24970, loss = 23274786390016.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:33:53.098669: step 24980, loss = 23089291198464.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:34:03.462727: step 24990, loss = 22905282887680.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:34:14.158417: step 25000, loss = 22722734194688.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 16:34:24.481777: step 25010, loss = 22541643022336.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:34:34.843754: step 25020, loss = 22361990496256.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:34:45.165248: step 25030, loss = 22183772422144.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:34:55.521613: step 25040, loss = 22006972022784.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:35:05.962159: step 25050, loss = 21831587201024.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 16:35:16.336170: step 25060, loss = 21657592791040.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 16:35:26.648974: step 25070, loss = 21484990889984.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:35:36.975712: step 25080, loss = 21313768914944.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 16:35:47.261167: step 25090, loss = 21143903797248.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:35:57.919293: step 25100, loss = 20975393439744.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 16:36:08.283152: step 25110, loss = 20808225259520.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:36:18.632975: step 25120, loss = 20642390867968.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:36:28.951077: step 25130, loss = 20477875585024.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:36:39.315620: step 25140, loss = 20314675216384.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:36:49.651101: step 25150, loss = 20152775081984.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:36:59.988288: step 25160, loss = 19992160501760.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:37:10.373165: step 25170, loss = 19832831475712.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 16:37:20.722113: step 25180, loss = 19674767032320.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:37:31.062464: step 25190, loss = 19517965074432.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:37:41.719050: step 25200, loss = 19362408824832.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 16:37:52.075961: step 25210, loss = 19208100380672.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:38:02.536610: step 25220, loss = 19055016673280.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 16:38:12.954738: step 25230, loss = 18903153508352.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 16:38:23.339875: step 25240, loss = 18752506691584.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 16:38:33.731702: step 25250, loss = 18603055251456.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:38:44.083813: step 25260, loss = 18454794993664.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:38:54.381341: step 25270, loss = 18307713335296.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:39:04.813683: step 25280, loss = 18161803984896.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 16:39:15.182855: step 25290, loss = 18017060651008.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 16:39:25.838055: step 25300, loss = 17873468653568.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 16:39:36.145246: step 25310, loss = 17731025895424.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:39:46.507285: step 25320, loss = 17589715599360.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:39:56.957305: step 25330, loss = 17449529376768.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 16:40:07.393032: step 25340, loss = 17310463033344.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 16:40:17.761159: step 25350, loss = 17172502937600.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:40:28.081188: step 25360, loss = 17035643846656.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:40:38.384434: step 25370, loss = 16899875274752.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 16:40:48.673792: step 25380, loss = 16765189881856.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:40:58.983408: step 25390, loss = 16631575085056.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:41:09.666700: step 25400, loss = 16499027738624.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 16:41:20.040418: step 25410, loss = 16367533162496.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 16:41:30.339702: step 25420, loss = 16237089259520.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:41:40.675476: step 25430, loss = 16107688689664.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:41:51.051026: step 25440, loss = 15979312578560.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 16:42:01.391813: step 25450, loss = 15851964071936.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:42:11.778504: step 25460, loss = 15725628489728.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:42:22.125735: step 25470, loss = 15600297443328.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:42:32.441275: step 25480, loss = 15475969884160.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:42:42.776036: step 25490, loss = 15352629035008.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:42:53.393797: step 25500, loss = 15230272798720.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 16:43:03.693965: step 25510, loss = 15108894883840.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:43:14.032388: step 25520, loss = 14988482707456.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:43:24.488897: step 25530, loss = 14869028929536.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 16:43:34.994331: step 25540, loss = 14750526210048.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 16:43:45.331676: step 25550, loss = 14632972451840.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:43:55.666749: step 25560, loss = 14516351926272.00 (123.9 examples/sec; 1.034 sec/batch)
2018-04-09 16:44:06.076352: step 25570, loss = 14400661487616.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 16:44:16.415882: step 25580, loss = 14285890650112.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:44:26.701615: step 25590, loss = 14172036268032.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:44:37.344741: step 25600, loss = 14059091001344.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 16:44:47.651438: step 25610, loss = 13947046461440.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:44:57.999344: step 25620, loss = 13835890065408.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:45:08.365811: step 25630, loss = 13725624958976.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:45:18.686900: step 25640, loss = 13616233316352.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:45:28.991648: step 25650, loss = 13507711991808.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 16:45:39.322543: step 25660, loss = 13400067276800.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:45:49.634596: step 25670, loss = 13293277151232.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:45:59.954450: step 25680, loss = 13187330080768.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:46:10.312105: step 25690, loss = 13082229211136.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:46:20.948327: step 25700, loss = 12977968250880.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 16:46:31.218238: step 25710, loss = 12874540908544.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:46:41.509537: step 25720, loss = 12771934601216.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:46:51.821499: step 25730, loss = 12670143037440.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:47:02.151007: step 25740, loss = 12569167265792.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:47:12.495362: step 25750, loss = 12468994703360.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 16:47:22.796049: step 25760, loss = 12369621155840.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:47:33.145033: step 25770, loss = 12271040331776.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:47:43.481597: step 25780, loss = 12173243842560.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:47:53.837493: step 25790, loss = 12076226445312.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:48:04.671016: step 25800, loss = 11979980800000.00 (118.2 examples/sec; 1.083 sec/batch)
2018-04-09 16:48:15.090036: step 25810, loss = 11884505858048.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 16:48:25.493857: step 25820, loss = 11789790085120.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 16:48:35.870667: step 25830, loss = 11695832432640.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 16:48:46.187752: step 25840, loss = 11602621366272.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:48:56.502509: step 25850, loss = 11510150594560.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:49:06.916630: step 25860, loss = 11418418020352.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 16:49:17.268608: step 25870, loss = 11327417352192.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 16:49:27.613587: step 25880, loss = 11237141250048.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 16:49:37.957151: step 25890, loss = 11147588665344.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 16:49:48.607534: step 25900, loss = 11058743869440.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 16:49:59.365107: step 25910, loss = 10970608959488.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 16:50:10.073923: step 25920, loss = 10883177644032.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 16:50:20.458024: step 25930, loss = 10796444680192.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 16:50:30.774511: step 25940, loss = 10710401679360.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:50:41.133373: step 25950, loss = 10625041301504.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:50:51.482911: step 25960, loss = 10540362498048.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:51:01.865248: step 25970, loss = 10456360026112.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 16:51:12.276271: step 25980, loss = 10373026545664.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 16:51:22.621677: step 25990, loss = 10290356813824.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 16:51:33.265965: step 26000, loss = 10208344539136.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 16:51:43.596025: step 26010, loss = 10126987624448.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:51:53.983277: step 26020, loss = 10046277681152.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 16:52:04.391137: step 26030, loss = 9966213660672.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 16:52:14.761799: step 26040, loss = 9886786125824.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 16:52:25.091359: step 26050, loss = 9807994028032.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:52:35.422311: step 26060, loss = 9729825832960.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:52:45.758089: step 26070, loss = 9652281540608.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 16:52:56.085463: step 26080, loss = 9575354859520.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:53:06.485047: step 26090, loss = 9499043692544.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 16:53:17.172400: step 26100, loss = 9423339651072.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 16:53:27.470360: step 26110, loss = 9348237492224.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:53:37.789912: step 26120, loss = 9273733021696.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:53:48.083565: step 26130, loss = 9199823093760.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 16:53:58.446159: step 26140, loss = 9126503514112.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:54:08.860933: step 26150, loss = 9053767991296.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 16:54:19.222288: step 26160, loss = 8981613379584.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 16:54:29.519604: step 26170, loss = 8910032338944.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:54:39.843057: step 26180, loss = 8839022772224.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:54:50.271180: step 26190, loss = 8768578912256.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 16:55:01.053853: step 26200, loss = 8698696564736.00 (118.7 examples/sec; 1.078 sec/batch)
2018-04-09 16:55:11.504749: step 26210, loss = 8629371535360.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 16:55:21.926951: step 26220, loss = 8560596484096.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 16:55:32.302208: step 26230, loss = 8492370886656.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 16:55:42.611912: step 26240, loss = 8424690548736.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:55:52.898775: step 26250, loss = 8357548130304.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:56:03.266380: step 26260, loss = 8290941009920.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:56:13.594848: step 26270, loss = 8224864468992.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:56:23.878943: step 26280, loss = 8159313788928.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 16:56:34.184830: step 26290, loss = 8094288969728.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 16:56:44.800181: step 26300, loss = 8029780049920.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 16:56:55.052901: step 26310, loss = 7965784932352.00 (124.8 examples/sec; 1.025 sec/batch)
2018-04-09 16:57:05.411777: step 26320, loss = 7902300995584.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 16:57:15.740843: step 26330, loss = 7839322472448.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:57:26.005483: step 26340, loss = 7776846217216.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 16:57:36.300946: step 26350, loss = 7714868035584.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 16:57:46.623816: step 26360, loss = 7653382160384.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:57:56.888749: step 26370, loss = 7592388067328.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 16:58:07.255220: step 26380, loss = 7531878940672.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 16:58:17.572334: step 26390, loss = 7471851634688.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:58:28.238864: step 26400, loss = 7412305100800.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 16:58:38.555787: step 26410, loss = 7353230426112.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 16:58:48.868483: step 26420, loss = 7294627610624.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 16:58:59.193071: step 26430, loss = 7236491411456.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 16:59:09.566758: step 26440, loss = 7178819207168.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 16:59:19.895251: step 26450, loss = 7121606803456.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 16:59:30.188533: step 26460, loss = 7064849481728.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 16:59:40.458191: step 26470, loss = 7008545669120.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 16:59:50.889778: step 26480, loss = 6952689074176.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 17:00:01.209540: step 26490, loss = 6897278124032.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:00:12.065484: step 26500, loss = 6842309672960.00 (117.9 examples/sec; 1.086 sec/batch)
2018-04-09 17:00:22.470975: step 26510, loss = 6787778478080.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:00:32.908994: step 26520, loss = 6733680869376.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 17:00:43.322391: step 26530, loss = 6680016322560.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 17:00:53.736729: step 26540, loss = 6626778021888.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 17:01:04.131825: step 26550, loss = 6573965443072.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:01:14.496026: step 26560, loss = 6521572818944.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:01:24.767415: step 26570, loss = 6469598576640.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 17:01:35.031773: step 26580, loss = 6418036948992.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 17:01:45.298336: step 26590, loss = 6366886887424.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 17:01:55.908723: step 26600, loss = 6316146294784.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 17:02:06.296826: step 26610, loss = 6265808355328.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:02:16.667197: step 26620, loss = 6215871496192.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:02:26.952997: step 26630, loss = 6166332571648.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:02:37.241368: step 26640, loss = 6117190533120.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:02:47.522491: step 26650, loss = 6068437516288.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:02:57.846209: step 26660, loss = 6020071424000.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:03:08.175005: step 26670, loss = 5972089634816.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:03:18.479559: step 26680, loss = 5924490575872.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:03:28.741207: step 26690, loss = 5877271625728.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 17:03:39.348358: step 26700, loss = 5830429114368.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 17:03:49.708218: step 26710, loss = 5783961993216.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:04:00.075650: step 26720, loss = 5737866067968.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:04:10.465641: step 26730, loss = 5692136620032.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:04:20.776576: step 26740, loss = 5646772076544.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:04:31.089396: step 26750, loss = 5601769291776.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:04:41.395079: step 26760, loss = 5557125120000.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:04:51.698141: step 26770, loss = 5512834842624.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:05:02.064452: step 26780, loss = 5468900032512.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:05:12.398546: step 26790, loss = 5425315446784.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:05:22.993793: step 26800, loss = 5382077939712.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 17:05:33.269120: step 26810, loss = 5339184365568.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 17:05:43.585643: step 26820, loss = 5296632627200.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:05:53.867716: step 26830, loss = 5254420103168.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:06:04.235724: step 26840, loss = 5212543647744.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:06:14.577439: step 26850, loss = 5171001163776.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:06:24.869614: step 26860, loss = 5129790029824.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:06:35.132492: step 26870, loss = 5088907624448.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 17:06:45.426091: step 26880, loss = 5048350801920.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 17:06:55.753296: step 26890, loss = 5008117465088.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:07:06.512626: step 26900, loss = 4968204468224.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 17:07:16.874806: step 26910, loss = 4928609714176.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:07:27.207125: step 26920, loss = 4889330581504.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:07:37.537464: step 26930, loss = 4850364448768.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:07:47.818626: step 26940, loss = 4811708170240.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:07:58.104963: step 26950, loss = 4773360697344.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:08:08.472268: step 26960, loss = 4735318360064.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:08:18.827509: step 26970, loss = 4697580109824.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:08:29.143756: step 26980, loss = 4660140703744.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:08:39.444028: step 26990, loss = 4623001714688.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:08:50.123282: step 27000, loss = 4586158424064.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 17:09:00.568935: step 27010, loss = 4549608734720.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:09:11.033843: step 27020, loss = 4513350025216.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 17:09:21.355018: step 27030, loss = 4477380722688.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:09:31.639101: step 27040, loss = 4441696632832.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:09:41.936979: step 27050, loss = 4406297231360.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:09:52.341644: step 27060, loss = 4371180158976.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 17:10:02.671056: step 27070, loss = 4336343580672.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:10:13.017216: step 27080, loss = 4301784350720.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:10:23.306153: step 27090, loss = 4267500371968.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:10:33.928356: step 27100, loss = 4233490071552.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 17:10:44.199093: step 27110, loss = 4199750828032.00 (124.6 examples/sec; 1.027 sec/batch)
2018-04-09 17:10:54.499440: step 27120, loss = 4166279233536.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:11:04.908491: step 27130, loss = 4133076074496.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:11:15.246870: step 27140, loss = 4100138205184.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:11:25.571210: step 27150, loss = 4067460382720.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:11:35.886979: step 27160, loss = 4035044442112.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:11:46.217676: step 27170, loss = 4002885926912.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:11:56.524551: step 27180, loss = 3970984312832.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:12:06.894995: step 27190, loss = 3939336716288.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:12:17.660134: step 27200, loss = 3907941564416.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 17:12:27.963620: step 27210, loss = 3876796235776.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:12:38.315259: step 27220, loss = 3845898895360.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:12:48.646809: step 27230, loss = 3815249018880.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:12:59.089066: step 27240, loss = 3784842936320.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 17:13:09.499904: step 27250, loss = 3754679336960.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 17:13:19.881718: step 27260, loss = 3724755337216.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:13:30.194200: step 27270, loss = 3695070150656.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:13:40.529358: step 27280, loss = 3665621942272.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:13:50.879755: step 27290, loss = 3636408090624.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:14:01.558026: step 27300, loss = 3607427022848.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 17:14:11.869608: step 27310, loss = 3578677166080.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:14:22.285086: step 27320, loss = 3550155636736.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 17:14:32.740527: step 27330, loss = 3521861910528.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 17:14:43.183003: step 27340, loss = 3493794152448.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 17:14:53.495058: step 27350, loss = 3465949478912.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:15:03.896047: step 27360, loss = 3438327365632.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:15:14.260900: step 27370, loss = 3410924929024.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:15:24.596197: step 27380, loss = 3383741906944.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:15:34.917194: step 27390, loss = 3356774629376.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:15:45.579751: step 27400, loss = 3330022047744.00 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 17:15:55.866835: step 27410, loss = 3303483375616.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:16:06.247086: step 27420, loss = 3277156253696.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:16:16.535285: step 27430, loss = 3251038060544.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:16:26.953731: step 27440, loss = 3225128271872.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 17:16:37.325966: step 27450, loss = 3199425052672.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:16:47.655133: step 27460, loss = 3173927354368.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:16:57.956056: step 27470, loss = 3148631769088.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:17:08.360283: step 27480, loss = 3123538296832.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 17:17:18.769694: step 27490, loss = 3098644578304.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:17:29.420519: step 27500, loss = 3073949564928.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 17:17:39.778754: step 27510, loss = 3049451683840.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:17:50.121459: step 27520, loss = 3025147789312.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:18:00.439348: step 27530, loss = 3001038405632.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:18:10.818672: step 27540, loss = 2977121435648.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:18:21.122731: step 27550, loss = 2953394257920.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:18:31.434003: step 27560, loss = 2929857134592.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:18:41.743652: step 27570, loss = 2906506919936.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:18:52.044101: step 27580, loss = 2883343089664.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:19:02.398446: step 27590, loss = 2860364333056.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:19:13.079268: step 27600, loss = 2837567766528.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 17:19:23.370530: step 27610, loss = 2814953652224.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:19:33.653217: step 27620, loss = 2792519106560.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:19:43.936846: step 27630, loss = 2770263867392.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:19:54.310451: step 27640, loss = 2748185837568.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:20:04.697762: step 27650, loss = 2726283706368.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:20:15.024160: step 27660, loss = 2704555638784.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 17:20:25.307775: step 27670, loss = 2683001896960.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:20:35.597310: step 27680, loss = 2661619335168.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:20:45.895751: step 27690, loss = 2640406904832.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:20:56.492273: step 27700, loss = 2619363557376.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 17:21:06.809423: step 27710, loss = 2598488506368.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:21:17.139947: step 27720, loss = 2577779392512.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:21:27.494285: step 27730, loss = 2557235167232.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:21:37.839334: step 27740, loss = 2536855044096.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:21:48.171397: step 27750, loss = 2516636925952.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:21:58.531109: step 27760, loss = 2496579764224.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:22:08.918546: step 27770, loss = 2476683034624.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:22:19.324752: step 27780, loss = 2456944640000.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:22:29.771594: step 27790, loss = 2437363793920.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:22:40.424713: step 27800, loss = 2417938399232.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 17:22:50.735501: step 27810, loss = 2398668455936.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:23:01.095829: step 27820, loss = 2379551866880.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:23:11.455943: step 27830, loss = 2360587321344.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:23:21.759847: step 27840, loss = 2341774295040.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:23:32.108855: step 27850, loss = 2323111215104.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:23:42.395304: step 27860, loss = 2304597557248.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:23:52.691229: step 27870, loss = 2286230962176.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:24:03.033073: step 27880, loss = 2268009857024.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:24:13.422516: step 27890, loss = 2249934766080.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:24:24.097378: step 27900, loss = 2232003854336.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 17:24:34.429192: step 27910, loss = 2214215024640.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:24:44.815582: step 27920, loss = 2196568670208.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:24:55.156800: step 27930, loss = 2179062824960.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:25:05.574623: step 27940, loss = 2161696309248.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 17:25:15.966571: step 27950, loss = 2144468074496.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:25:26.283551: step 27960, loss = 2127377334272.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:25:36.574016: step 27970, loss = 2110423040000.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:25:46.909489: step 27980, loss = 2093603618816.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:25:57.185317: step 27990, loss = 2076918022144.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 17:26:07.875541: step 28000, loss = 2060365856768.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 17:26:18.200672: step 28010, loss = 2043945287680.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 17:26:28.552954: step 28020, loss = 2027655921664.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:26:38.858837: step 28030, loss = 2011496185856.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:26:49.178455: step 28040, loss = 1995464900608.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:26:59.548455: step 28050, loss = 1979561541632.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:27:09.901666: step 28060, loss = 1963785584640.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:27:20.224423: step 28070, loss = 1948135063552.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:27:30.585729: step 28080, loss = 1932609060864.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:27:40.946301: step 28090, loss = 1917206528000.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:27:51.566062: step 28100, loss = 1901926940672.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 17:28:01.869977: step 28110, loss = 1886769381376.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:28:12.217943: step 28120, loss = 1871732277248.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:28:22.517002: step 28130, loss = 1856815104000.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:28:32.827702: step 28140, loss = 1842016550912.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:28:43.128852: step 28150, loss = 1827336355840.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:28:53.453227: step 28160, loss = 1812773470208.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:29:03.815010: step 28170, loss = 1798325927936.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:29:14.164689: step 28180, loss = 1783993729024.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:29:24.450912: step 28190, loss = 1769775955968.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:29:35.081430: step 28200, loss = 1755671166976.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 17:29:45.368578: step 28210, loss = 1741679230976.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:29:55.760945: step 28220, loss = 1727798837248.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:30:06.131553: step 28230, loss = 1714028675072.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:30:16.484184: step 28240, loss = 1700368351232.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:30:26.800158: step 28250, loss = 1686816948224.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:30:37.170625: step 28260, loss = 1673373417472.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:30:47.594414: step 28270, loss = 1660037365760.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 17:30:58.029022: step 28280, loss = 1646807482368.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 17:31:08.474673: step 28290, loss = 1633682849792.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:31:19.174557: step 28300, loss = 1620662812672.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 17:31:29.549630: step 28310, loss = 1607746584576.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 17:31:39.995877: step 28320, loss = 1594933379072.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:31:50.524235: step 28330, loss = 1582222540800.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 17:32:01.028984: step 28340, loss = 1569612496896.00 (121.8 examples/sec; 1.050 sec/batch)
2018-04-09 17:32:11.428755: step 28350, loss = 1557103509504.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:32:21.826241: step 28360, loss = 1544693219328.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:32:32.185973: step 28370, loss = 1532383461376.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:32:42.496183: step 28380, loss = 1520170565632.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:32:52.797718: step 28390, loss = 1508055056384.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:33:03.487380: step 28400, loss = 1496035622912.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 17:33:13.806350: step 28410, loss = 1484112003072.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:33:24.106304: step 28420, loss = 1472283279360.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:33:34.468126: step 28430, loss = 1460549058560.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:33:44.847696: step 28440, loss = 1448908685312.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:33:55.201080: step 28450, loss = 1437361504256.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:34:05.627681: step 28460, loss = 1425906466816.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 17:34:16.087837: step 28470, loss = 1414542131200.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 17:34:26.475453: step 28480, loss = 1403268890624.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:34:36.825241: step 28490, loss = 1392085041152.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:34:47.592343: step 28500, loss = 1380990713856.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 17:34:57.942989: step 28510, loss = 1369984598016.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:35:08.352514: step 28520, loss = 1359066300416.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:35:18.718574: step 28530, loss = 1348234903552.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:35:29.111783: step 28540, loss = 1337489883136.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:35:39.513837: step 28550, loss = 1326830583808.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:35:49.869666: step 28560, loss = 1316255956992.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:36:00.201989: step 28570, loss = 1305765871616.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:36:10.584978: step 28580, loss = 1295359279104.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:36:20.868967: step 28590, loss = 1285035786240.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:36:31.439884: step 28600, loss = 1274794344448.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 17:36:41.863626: step 28610, loss = 1264634691584.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 17:36:52.129234: step 28620, loss = 1254555910144.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 17:37:02.428222: step 28630, loss = 1244557475840.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:37:12.738650: step 28640, loss = 1234638864384.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:37:23.028691: step 28650, loss = 1224799158272.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:37:33.367776: step 28660, loss = 1215037964288.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:37:43.689616: step 28670, loss = 1205354364928.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:37:53.988630: step 28680, loss = 1195748098048.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:38:04.344982: step 28690, loss = 1186218246144.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:38:15.012023: step 28700, loss = 1176764678144.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 17:38:25.314569: step 28710, loss = 1167386214400.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:38:35.630829: step 28720, loss = 1158082330624.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:38:45.915528: step 28730, loss = 1148853026816.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:38:56.199554: step 28740, loss = 1139696992256.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:39:06.608528: step 28750, loss = 1130613702656.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:39:17.017433: step 28760, loss = 1121603158016.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 17:39:27.314951: step 28770, loss = 1112664440832.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:39:37.646225: step 28780, loss = 1103796895744.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:39:47.973049: step 28790, loss = 1094999998464.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:39:58.837797: step 28800, loss = 1086273290240.00 (117.8 examples/sec; 1.086 sec/batch)
2018-04-09 17:40:09.365650: step 28810, loss = 1077616115712.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 17:40:19.865104: step 28820, loss = 1069027819520.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 17:40:30.327517: step 28830, loss = 1060508205056.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 17:40:40.839013: step 28840, loss = 1052056158208.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 17:40:51.310085: step 28850, loss = 1043671744512.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 17:41:01.778293: step 28860, loss = 1035354046464.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 17:41:12.103885: step 28870, loss = 1027102605312.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 17:41:22.396621: step 28880, loss = 1018917027840.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:41:32.674399: step 28890, loss = 1010796789760.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:41:43.283634: step 28900, loss = 1002740908032.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 17:41:53.570057: step 28910, loss = 994749317120.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:42:03.961388: step 28920, loss = 986821492736.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:42:14.330110: step 28930, loss = 978956910592.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:42:24.679794: step 28940, loss = 971154915328.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:42:34.992833: step 28950, loss = 963415244800.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:42:45.354848: step 28960, loss = 955737047040.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 17:42:55.653037: step 28970, loss = 948120322048.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:43:05.977589: step 28980, loss = 940564152320.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:43:16.292091: step 28990, loss = 933068079104.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:43:26.910411: step 29000, loss = 925631971328.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 17:43:37.161240: step 29010, loss = 918254911488.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 17:43:47.546552: step 29020, loss = 910936571904.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 17:43:58.018868: step 29030, loss = 903676952576.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 17:44:08.581788: step 29040, loss = 896474939392.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 17:44:19.085505: step 29050, loss = 889330401280.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 17:44:29.595323: step 29060, loss = 882242813952.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 17:44:40.074875: step 29070, loss = 875211522048.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 17:44:50.553573: step 29080, loss = 868236525568.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 17:45:01.013587: step 29090, loss = 861317038080.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 17:45:11.901830: step 29100, loss = 854452666368.00 (117.6 examples/sec; 1.089 sec/batch)
2018-04-09 17:45:22.352571: step 29110, loss = 847642886144.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:45:32.851659: step 29120, loss = 840887238656.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 17:45:43.298433: step 29130, loss = 834185134080.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:45:53.744294: step 29140, loss = 827536703488.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 17:46:04.168716: step 29150, loss = 820941357056.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 17:46:14.522267: step 29160, loss = 814398701568.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:46:24.913577: step 29170, loss = 807908409344.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:46:35.290569: step 29180, loss = 801469562880.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:46:45.623641: step 29190, loss = 795082096640.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:46:56.366973: step 29200, loss = 788745551872.00 (119.1 examples/sec; 1.074 sec/batch)
2018-04-09 17:47:06.942740: step 29210, loss = 782459666432.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 17:47:17.330578: step 29220, loss = 776223588352.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:47:27.729496: step 29230, loss = 770037514240.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:47:38.101218: step 29240, loss = 763900461056.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:47:48.539745: step 29250, loss = 757812494336.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 17:47:58.893769: step 29260, loss = 751772958720.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 17:48:09.273780: step 29270, loss = 745781657600.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:48:19.619165: step 29280, loss = 739837935616.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:48:29.921094: step 29290, loss = 733941596160.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:48:40.543240: step 29300, loss = 728092377088.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 17:48:50.845480: step 29310, loss = 722289623040.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:49:01.188245: step 29320, loss = 716533071872.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:49:11.531901: step 29330, loss = 710822658048.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 17:49:21.898733: step 29340, loss = 705157660672.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:49:32.235933: step 29350, loss = 699537686528.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:49:42.778196: step 29360, loss = 693962539008.00 (121.4 examples/sec; 1.054 sec/batch)
2018-04-09 17:49:53.212448: step 29370, loss = 688431890432.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 17:50:03.552504: step 29380, loss = 682945282048.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:50:13.926535: step 29390, loss = 677502451712.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:50:24.533932: step 29400, loss = 672103137280.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 17:50:34.824399: step 29410, loss = 666746748928.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:50:45.165293: step 29420, loss = 661432958976.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:50:55.510746: step 29430, loss = 656161570816.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 17:51:05.938879: step 29440, loss = 650932256768.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 17:51:16.318098: step 29450, loss = 645744427008.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:51:26.649462: step 29460, loss = 640598081536.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:51:36.989740: step 29470, loss = 635492696064.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:51:47.359532: step 29480, loss = 630428008448.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:51:57.683634: step 29490, loss = 625403691008.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:52:08.351111: step 29500, loss = 620419416064.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 17:52:18.670611: step 29510, loss = 615474987008.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:52:28.952580: step 29520, loss = 610569814016.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:52:39.231739: step 29530, loss = 605703634944.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:52:49.562702: step 29540, loss = 600876318720.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:52:59.920827: step 29550, loss = 596087603200.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 17:53:10.296799: step 29560, loss = 591337029632.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 17:53:20.601599: step 29570, loss = 586624270336.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 17:53:30.867096: step 29580, loss = 581948997632.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 17:53:41.173751: step 29590, loss = 577311014912.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:53:51.815752: step 29600, loss = 572710060032.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 17:54:02.129979: step 29610, loss = 568145805312.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:54:12.514596: step 29620, loss = 563617792000.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 17:54:22.825103: step 29630, loss = 559125889024.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:54:33.145027: step 29640, loss = 554669899776.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:54:43.477611: step 29650, loss = 550249365504.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 17:54:53.785270: step 29660, loss = 545864024064.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:55:04.151920: step 29670, loss = 541513646080.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:55:14.491714: step 29680, loss = 537197936640.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:55:24.781493: step 29690, loss = 532916699136.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:55:35.413653: step 29700, loss = 528669540352.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 17:55:45.695047: step 29710, loss = 524456132608.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:55:56.008020: step 29720, loss = 520276377600.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:56:06.403568: step 29730, loss = 516129947648.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 17:56:16.807909: step 29740, loss = 512016547840.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 17:56:27.176115: step 29750, loss = 507935948800.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 17:56:37.485687: step 29760, loss = 503887757312.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:56:47.772142: step 29770, loss = 499871875072.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:56:58.080466: step 29780, loss = 495888007168.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 17:57:08.425166: step 29790, loss = 491935956992.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 17:57:19.151071: step 29800, loss = 488015462400.00 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 17:57:29.473692: step 29810, loss = 484126097408.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 17:57:39.787338: step 29820, loss = 480267698176.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 17:57:50.105020: step 29830, loss = 476440133632.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 17:58:00.570828: step 29840, loss = 472643141632.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 17:58:10.956310: step 29850, loss = 468876263424.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 17:58:21.400299: step 29860, loss = 465139531776.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 17:58:31.691605: step 29870, loss = 461432487936.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 17:58:42.030663: step 29880, loss = 457755099136.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:58:52.366484: step 29890, loss = 454106906624.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 17:59:03.047886: step 29900, loss = 450487812096.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 17:59:13.422523: step 29910, loss = 446897618944.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 17:59:23.723501: step 29920, loss = 443335966720.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 17:59:34.008033: step 29930, loss = 439802757120.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 17:59:44.447430: step 29940, loss = 436297695232.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 17:59:54.911732: step 29950, loss = 432820518912.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 18:00:05.283181: step 29960, loss = 429371064320.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:00:15.627902: step 29970, loss = 425949134848.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 18:00:25.962855: step 29980, loss = 422554501120.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:00:36.323790: step 29990, loss = 419186868224.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:00:47.001082: step 30000, loss = 415846072320.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 18:00:57.353077: step 30010, loss = 412531884032.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 18:01:07.794603: step 30020, loss = 409244106752.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:01:18.176154: step 30030, loss = 405982609408.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:01:28.532318: step 30040, loss = 402747097088.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:01:38.948206: step 30050, loss = 399537373184.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:01:49.514655: step 30060, loss = 396353142784.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 18:02:00.056224: step 30070, loss = 393194307584.00 (121.4 examples/sec; 1.054 sec/batch)
2018-04-09 18:02:10.670106: step 30080, loss = 390060736512.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 18:02:21.196687: step 30090, loss = 386952036352.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:02:32.066056: step 30100, loss = 383868207104.00 (117.8 examples/sec; 1.087 sec/batch)
2018-04-09 18:02:42.565559: step 30110, loss = 380808888320.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 18:02:53.115541: step 30120, loss = 377773948928.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 18:03:03.707907: step 30130, loss = 374763094016.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 18:03:14.296492: step 30140, loss = 371776126976.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 18:03:24.804455: step 30150, loss = 368812982272.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:03:35.327164: step 30160, loss = 365873561600.00 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 18:03:45.812726: step 30170, loss = 362957537280.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 18:03:56.305191: step 30180, loss = 360064974848.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 18:04:06.845033: step 30190, loss = 357195448320.00 (121.4 examples/sec; 1.054 sec/batch)
2018-04-09 18:04:17.733079: step 30200, loss = 354348531712.00 (117.6 examples/sec; 1.089 sec/batch)
2018-04-09 18:04:28.210824: step 30210, loss = 351524683776.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 18:04:38.691810: step 30220, loss = 348723052544.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 18:04:49.158527: step 30230, loss = 345943834624.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:04:59.672312: step 30240, loss = 343186833408.00 (121.7 examples/sec; 1.051 sec/batch)
2018-04-09 18:05:10.326601: step 30250, loss = 340451655680.00 (120.1 examples/sec; 1.065 sec/batch)
2018-04-09 18:05:20.904220: step 30260, loss = 337738366976.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 18:05:31.382561: step 30270, loss = 335046672384.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 18:05:41.869085: step 30280, loss = 332376473600.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 18:05:52.258807: step 30290, loss = 329727574016.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:06:02.986835: step 30300, loss = 327099809792.00 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 18:06:13.352339: step 30310, loss = 324492886016.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 18:06:23.705696: step 30320, loss = 321906769920.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 18:06:34.031326: step 30330, loss = 319341330432.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 18:06:44.499802: step 30340, loss = 316796239872.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:06:54.987412: step 30350, loss = 314271498240.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 18:07:05.515596: step 30360, loss = 311766810624.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:07:16.026572: step 30370, loss = 309282144256.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:07:26.521596: step 30380, loss = 306817236992.00 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 18:07:37.030419: step 30390, loss = 304371990528.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:07:47.883409: step 30400, loss = 301946241024.00 (117.9 examples/sec; 1.085 sec/batch)
2018-04-09 18:07:58.386771: step 30410, loss = 299539857408.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 18:08:09.008004: step 30420, loss = 297152577536.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 18:08:19.544956: step 30430, loss = 294784368640.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 18:08:30.045538: step 30440, loss = 292435034112.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 18:08:40.541691: step 30450, loss = 290104442880.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 18:08:51.070599: step 30460, loss = 287792398336.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:09:01.617923: step 30470, loss = 285498769408.00 (121.4 examples/sec; 1.055 sec/batch)
2018-04-09 18:09:12.181753: step 30480, loss = 283223523328.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 18:09:22.691533: step 30490, loss = 280966266880.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:09:33.517466: step 30500, loss = 278727065600.00 (118.2 examples/sec; 1.083 sec/batch)
2018-04-09 18:09:43.983941: step 30510, loss = 276505690112.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:09:54.697474: step 30520, loss = 274301976576.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 18:10:05.191228: step 30530, loss = 272115908608.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 18:10:15.774643: step 30540, loss = 269947256832.00 (120.9 examples/sec; 1.058 sec/batch)
2018-04-09 18:10:26.302087: step 30550, loss = 267795873792.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:10:36.814927: step 30560, loss = 265661612032.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:10:47.283732: step 30570, loss = 263544373248.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:10:57.797039: step 30580, loss = 261444026368.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:11:08.283718: step 30590, loss = 259360358400.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 18:11:19.056807: step 30600, loss = 257293287424.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 18:11:29.437650: step 30610, loss = 255242682368.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:11:39.852174: step 30620, loss = 253208461312.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 18:11:50.208661: step 30630, loss = 251190509568.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:12:00.594490: step 30640, loss = 249188581376.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:12:11.065343: step 30650, loss = 247202611200.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 18:12:21.568464: step 30660, loss = 245232484352.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 18:12:32.000375: step 30670, loss = 243278036992.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:12:42.423872: step 30680, loss = 241339203584.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:12:52.813023: step 30690, loss = 239415787520.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:13:03.586893: step 30700, loss = 237507739648.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 18:13:14.093006: step 30710, loss = 235614928896.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:13:24.502021: step 30720, loss = 233737125888.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 18:13:34.919258: step 30730, loss = 231874330624.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:13:45.496130: step 30740, loss = 230026362880.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 18:13:55.900708: step 30750, loss = 228193157120.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 18:14:06.373680: step 30760, loss = 226374533120.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 18:14:16.806484: step 30770, loss = 224570408960.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:14:27.203454: step 30780, loss = 222780620800.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:14:37.532369: step 30790, loss = 221005168640.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:14:48.200076: step 30800, loss = 219243806720.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 18:14:58.564320: step 30810, loss = 217496485888.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:15:09.014421: step 30820, loss = 215763156992.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:15:19.444810: step 30830, loss = 214043574272.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:15:29.825788: step 30840, loss = 212337721344.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:15:40.189498: step 30850, loss = 210645516288.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:15:50.600631: step 30860, loss = 208966729728.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 18:16:01.004582: step 30870, loss = 207301361664.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 18:16:11.444774: step 30880, loss = 205649264640.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:16:21.807930: step 30890, loss = 204010323968.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:16:32.502997: step 30900, loss = 202384408576.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 18:16:42.853379: step 30910, loss = 200771502080.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:16:53.199537: step 30920, loss = 199171375104.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:17:03.616690: step 30930, loss = 197584060416.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:17:14.073128: step 30940, loss = 196009377792.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 18:17:24.444960: step 30950, loss = 194447261696.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:17:34.802560: step 30960, loss = 192897564672.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:17:45.163650: step 30970, loss = 191360237568.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:17:55.602520: step 30980, loss = 189835165696.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:18:06.052001: step 30990, loss = 188322234368.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:18:16.903596: step 31000, loss = 186821378048.00 (118.0 examples/sec; 1.085 sec/batch)
2018-04-09 18:18:27.336474: step 31010, loss = 185332498432.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:18:37.654629: step 31020, loss = 183855415296.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 18:18:47.955387: step 31030, loss = 182390177792.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:18:58.275898: step 31040, loss = 180936589312.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 18:19:08.637520: step 31050, loss = 179494633472.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:19:18.992513: step 31060, loss = 178064097280.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 18:19:29.335969: step 31070, loss = 176644980736.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 18:19:39.705986: step 31080, loss = 175237169152.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:19:50.093299: step 31090, loss = 173840580608.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:20:01.085522: step 31100, loss = 172455165952.00 (116.4 examples/sec; 1.099 sec/batch)
2018-04-09 18:20:11.686194: step 31110, loss = 171080728576.00 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 18:20:22.255949: step 31120, loss = 169717284864.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 18:20:32.764161: step 31130, loss = 168364670976.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:20:43.275676: step 31140, loss = 167022903296.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:20:53.805922: step 31150, loss = 165691768832.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:21:04.451637: step 31160, loss = 164371283968.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 18:21:15.010127: step 31170, loss = 163061301248.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 18:21:25.546831: step 31180, loss = 161761755136.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 18:21:36.072358: step 31190, loss = 160472563712.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:21:46.944601: step 31200, loss = 159193645056.00 (117.7 examples/sec; 1.087 sec/batch)
2018-04-09 18:21:57.474583: step 31210, loss = 157924933632.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 18:22:08.072244: step 31220, loss = 156666314752.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 18:22:18.534762: step 31230, loss = 155417739264.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 18:22:28.949879: step 31240, loss = 154179125248.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:22:39.335715: step 31250, loss = 152950358016.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:22:49.740664: step 31260, loss = 151731372032.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 18:23:00.082970: step 31270, loss = 150522134528.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:23:10.479420: step 31280, loss = 149322514432.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:23:21.041671: step 31290, loss = 148132478976.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 18:23:31.777116: step 31300, loss = 146951880704.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 18:23:42.066245: step 31310, loss = 145780752384.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:23:52.378914: step 31320, loss = 144618946560.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 18:24:02.724039: step 31330, loss = 143466414080.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:24:13.074450: step 31340, loss = 142323023872.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:24:23.373907: step 31350, loss = 141188743168.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:24:33.710029: step 31360, loss = 140063490048.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:24:44.016006: step 31370, loss = 138947248128.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 18:24:54.324398: step 31380, loss = 137839886336.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 18:25:04.700792: step 31390, loss = 136741339136.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 18:25:15.398553: step 31400, loss = 135651557376.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 18:25:25.713727: step 31410, loss = 134570450944.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 18:25:36.036244: step 31420, loss = 133497987072.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 18:25:46.335024: step 31430, loss = 132434059264.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:25:56.663690: step 31440, loss = 131378593792.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:26:07.111731: step 31450, loss = 130331566080.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:26:17.512953: step 31460, loss = 129292861440.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:26:27.895601: step 31470, loss = 128262430720.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:26:38.402470: step 31480, loss = 127240241152.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:26:49.016579: step 31490, loss = 126226194432.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 18:26:59.928215: step 31500, loss = 125220208640.00 (117.3 examples/sec; 1.091 sec/batch)
2018-04-09 18:27:10.577077: step 31510, loss = 124222234624.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 18:27:21.206414: step 31520, loss = 123232231424.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 18:27:31.838152: step 31530, loss = 122250141696.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 18:27:42.437493: step 31540, loss = 121275817984.00 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 18:27:53.005701: step 31550, loss = 120309293056.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 18:28:03.430162: step 31560, loss = 119350444032.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:28:13.871389: step 31570, loss = 118399295488.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:28:24.337102: step 31580, loss = 117455683584.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:28:34.775282: step 31590, loss = 116519591936.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:28:45.515462: step 31600, loss = 115590971392.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 18:28:55.913167: step 31610, loss = 114669748224.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:29:06.361334: step 31620, loss = 113755873280.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:29:16.755119: step 31630, loss = 112849281024.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:29:27.167536: step 31640, loss = 111949914112.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 18:29:37.578448: step 31650, loss = 111057715200.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 18:29:48.020462: step 31660, loss = 110172618752.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:29:58.500317: step 31670, loss = 109294583808.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 18:30:08.921756: step 31680, loss = 108423544832.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:30:19.240674: step 31690, loss = 107559428096.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 18:30:29.886759: step 31700, loss = 106702233600.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 18:30:40.173772: step 31710, loss = 105851838464.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:30:50.533589: step 31720, loss = 105008250880.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:31:00.869684: step 31730, loss = 104171364352.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:31:11.287711: step 31740, loss = 103341154304.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:31:21.656647: step 31750, loss = 102517555200.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:31:32.042516: step 31760, loss = 101700526080.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:31:42.423625: step 31770, loss = 100890001408.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:31:52.890090: step 31780, loss = 100085948416.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:32:03.458321: step 31790, loss = 99288276992.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 18:32:14.313610: step 31800, loss = 98497003520.00 (117.9 examples/sec; 1.086 sec/batch)
2018-04-09 18:32:24.763862: step 31810, loss = 97712005120.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:32:35.234513: step 31820, loss = 96933273600.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 18:32:45.748673: step 31830, loss = 96160735232.00 (121.7 examples/sec; 1.051 sec/batch)
2018-04-09 18:32:56.228344: step 31840, loss = 95394390016.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 18:33:06.736158: step 31850, loss = 94634123264.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 18:33:17.186338: step 31860, loss = 93879902208.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:33:27.620869: step 31870, loss = 93131653120.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:33:38.045979: step 31880, loss = 92389384192.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 18:33:48.430043: step 31890, loss = 91653038080.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:33:59.150391: step 31900, loss = 90922565632.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 18:34:09.604318: step 31910, loss = 90197950464.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 18:34:20.035258: step 31920, loss = 89479151616.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:34:30.421921: step 31930, loss = 88765988864.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:34:40.845356: step 31940, loss = 88058576896.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:34:51.219546: step 31950, loss = 87356760064.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:35:01.606783: step 31960, loss = 86660571136.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:35:11.978448: step 31970, loss = 85969911808.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:35:22.320279: step 31980, loss = 85284749312.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:35:32.653169: step 31990, loss = 84605050880.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:35:43.397010: step 32000, loss = 83930759168.00 (119.1 examples/sec; 1.074 sec/batch)
2018-04-09 18:35:53.796176: step 32010, loss = 83261865984.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:36:04.298531: step 32020, loss = 82598297600.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 18:36:14.747591: step 32030, loss = 81940021248.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:36:25.299578: step 32040, loss = 81286995968.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 18:36:35.670275: step 32050, loss = 80639148032.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:36:46.020723: step 32060, loss = 79996493824.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:36:56.314419: step 32070, loss = 79358951424.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 18:37:06.676405: step 32080, loss = 78726488064.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:37:16.994577: step 32090, loss = 78099062784.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 18:37:27.623584: step 32100, loss = 77476642816.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 18:37:37.922010: step 32110, loss = 76859179008.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:37:48.236668: step 32120, loss = 76246622208.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 18:37:58.533834: step 32130, loss = 75638964224.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:38:08.897885: step 32140, loss = 75036147712.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:38:19.246477: step 32150, loss = 74438139904.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:38:29.565508: step 32160, loss = 73844899840.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 18:38:39.892506: step 32170, loss = 73256378368.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:38:50.173809: step 32180, loss = 72672542720.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 18:39:00.467146: step 32190, loss = 72093351936.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:39:11.192125: step 32200, loss = 71518806016.00 (119.3 examples/sec; 1.072 sec/batch)
2018-04-09 18:39:21.478137: step 32210, loss = 70948823040.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:39:31.775341: step 32220, loss = 70383403008.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:39:42.068390: step 32230, loss = 69822455808.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:39:52.373981: step 32240, loss = 69265997824.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 18:40:02.803005: step 32250, loss = 68713971712.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 18:40:13.183132: step 32260, loss = 68166361088.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:40:23.491945: step 32270, loss = 67623088128.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 18:40:33.804005: step 32280, loss = 67084148736.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 18:40:44.106693: step 32290, loss = 66549510144.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 18:40:54.714029: step 32300, loss = 66019135488.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 18:41:05.070382: step 32310, loss = 65492971520.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:41:15.420187: step 32320, loss = 64971026432.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:41:25.702903: step 32330, loss = 64453234688.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 18:41:35.982791: step 32340, loss = 63939571712.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 18:41:46.299597: step 32350, loss = 63430000640.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 18:41:56.588870: step 32360, loss = 62924480512.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:42:06.912533: step 32370, loss = 62422994944.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 18:42:17.269294: step 32380, loss = 61925502976.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:42:27.555271: step 32390, loss = 61431975936.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:42:38.218062: step 32400, loss = 60942372864.00 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 18:42:48.501840: step 32410, loss = 60456689664.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 18:42:58.831821: step 32420, loss = 59974864896.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:43:09.195186: step 32430, loss = 59496886272.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:43:19.494211: step 32440, loss = 59022704640.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:43:29.781609: step 32450, loss = 58552311808.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 18:43:40.156815: step 32460, loss = 58085666816.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 18:43:50.459108: step 32470, loss = 57622749184.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 18:44:00.821391: step 32480, loss = 57163517952.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:44:11.182771: step 32490, loss = 56707944448.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:44:21.883837: step 32500, loss = 56256012288.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 18:44:32.233274: step 32510, loss = 55807655936.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:44:42.592714: step 32520, loss = 55362891776.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:44:52.918352: step 32530, loss = 54921670656.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 18:45:03.254191: step 32540, loss = 54483963904.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:45:13.609244: step 32550, loss = 54049742848.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:45:23.948977: step 32560, loss = 53618982912.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:45:34.291781: step 32570, loss = 53191655424.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:45:44.678360: step 32580, loss = 52767727616.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:45:55.006608: step 32590, loss = 52347170816.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:46:05.720336: step 32600, loss = 51929972736.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 18:46:16.021278: step 32610, loss = 51516092416.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:46:26.376813: step 32620, loss = 51105521664.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:46:36.716253: step 32630, loss = 50698235904.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:46:47.090864: step 32640, loss = 50294177792.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:46:57.507235: step 32650, loss = 49893351424.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:47:07.944704: step 32660, loss = 49495715840.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:47:18.349432: step 32670, loss = 49101254656.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 18:47:28.808692: step 32680, loss = 48709926912.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 18:47:39.187166: step 32690, loss = 48321736704.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:47:49.894727: step 32700, loss = 47936626688.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 18:48:00.225976: step 32710, loss = 47554584576.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:48:10.634060: step 32720, loss = 47175598080.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 18:48:20.973168: step 32730, loss = 46799622144.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:48:31.295984: step 32740, loss = 46426648576.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 18:48:41.666863: step 32750, loss = 46056632320.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:48:52.016885: step 32760, loss = 45689573376.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:49:02.380428: step 32770, loss = 45325447168.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:49:12.754815: step 32780, loss = 44964212736.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 18:49:23.085149: step 32790, loss = 44605874176.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:49:33.693294: step 32800, loss = 44250374144.00 (120.7 examples/sec; 1.061 sec/batch)
2018-04-09 18:49:43.970789: step 32810, loss = 43897712640.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 18:49:54.319189: step 32820, loss = 43547869184.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:50:04.809340: step 32830, loss = 43200802816.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 18:50:15.188559: step 32840, loss = 42856513536.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:50:25.539451: step 32850, loss = 42514956288.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:50:35.917973: step 32860, loss = 42176126976.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:50:46.250696: step 32870, loss = 41839992832.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:50:56.598465: step 32880, loss = 41506545664.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:51:06.985829: step 32890, loss = 41175756800.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:51:17.666282: step 32900, loss = 40847601664.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 18:51:27.996458: step 32910, loss = 40522055680.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:51:38.332278: step 32920, loss = 40199110656.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:51:48.685606: step 32930, loss = 39878733824.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 18:51:59.020358: step 32940, loss = 39560912896.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:52:09.516274: step 32950, loss = 39245627392.00 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 18:52:19.953340: step 32960, loss = 38932848640.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:52:30.335064: step 32970, loss = 38622560256.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 18:52:40.737250: step 32980, loss = 38314758144.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:52:51.140664: step 32990, loss = 38009401344.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 18:53:01.926554: step 33000, loss = 37706477568.00 (118.7 examples/sec; 1.079 sec/batch)
2018-04-09 18:53:12.393957: step 33010, loss = 37405966336.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 18:53:22.811552: step 33020, loss = 37107851264.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 18:53:33.205539: step 33030, loss = 36812115968.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 18:53:43.640558: step 33040, loss = 36518744064.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 18:53:54.093028: step 33050, loss = 36227698688.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 18:54:04.566577: step 33060, loss = 35938971648.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 18:54:15.007761: step 33070, loss = 35652558848.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 18:54:25.430921: step 33080, loss = 35368415232.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:54:35.851148: step 33090, loss = 35086540800.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:54:46.596644: step 33100, loss = 34806906880.00 (119.1 examples/sec; 1.075 sec/batch)
2018-04-09 18:54:57.002403: step 33110, loss = 34529513472.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 18:55:07.549389: step 33120, loss = 34254321664.00 (121.4 examples/sec; 1.055 sec/batch)
2018-04-09 18:55:18.038653: step 33130, loss = 33981329408.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 18:55:28.463228: step 33140, loss = 33710510080.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 18:55:38.919114: step 33150, loss = 33441843200.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 18:55:49.390653: step 33160, loss = 33175322624.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 18:55:59.847732: step 33170, loss = 32910923776.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 18:56:10.305249: step 33180, loss = 32648634368.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 18:56:20.681105: step 33190, loss = 32388435968.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 18:56:31.329111: step 33200, loss = 32130312192.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 18:56:41.639686: step 33210, loss = 31874244608.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 18:56:51.997050: step 33220, loss = 31620212736.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:57:02.372900: step 33230, loss = 31368208384.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 18:57:12.778928: step 33240, loss = 31118215168.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 18:57:23.135010: step 33250, loss = 30870206464.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 18:57:33.482373: step 33260, loss = 30624180224.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 18:57:43.781346: step 33270, loss = 30380111872.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 18:57:54.119707: step 33280, loss = 30137993216.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:58:04.527091: step 33290, loss = 29897801728.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 18:58:15.263068: step 33300, loss = 29659529216.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 18:58:25.617580: step 33310, loss = 29423147008.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 18:58:35.953456: step 33320, loss = 29188651008.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 18:58:46.297947: step 33330, loss = 28956030976.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 18:58:56.686710: step 33340, loss = 28725258240.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 18:59:07.113414: step 33350, loss = 28496328704.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 18:59:17.513502: step 33360, loss = 28269217792.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 18:59:27.843987: step 33370, loss = 28043925504.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 18:59:38.169859: step 33380, loss = 27820423168.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 18:59:48.532275: step 33390, loss = 27598700544.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 18:59:59.325002: step 33400, loss = 27378747392.00 (118.6 examples/sec; 1.079 sec/batch)
2018-04-09 19:00:09.713220: step 33410, loss = 27160543232.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:00:20.089798: step 33420, loss = 26944081920.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 19:00:30.459376: step 33430, loss = 26729347072.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:00:40.833164: step 33440, loss = 26516324352.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:00:51.204737: step 33450, loss = 26304999424.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:01:01.566971: step 33460, loss = 26095353856.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:01:11.993269: step 33470, loss = 25887387648.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 19:01:22.346290: step 33480, loss = 25681070080.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:01:32.661460: step 33490, loss = 25476403200.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:01:43.354842: step 33500, loss = 25273364480.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 19:01:53.747661: step 33510, loss = 25071941632.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:02:04.203265: step 33520, loss = 24872128512.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 19:02:14.586575: step 33530, loss = 24673908736.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:02:24.914732: step 33540, loss = 24477257728.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:02:35.192846: step 33550, loss = 24282189824.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:02:45.492700: step 33560, loss = 24088655872.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:02:55.785282: step 33570, loss = 23896686592.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 19:03:06.111147: step 33580, loss = 23706241024.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 19:03:16.466255: step 33590, loss = 23517304832.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:03:27.084658: step 33600, loss = 23329869824.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 19:03:37.335694: step 33610, loss = 23143931904.00 (124.9 examples/sec; 1.025 sec/batch)
2018-04-09 19:03:47.643032: step 33620, loss = 22959470592.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:03:57.982044: step 33630, loss = 22776487936.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:04:08.358271: step 33640, loss = 22594967552.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 19:04:18.721584: step 33650, loss = 22414901248.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:04:29.027586: step 33660, loss = 22236254208.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:04:39.321458: step 33670, loss = 22059042816.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 19:04:49.627002: step 33680, loss = 21883236352.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:04:59.905741: step 33690, loss = 21708838912.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:05:10.580174: step 33700, loss = 21535821824.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 19:05:20.878356: step 33710, loss = 21364187136.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:05:31.143891: step 33720, loss = 21193922560.00 (124.7 examples/sec; 1.027 sec/batch)
2018-04-09 19:05:41.407268: step 33730, loss = 21025013760.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 19:05:51.706113: step 33740, loss = 20857450496.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:06:02.041960: step 33750, loss = 20691222528.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:06:12.380810: step 33760, loss = 20526321664.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:06:22.675243: step 33770, loss = 20362735616.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 19:06:32.963826: step 33780, loss = 20200450048.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 19:06:43.227573: step 33790, loss = 20039462912.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-09 19:06:53.844374: step 33800, loss = 19879751680.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 19:07:04.153573: step 33810, loss = 19721318400.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:07:14.556004: step 33820, loss = 19564148736.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:07:24.850159: step 33830, loss = 19408228352.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 19:07:35.220895: step 33840, loss = 19253547008.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:07:45.537612: step 33850, loss = 19100102656.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:07:55.886594: step 33860, loss = 18947883008.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:08:06.248029: step 33870, loss = 18796873728.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:08:16.664645: step 33880, loss = 18647064576.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 19:08:27.024868: step 33890, loss = 18498455552.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:08:37.686175: step 33900, loss = 18351028224.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 19:08:47.989354: step 33910, loss = 18204778496.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 19:08:58.306674: step 33920, loss = 18059689984.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:09:08.734580: step 33930, loss = 17915758592.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 19:09:19.103430: step 33940, loss = 17772976128.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:09:29.424626: step 33950, loss = 17631326208.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:09:39.759560: step 33960, loss = 17490814976.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:09:50.075497: step 33970, loss = 17351415808.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:10:00.534127: step 33980, loss = 17213132800.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 19:10:10.941636: step 33990, loss = 17075948544.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 19:10:21.604200: step 34000, loss = 16939857920.00 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 19:10:31.901842: step 34010, loss = 16804855808.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:10:42.197434: step 34020, loss = 16670923776.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:10:52.558984: step 34030, loss = 16538062848.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:11:02.948939: step 34040, loss = 16406259712.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:11:13.338815: step 34050, loss = 16275506176.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:11:23.755759: step 34060, loss = 16145795072.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 19:11:34.082263: step 34070, loss = 16017116160.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 19:11:44.458764: step 34080, loss = 15889464320.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 19:11:54.808537: step 34090, loss = 15762831360.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:12:05.515218: step 34100, loss = 15637207040.00 (119.6 examples/sec; 1.071 sec/batch)
2018-04-09 19:12:15.863646: step 34110, loss = 15512582144.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:12:26.193213: step 34120, loss = 15388950528.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:12:36.525733: step 34130, loss = 15266306048.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:12:46.888254: step 34140, loss = 15144639488.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:12:57.309630: step 34150, loss = 15023942656.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 19:13:07.695733: step 34160, loss = 14904205312.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:13:18.093129: step 34170, loss = 14785421312.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 19:13:28.384356: step 34180, loss = 14667586560.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 19:13:38.694654: step 34190, loss = 14550690816.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:13:49.339808: step 34200, loss = 14434726912.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 19:13:59.669317: step 34210, loss = 14319687680.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:14:10.089565: step 34220, loss = 14205562880.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 19:14:20.461846: step 34230, loss = 14092349440.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:14:30.773883: step 34240, loss = 13980038144.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:14:41.088494: step 34250, loss = 13868621824.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:14:51.415866: step 34260, loss = 13758094336.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:15:01.742358: step 34270, loss = 13648447488.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 19:15:12.149060: step 34280, loss = 13539674112.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 19:15:22.480658: step 34290, loss = 13431768064.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:15:33.110658: step 34300, loss = 13324718080.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 19:15:43.416312: step 34310, loss = 13218525184.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:15:53.756439: step 34320, loss = 13113182208.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:16:04.129441: step 34330, loss = 13008677888.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:16:14.487151: step 34340, loss = 12905005056.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:16:24.841540: step 34350, loss = 12802157568.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:16:35.222801: step 34360, loss = 12700125184.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:16:45.556098: step 34370, loss = 12598912000.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:16:55.865177: step 34380, loss = 12498501632.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:17:06.229292: step 34390, loss = 12398892032.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:17:16.935753: step 34400, loss = 12300079104.00 (119.6 examples/sec; 1.071 sec/batch)
2018-04-09 19:17:27.304619: step 34410, loss = 12202050560.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:17:37.737047: step 34420, loss = 12104805376.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 19:17:48.072715: step 34430, loss = 12008332288.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:17:58.439403: step 34440, loss = 11912631296.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:18:08.870686: step 34450, loss = 11817688064.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 19:18:19.247754: step 34460, loss = 11723503616.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:18:29.572246: step 34470, loss = 11630070784.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:18:39.955655: step 34480, loss = 11537385472.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:18:50.323080: step 34490, loss = 11445434368.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:19:00.960945: step 34500, loss = 11354219520.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 19:19:11.365247: step 34510, loss = 11263729664.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:19:21.757308: step 34520, loss = 11173961728.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:19:32.073670: step 34530, loss = 11084908544.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:19:42.408543: step 34540, loss = 10996564992.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:19:52.749463: step 34550, loss = 10908925952.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:20:03.313809: step 34560, loss = 10821986304.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 19:20:13.780663: step 34570, loss = 10735737856.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 19:20:24.186100: step 34580, loss = 10650175488.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 19:20:34.546546: step 34590, loss = 10565298176.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:20:45.225636: step 34600, loss = 10481094656.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 19:20:55.552967: step 34610, loss = 10397562880.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:21:05.939641: step 34620, loss = 10314698752.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:21:16.315519: step 34630, loss = 10232496128.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 19:21:26.703719: step 34640, loss = 10150945792.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:21:37.071727: step 34650, loss = 10070046720.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:21:47.438814: step 34660, loss = 9989789696.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:21:57.733128: step 34670, loss = 9910176768.00 (124.3 examples/sec; 1.029 sec/batch)
2018-04-09 19:22:08.080972: step 34680, loss = 9831195648.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:22:18.401734: step 34690, loss = 9752843264.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:22:29.087419: step 34700, loss = 9675115520.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 19:22:39.408168: step 34710, loss = 9598008320.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:22:49.761802: step 34720, loss = 9521514496.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:23:00.076910: step 34730, loss = 9445630976.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:23:10.448053: step 34740, loss = 9370351616.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:23:20.832954: step 34750, loss = 9295673344.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:23:31.198117: step 34760, loss = 9221590016.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:23:41.559801: step 34770, loss = 9148096512.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:23:51.963978: step 34780, loss = 9075186688.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:24:02.373081: step 34790, loss = 9002858496.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 19:24:13.152785: step 34800, loss = 8931107840.00 (118.7 examples/sec; 1.078 sec/batch)
2018-04-09 19:24:23.555607: step 34810, loss = 8859929600.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:24:33.949290: step 34820, loss = 8789319680.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:24:44.265180: step 34830, loss = 8719270912.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:24:54.594524: step 34840, loss = 8649781248.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:25:04.969902: step 34850, loss = 8580845568.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 19:25:15.326781: step 34860, loss = 8512459264.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:25:25.641523: step 34870, loss = 8444618752.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:25:35.959235: step 34880, loss = 8377317376.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:25:46.276387: step 34890, loss = 8310552064.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:25:56.941797: step 34900, loss = 8244320256.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 19:26:07.308537: step 34910, loss = 8178615808.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:26:17.671597: step 34920, loss = 8113434624.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:26:28.006482: step 34930, loss = 8048774144.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:26:38.322457: step 34940, loss = 7984627200.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:26:48.648115: step 34950, loss = 7920992256.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 19:26:58.974860: step 34960, loss = 7857866752.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:27:09.320420: step 34970, loss = 7795239936.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:27:19.652460: step 34980, loss = 7733113344.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:27:29.958898: step 34990, loss = 7671482368.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:27:40.575111: step 35000, loss = 7610343424.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 19:27:50.855461: step 35010, loss = 7549689344.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:28:01.167812: step 35020, loss = 7489520640.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:28:11.564601: step 35030, loss = 7429831168.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 19:28:21.886456: step 35040, loss = 7370618368.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:28:32.230555: step 35050, loss = 7311877120.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 19:28:42.568875: step 35060, loss = 7253605376.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:28:52.884343: step 35070, loss = 7195795456.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:29:03.241612: step 35080, loss = 7138447360.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:29:13.558000: step 35090, loss = 7081556992.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:29:24.204588: step 35100, loss = 7025118720.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 19:29:34.492424: step 35110, loss = 6969131008.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 19:29:44.775394: step 35120, loss = 6913589248.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:29:55.077031: step 35130, loss = 6858490368.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:30:05.580920: step 35140, loss = 6803831296.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 19:30:15.964184: step 35150, loss = 6749606400.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:30:26.283773: step 35160, loss = 6695814144.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:30:36.602464: step 35170, loss = 6642449920.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:30:46.907783: step 35180, loss = 6589512192.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:30:57.247872: step 35190, loss = 6536995328.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:31:07.947848: step 35200, loss = 6484897280.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 19:31:18.268038: step 35210, loss = 6433214976.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:31:28.575937: step 35220, loss = 6381944320.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:31:38.881422: step 35230, loss = 6331082752.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:31:49.201549: step 35240, loss = 6280626176.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:31:59.535351: step 35250, loss = 6230572032.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:32:10.122805: step 35260, loss = 6180916224.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 19:32:20.644332: step 35270, loss = 6131655680.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 19:32:30.956977: step 35280, loss = 6082788864.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:32:41.269166: step 35290, loss = 6034310144.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:32:51.922249: step 35300, loss = 5986219008.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 19:33:02.274875: step 35310, loss = 5938512384.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:33:12.647840: step 35320, loss = 5891183104.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:33:22.974165: step 35330, loss = 5844231680.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 19:33:33.334025: step 35340, loss = 5797652992.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:33:43.676169: step 35350, loss = 5751444992.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:33:54.021876: step 35360, loss = 5705606656.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:34:04.368346: step 35370, loss = 5660135424.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:34:14.715477: step 35380, loss = 5615027200.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:34:25.065324: step 35390, loss = 5570275840.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:34:35.712434: step 35400, loss = 5525883392.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 19:34:45.995503: step 35410, loss = 5481843200.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:34:56.326952: step 35420, loss = 5438155776.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:35:06.729969: step 35430, loss = 5394813952.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:35:17.073543: step 35440, loss = 5351819264.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 19:35:27.394228: step 35450, loss = 5309168128.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:35:37.745122: step 35460, loss = 5266855936.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:35:48.057965: step 35470, loss = 5224881664.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:35:58.395991: step 35480, loss = 5183240704.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:36:08.790341: step 35490, loss = 5141931520.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 19:36:19.500247: step 35500, loss = 5100951552.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 19:36:29.901447: step 35510, loss = 5060299264.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 19:36:40.267260: step 35520, loss = 5019969536.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 19:36:50.573526: step 35530, loss = 4979961344.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:37:00.907975: step 35540, loss = 4940273664.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:37:11.284240: step 35550, loss = 4900900864.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 19:37:21.634721: step 35560, loss = 4861841920.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:37:31.919567: step 35570, loss = 4823095296.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:37:42.233617: step 35580, loss = 4784655872.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:37:52.554666: step 35590, loss = 4746523648.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:38:03.232186: step 35600, loss = 4708695040.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 19:38:13.585283: step 35610, loss = 4671169536.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:38:23.914007: step 35620, loss = 4633940992.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:38:34.252500: step 35630, loss = 4597010944.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:38:44.623235: step 35640, loss = 4560373248.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:38:54.926316: step 35650, loss = 4524029440.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 19:39:05.345440: step 35660, loss = 4487974400.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 19:39:15.725879: step 35670, loss = 4452206592.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:39:26.182131: step 35680, loss = 4416724480.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 19:39:36.552082: step 35690, loss = 4381524480.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:39:47.193672: step 35700, loss = 4346605056.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 19:39:57.486988: step 35710, loss = 4311964160.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 19:40:07.997570: step 35720, loss = 4277598720.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 19:40:18.355586: step 35730, loss = 4243507456.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:40:28.664253: step 35740, loss = 4209688576.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:40:39.011079: step 35750, loss = 4176139008.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:40:49.352784: step 35760, loss = 4142856192.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:40:59.705211: step 35770, loss = 4109839616.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:41:10.166466: step 35780, loss = 4077084672.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 19:41:20.528707: step 35790, loss = 4044592896.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:41:31.178290: step 35800, loss = 4012358656.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 19:41:41.499487: step 35810, loss = 3980382208.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:41:51.815611: step 35820, loss = 3948658944.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:42:02.160630: step 35830, loss = 3917189632.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:42:12.545412: step 35840, loss = 3885970688.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:42:22.905947: step 35850, loss = 3855001344.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:42:33.252139: step 35860, loss = 3824278528.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:42:43.588461: step 35870, loss = 3793799680.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:42:53.931775: step 35880, loss = 3763564800.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:43:04.279460: step 35890, loss = 3733570304.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:43:14.963151: step 35900, loss = 3703814656.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 19:43:25.275979: step 35910, loss = 3674296832.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:43:35.579790: step 35920, loss = 3645013760.00 (124.2 examples/sec; 1.030 sec/batch)
2018-04-09 19:43:45.906250: step 35930, loss = 3615964416.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 19:43:56.310267: step 35940, loss = 3587145728.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:44:06.713476: step 35950, loss = 3558557184.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 19:44:17.060574: step 35960, loss = 3530196480.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:44:27.420119: step 35970, loss = 3502062336.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:44:37.896533: step 35980, loss = 3474151424.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 19:44:48.233406: step 35990, loss = 3446464000.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:44:58.889042: step 36000, loss = 3418996736.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 19:45:09.220055: step 36010, loss = 3391748352.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:45:19.611134: step 36020, loss = 3364716800.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:45:29.887207: step 36030, loss = 3337901824.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 19:45:40.192603: step 36040, loss = 3311300352.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:45:50.590772: step 36050, loss = 3284909824.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 19:46:00.940498: step 36060, loss = 3258729216.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:46:11.325940: step 36070, loss = 3232757760.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:46:21.690779: step 36080, loss = 3206993664.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:46:32.002684: step 36090, loss = 3181435904.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:46:42.638123: step 36100, loss = 3156080128.00 (120.4 examples/sec; 1.064 sec/batch)
2018-04-09 19:46:52.938244: step 36110, loss = 3130927616.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:47:03.280044: step 36120, loss = 3105974784.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:47:13.650353: step 36130, loss = 3081220608.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:47:23.974662: step 36140, loss = 3056665088.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:47:34.383654: step 36150, loss = 3032304128.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 19:47:44.742385: step 36160, loss = 3008137472.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:47:55.062006: step 36170, loss = 2984163840.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:48:05.467129: step 36180, loss = 2960380416.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 19:48:15.830719: step 36190, loss = 2936786944.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:48:26.528456: step 36200, loss = 2913382144.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 19:48:36.812056: step 36210, loss = 2890163456.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 19:48:47.119696: step 36220, loss = 2867129856.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:48:57.450891: step 36230, loss = 2844279552.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:49:07.823992: step 36240, loss = 2821612288.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:49:18.208055: step 36250, loss = 2799124736.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:49:28.568322: step 36260, loss = 2776816640.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:49:38.939086: step 36270, loss = 2754686208.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:49:49.281931: step 36280, loss = 2732732928.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:49:59.820901: step 36290, loss = 2710953472.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 19:50:10.464055: step 36300, loss = 2689348096.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 19:50:20.827871: step 36310, loss = 2667914752.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:50:31.168515: step 36320, loss = 2646652672.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 19:50:41.516416: step 36330, loss = 2625559296.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:50:51.863713: step 36340, loss = 2604634112.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:51:02.225579: step 36350, loss = 2583875840.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:51:12.578122: step 36360, loss = 2563282688.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:51:22.946702: step 36370, loss = 2542853888.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:51:33.328980: step 36380, loss = 2522588672.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:51:43.685297: step 36390, loss = 2502484480.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 19:51:54.366607: step 36400, loss = 2482540288.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 19:52:04.759759: step 36410, loss = 2462755328.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:52:15.141974: step 36420, loss = 2443128320.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 19:52:25.536771: step 36430, loss = 2423657728.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 19:52:35.956257: step 36440, loss = 2404341504.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 19:52:46.345982: step 36450, loss = 2385179904.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:52:56.743931: step 36460, loss = 2366170624.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 19:53:07.192163: step 36470, loss = 2347312640.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 19:53:17.621941: step 36480, loss = 2328605696.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 19:53:27.984424: step 36490, loss = 2310047232.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:53:38.655273: step 36500, loss = 2291636992.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 19:53:49.007377: step 36510, loss = 2273373440.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 19:53:59.376952: step 36520, loss = 2255255552.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:54:09.764030: step 36530, loss = 2237282304.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 19:54:20.088627: step 36540, loss = 2219451648.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:54:30.412628: step 36550, loss = 2201763584.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:54:40.743935: step 36560, loss = 2184216320.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:54:51.060567: step 36570, loss = 2166808576.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:55:01.393038: step 36580, loss = 2149540096.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:55:11.767476: step 36590, loss = 2132408448.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:55:22.423623: step 36600, loss = 2115414144.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 19:55:32.724349: step 36610, loss = 2098555008.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:55:43.054648: step 36620, loss = 2081830272.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:55:53.379418: step 36630, loss = 2065238912.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:56:03.817760: step 36640, loss = 2048779648.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 19:56:14.280846: step 36650, loss = 2032451584.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 19:56:24.721841: step 36660, loss = 2016253824.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 19:56:35.032743: step 36670, loss = 2000184704.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 19:56:45.321051: step 36680, loss = 1984244096.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 19:56:55.644987: step 36690, loss = 1968430336.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:57:06.341579: step 36700, loss = 1952742656.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 19:57:16.671956: step 36710, loss = 1937179648.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:57:27.127758: step 36720, loss = 1921740672.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 19:57:37.456929: step 36730, loss = 1906425088.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 19:57:47.817739: step 36740, loss = 1891231104.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 19:57:58.167609: step 36750, loss = 1876158592.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 19:58:08.689979: step 36760, loss = 1861206272.00 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 19:58:19.014384: step 36770, loss = 1846373248.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:58:29.338667: step 36780, loss = 1831658112.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 19:58:39.648051: step 36790, loss = 1817060480.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:58:50.273469: step 36800, loss = 1802578944.00 (120.5 examples/sec; 1.063 sec/batch)
2018-04-09 19:59:00.591633: step 36810, loss = 1788212992.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 19:59:10.987503: step 36820, loss = 1773961472.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 19:59:21.357285: step 36830, loss = 1759823744.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 19:59:31.657095: step 36840, loss = 1745798656.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 19:59:41.964316: step 36850, loss = 1731885056.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 19:59:52.333203: step 36860, loss = 1718082432.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:00:02.840809: step 36870, loss = 1704389888.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 20:00:13.288562: step 36880, loss = 1690806400.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:00:23.645359: step 36890, loss = 1677331456.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 20:00:34.301949: step 36900, loss = 1663963520.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 20:00:44.622719: step 36910, loss = 1650702336.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:00:54.952832: step 36920, loss = 1637547008.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:01:05.431398: step 36930, loss = 1624496256.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 20:01:15.826771: step 36940, loss = 1611549440.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:01:26.176937: step 36950, loss = 1598705920.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:01:36.555779: step 36960, loss = 1585965056.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:01:46.876618: step 36970, loss = 1573325312.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:01:57.210617: step 36980, loss = 1560786304.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:02:07.624416: step 36990, loss = 1548347136.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:02:18.321491: step 37000, loss = 1536007680.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 20:02:28.629161: step 37010, loss = 1523766144.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 20:02:38.958529: step 37020, loss = 1511622144.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:02:49.280639: step 37030, loss = 1499574528.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:02:59.638030: step 37040, loss = 1487623936.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 20:03:10.039576: step 37050, loss = 1475768192.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:03:20.410507: step 37060, loss = 1464006656.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:03:30.753137: step 37070, loss = 1452338432.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:03:41.104011: step 37080, loss = 1440763392.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:03:51.443103: step 37090, loss = 1429280768.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:04:02.184301: step 37100, loss = 1417889792.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 20:04:12.569149: step 37110, loss = 1406590208.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:04:22.950831: step 37120, loss = 1395379968.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:04:33.324029: step 37130, loss = 1384259072.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:04:43.693746: step 37140, loss = 1373227136.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:04:54.143735: step 37150, loss = 1362282752.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:05:04.576523: step 37160, loss = 1351425792.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:05:14.944053: step 37170, loss = 1340655360.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:05:25.286532: step 37180, loss = 1329970816.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:05:35.665566: step 37190, loss = 1319371392.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:05:46.365826: step 37200, loss = 1308856448.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 20:05:56.790045: step 37210, loss = 1298425472.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:06:07.288606: step 37220, loss = 1288077312.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 20:06:17.717876: step 37230, loss = 1277811840.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:06:28.097035: step 37240, loss = 1267628032.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:06:38.459489: step 37250, loss = 1257525504.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:06:48.823577: step 37260, loss = 1247503488.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:06:59.188103: step 37270, loss = 1237561216.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:07:09.605423: step 37280, loss = 1227698304.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 20:07:19.959760: step 37290, loss = 1217914112.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 20:07:30.616551: step 37300, loss = 1208207360.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 20:07:40.896299: step 37310, loss = 1198578432.00 (124.5 examples/sec; 1.028 sec/batch)
2018-04-09 20:07:51.234508: step 37320, loss = 1189026048.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:08:01.629761: step 37330, loss = 1179550080.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:08:11.997752: step 37340, loss = 1170149504.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:08:22.355018: step 37350, loss = 1160823808.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 20:08:32.685420: step 37360, loss = 1151572352.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:08:42.985053: step 37370, loss = 1142394624.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 20:08:53.306535: step 37380, loss = 1133290368.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:09:03.721087: step 37390, loss = 1124258304.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:09:14.435513: step 37400, loss = 1115298432.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 20:09:24.864531: step 37410, loss = 1106409728.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:09:35.213802: step 37420, loss = 1097592064.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:09:45.580928: step 37430, loss = 1088844672.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:09:55.928594: step 37440, loss = 1080166912.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:10:06.433013: step 37450, loss = 1071558464.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 20:10:16.858932: step 37460, loss = 1063018496.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 20:10:27.150396: step 37470, loss = 1054546688.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-09 20:10:37.457169: step 37480, loss = 1046142272.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 20:10:47.772901: step 37490, loss = 1037805056.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 20:10:58.439142: step 37500, loss = 1029534080.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 20:11:08.816968: step 37510, loss = 1021329152.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:11:19.189760: step 37520, loss = 1013189440.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:11:29.522828: step 37530, loss = 1005114624.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:11:39.843821: step 37540, loss = 997104256.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:11:50.180895: step 37550, loss = 989157760.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:12:00.539097: step 37560, loss = 981274496.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 20:12:10.921996: step 37570, loss = 973454016.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:12:21.298638: step 37580, loss = 965696000.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 20:12:31.637448: step 37590, loss = 957999680.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:12:42.276423: step 37600, loss = 950364672.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 20:12:52.611548: step 37610, loss = 942790592.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:13:02.996564: step 37620, loss = 935276992.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 20:13:13.375820: step 37630, loss = 927823168.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:13:23.767119: step 37640, loss = 920428672.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:13:34.096895: step 37650, loss = 913093248.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:13:44.439134: step 37660, loss = 905816192.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:13:54.758410: step 37670, loss = 898597184.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:14:05.157409: step 37680, loss = 891435584.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:14:15.505822: step 37690, loss = 884331200.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:14:26.180949: step 37700, loss = 877283392.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 20:14:36.503563: step 37710, loss = 870291648.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:14:46.802028: step 37720, loss = 863355648.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 20:14:57.119159: step 37730, loss = 856474944.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 20:15:07.512761: step 37740, loss = 849649280.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:15:17.859279: step 37750, loss = 842877760.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:15:28.173575: step 37760, loss = 836160320.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 20:15:38.450299: step 37770, loss = 829496192.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-09 20:15:48.750865: step 37780, loss = 822885376.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 20:15:59.060239: step 37790, loss = 816327552.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 20:16:09.800112: step 37800, loss = 809821888.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 20:16:20.191821: step 37810, loss = 803368192.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:16:30.561207: step 37820, loss = 796965376.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:16:40.928679: step 37830, loss = 790613824.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:16:51.245661: step 37840, loss = 784312768.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 20:17:01.590272: step 37850, loss = 778062080.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 20:17:11.970617: step 37860, loss = 771861184.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:17:22.293372: step 37870, loss = 765709632.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:17:32.623992: step 37880, loss = 759607232.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:17:42.921412: step 37890, loss = 753553280.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 20:17:53.605762: step 37900, loss = 747547712.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 20:18:03.970374: step 37910, loss = 741590080.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:18:14.344940: step 37920, loss = 735679680.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:18:24.698268: step 37930, loss = 729816768.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 20:18:35.037897: step 37940, loss = 724000320.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:18:45.353793: step 37950, loss = 718230272.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 20:18:55.680653: step 37960, loss = 712506240.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:19:06.088874: step 37970, loss = 706827712.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 20:19:16.458066: step 37980, loss = 701194560.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:19:26.839364: step 37990, loss = 695606272.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:19:37.517238: step 38000, loss = 690062528.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 20:19:47.930628: step 38010, loss = 684563072.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:19:58.395851: step 38020, loss = 679107200.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 20:20:09.049159: step 38030, loss = 673695040.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 20:20:19.548366: step 38040, loss = 668325888.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 20:20:30.012147: step 38050, loss = 662999424.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 20:20:40.433783: step 38060, loss = 657715648.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:20:50.823659: step 38070, loss = 652473792.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:21:01.253749: step 38080, loss = 647273728.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:21:11.687137: step 38090, loss = 642115136.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:21:22.407648: step 38100, loss = 636997696.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 20:21:32.804904: step 38110, loss = 631921024.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:21:43.200617: step 38120, loss = 626884928.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:21:53.595218: step 38130, loss = 621888768.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 20:22:04.030410: step 38140, loss = 616932544.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 20:22:14.461574: step 38150, loss = 612015808.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:22:24.893038: step 38160, loss = 607138304.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:22:35.278563: step 38170, loss = 602299520.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:22:45.681002: step 38180, loss = 597499520.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:22:56.064734: step 38190, loss = 592737536.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:23:06.831171: step 38200, loss = 588013632.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 20:23:17.219805: step 38210, loss = 583327360.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:23:27.651406: step 38220, loss = 578678336.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:23:38.036901: step 38230, loss = 574066560.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:23:48.412449: step 38240, loss = 569491456.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 20:23:58.825800: step 38250, loss = 564952704.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:24:09.317848: step 38260, loss = 560450240.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 20:24:19.739940: step 38270, loss = 555983616.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:24:30.185500: step 38280, loss = 551552512.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:24:40.600354: step 38290, loss = 547156864.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:24:51.336026: step 38300, loss = 542796160.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 20:25:01.735409: step 38310, loss = 538470336.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:25:12.228425: step 38320, loss = 534178848.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 20:25:22.636091: step 38330, loss = 529921664.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 20:25:33.116349: step 38340, loss = 525698336.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 20:25:43.528671: step 38350, loss = 521508736.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:25:53.984812: step 38360, loss = 517352512.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 20:26:04.435576: step 38370, loss = 513229344.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:26:14.872131: step 38380, loss = 509139104.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:26:25.268103: step 38390, loss = 505081408.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:26:36.058803: step 38400, loss = 501056064.00 (118.6 examples/sec; 1.079 sec/batch)
2018-04-09 20:26:46.388184: step 38410, loss = 497062848.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:26:56.752262: step 38420, loss = 493101408.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:27:07.154987: step 38430, loss = 489171456.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:27:17.534616: step 38440, loss = 485272992.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:27:27.888273: step 38450, loss = 481405408.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 20:27:38.278011: step 38460, loss = 477568704.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:27:48.643408: step 38470, loss = 473762656.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:27:59.005513: step 38480, loss = 469986912.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:28:09.449122: step 38490, loss = 466241280.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:28:20.167681: step 38500, loss = 462525472.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 20:28:30.544774: step 38510, loss = 458839232.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:28:40.955703: step 38520, loss = 455182400.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:28:51.336287: step 38530, loss = 451554688.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:29:01.768058: step 38540, loss = 447955968.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:29:12.203780: step 38550, loss = 444385920.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 20:29:22.654777: step 38560, loss = 440844320.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:29:33.104578: step 38570, loss = 437330944.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:29:43.629875: step 38580, loss = 433845536.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 20:29:54.102438: step 38590, loss = 430387968.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 20:30:04.994557: step 38600, loss = 426957984.00 (117.5 examples/sec; 1.089 sec/batch)
2018-04-09 20:30:15.443079: step 38610, loss = 423555168.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:30:25.852219: step 38620, loss = 420179648.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 20:30:36.236087: step 38630, loss = 416830944.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:30:46.619658: step 38640, loss = 413508896.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:30:57.054643: step 38650, loss = 410213376.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:31:07.462692: step 38660, loss = 406944128.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 20:31:17.884520: step 38670, loss = 403700960.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:31:28.347719: step 38680, loss = 400483648.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 20:31:38.811384: step 38690, loss = 397291872.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 20:31:49.584291: step 38700, loss = 394125600.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 20:32:00.016520: step 38710, loss = 390984512.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:32:10.539093: step 38720, loss = 387868480.00 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 20:32:21.238987: step 38730, loss = 384777312.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 20:32:31.865199: step 38740, loss = 381710720.00 (120.5 examples/sec; 1.063 sec/batch)
2018-04-09 20:32:42.175925: step 38750, loss = 378668672.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 20:32:52.503365: step 38760, loss = 375650752.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:33:02.871567: step 38770, loss = 372656992.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:33:13.286458: step 38780, loss = 369687104.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:33:23.659375: step 38790, loss = 366740800.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:33:34.326685: step 38800, loss = 363817856.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 20:33:44.662000: step 38810, loss = 360918304.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:33:55.055103: step 38820, loss = 358041824.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:34:05.468133: step 38830, loss = 355188352.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:34:15.844095: step 38840, loss = 352357696.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 20:34:26.184562: step 38850, loss = 349549536.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:34:36.535224: step 38860, loss = 346763712.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:34:46.899116: step 38870, loss = 344000160.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:34:57.248582: step 38880, loss = 341258528.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:35:07.699696: step 38890, loss = 338538816.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:35:18.645244: step 38900, loss = 335840864.00 (116.9 examples/sec; 1.095 sec/batch)
2018-04-09 20:35:29.262119: step 38910, loss = 333164288.00 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 20:35:39.665605: step 38920, loss = 330509120.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:35:49.999931: step 38930, loss = 327875008.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:36:00.345720: step 38940, loss = 325262016.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:36:10.790221: step 38950, loss = 322669760.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:36:21.202469: step 38960, loss = 320098176.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:36:31.582796: step 38970, loss = 317547136.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:36:41.966125: step 38980, loss = 315016352.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:36:52.331189: step 38990, loss = 312505792.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:37:03.054335: step 39000, loss = 310015264.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 20:37:13.469398: step 39010, loss = 307544576.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 20:37:23.872895: step 39020, loss = 305093536.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:37:34.245088: step 39030, loss = 302662048.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:37:44.677802: step 39040, loss = 300249952.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:37:55.071727: step 39050, loss = 297856992.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 20:38:05.525758: step 39060, loss = 295483168.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 20:38:15.981158: step 39070, loss = 293128256.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 20:38:26.397276: step 39080, loss = 290792096.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 20:38:36.793420: step 39090, loss = 288474624.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:38:47.542578: step 39100, loss = 286175552.00 (119.1 examples/sec; 1.075 sec/batch)
2018-04-09 20:38:57.929875: step 39110, loss = 283894816.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:39:08.433566: step 39120, loss = 281632288.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 20:39:18.831144: step 39130, loss = 279387776.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:39:29.223631: step 39140, loss = 277161120.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:39:39.620657: step 39150, loss = 274952192.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:39:49.973736: step 39160, loss = 272760896.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 20:40:00.296915: step 39170, loss = 270587072.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 20:40:10.796647: step 39180, loss = 268430576.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 20:40:21.159682: step 39190, loss = 266291296.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:40:31.879258: step 39200, loss = 264169024.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 20:40:42.226439: step 39210, loss = 262063696.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:40:52.605010: step 39220, loss = 259975136.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:41:03.009416: step 39230, loss = 257903216.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:41:13.427830: step 39240, loss = 255847808.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 20:41:23.903146: step 39250, loss = 253808736.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 20:41:34.368415: step 39260, loss = 251785952.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 20:41:44.897686: step 39270, loss = 249779232.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 20:41:55.321861: step 39280, loss = 247788544.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:42:05.802358: step 39290, loss = 245813776.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 20:42:16.570172: step 39300, loss = 243854704.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 20:42:27.014739: step 39310, loss = 241911248.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:42:37.506010: step 39320, loss = 239983280.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 20:42:47.967831: step 39330, loss = 238070704.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 20:42:58.422015: step 39340, loss = 236173328.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 20:43:08.957142: step 39350, loss = 234291120.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 20:43:19.431857: step 39360, loss = 232423888.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 20:43:29.919671: step 39370, loss = 230571552.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 20:43:40.602728: step 39380, loss = 228733984.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 20:43:51.197725: step 39390, loss = 226911072.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 20:44:02.306103: step 39400, loss = 225102640.00 (115.2 examples/sec; 1.111 sec/batch)
2018-04-09 20:44:12.746495: step 39410, loss = 223308640.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:44:23.162175: step 39420, loss = 221528976.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 20:44:33.599776: step 39430, loss = 219763456.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:44:44.030900: step 39440, loss = 218012000.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:44:54.433664: step 39450, loss = 216274544.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:45:04.873005: step 39460, loss = 214550864.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 20:45:15.287047: step 39470, loss = 212840960.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:45:25.619914: step 39480, loss = 211144736.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:45:35.998027: step 39490, loss = 209461920.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:45:46.696890: step 39500, loss = 207792656.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 20:45:57.050090: step 39510, loss = 206136608.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 20:46:07.466276: step 39520, loss = 204493776.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 20:46:17.842456: step 39530, loss = 202864000.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 20:46:28.191785: step 39540, loss = 201247248.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:46:38.528696: step 39550, loss = 199643392.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:46:48.896484: step 39560, loss = 198052304.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:46:59.270503: step 39570, loss = 196473920.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:47:09.679984: step 39580, loss = 194908064.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 20:47:20.040384: step 39590, loss = 193354736.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:47:30.750066: step 39600, loss = 191813760.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 20:47:41.110898: step 39610, loss = 190285088.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:47:51.475083: step 39620, loss = 188768592.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:48:01.854784: step 39630, loss = 187264144.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:48:12.252218: step 39640, loss = 185771696.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:48:22.647667: step 39650, loss = 184291216.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:48:33.058517: step 39660, loss = 182822464.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:48:43.420361: step 39670, loss = 181365424.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:48:53.802608: step 39680, loss = 179920016.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:49:04.233842: step 39690, loss = 178486096.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:49:15.011787: step 39700, loss = 177063648.00 (118.8 examples/sec; 1.078 sec/batch)
2018-04-09 20:49:25.366365: step 39710, loss = 175652512.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 20:49:35.701802: step 39720, loss = 174252608.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:49:46.100397: step 39730, loss = 172863856.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:49:56.432844: step 39740, loss = 171486208.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:50:07.043607: step 39750, loss = 170119488.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 20:50:17.463979: step 39760, loss = 168763744.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:50:27.829145: step 39770, loss = 167418752.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:50:38.214032: step 39780, loss = 166084464.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:50:48.543602: step 39790, loss = 164760816.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:50:59.237278: step 39800, loss = 163447728.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 20:51:09.694858: step 39810, loss = 162145104.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 20:51:20.071871: step 39820, loss = 160852864.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:51:30.444327: step 39830, loss = 159570912.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:51:40.815645: step 39840, loss = 158299184.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:51:51.143014: step 39850, loss = 157037600.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:52:01.508609: step 39860, loss = 155786080.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:52:11.918194: step 39870, loss = 154544528.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 20:52:22.295343: step 39880, loss = 153312848.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:52:32.656051: step 39890, loss = 152090976.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:52:43.327343: step 39900, loss = 150878864.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 20:52:53.667556: step 39910, loss = 149676400.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 20:53:04.097724: step 39920, loss = 148483552.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 20:53:14.502356: step 39930, loss = 147300176.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 20:53:24.866135: step 39940, loss = 146126224.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:53:35.232409: step 39950, loss = 144961648.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 20:53:45.634243: step 39960, loss = 143806336.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:53:56.007770: step 39970, loss = 142660256.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:54:06.443472: step 39980, loss = 141523328.00 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 20:54:16.837700: step 39990, loss = 140395472.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 20:54:27.596463: step 40000, loss = 139276528.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 20:54:37.996962: step 40010, loss = 138166560.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 20:54:48.383956: step 40020, loss = 137065408.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:54:58.772503: step 40030, loss = 135973024.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:55:09.222156: step 40040, loss = 134889360.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 20:55:19.636531: step 40050, loss = 133814336.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 20:55:30.010192: step 40060, loss = 132747872.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:55:40.387671: step 40070, loss = 131689920.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 20:55:50.759436: step 40080, loss = 130640384.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:56:01.152513: step 40090, loss = 129599208.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:56:11.924724: step 40100, loss = 128566344.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 20:56:22.288459: step 40110, loss = 127541720.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:56:32.678165: step 40120, loss = 126525240.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 20:56:43.048884: step 40130, loss = 125516880.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 20:56:53.413470: step 40140, loss = 124516536.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 20:57:03.836079: step 40150, loss = 123524192.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 20:57:14.305852: step 40160, loss = 122539776.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 20:57:24.776765: step 40170, loss = 121563152.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 20:57:35.256670: step 40180, loss = 120594336.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 20:57:45.763091: step 40190, loss = 119633232.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 20:57:56.585504: step 40200, loss = 118679776.00 (118.3 examples/sec; 1.082 sec/batch)
2018-04-09 20:58:07.098560: step 40210, loss = 117733936.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 20:58:17.587632: step 40220, loss = 116795624.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 20:58:28.063498: step 40230, loss = 115864800.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 20:58:38.519504: step 40240, loss = 114941408.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 20:58:48.984199: step 40250, loss = 114025376.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 20:58:59.486084: step 40260, loss = 113116616.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 20:59:09.871370: step 40270, loss = 112215112.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 20:59:20.198775: step 40280, loss = 111320776.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 20:59:30.544753: step 40290, loss = 110433592.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 20:59:41.194489: step 40300, loss = 109553480.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 20:59:51.527596: step 40310, loss = 108680360.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:00:01.903818: step 40320, loss = 107814216.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 21:00:12.391902: step 40330, loss = 106954968.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 21:00:22.716286: step 40340, loss = 106102568.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 21:00:33.070470: step 40350, loss = 105256960.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:00:43.428465: step 40360, loss = 104418088.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:00:53.753879: step 40370, loss = 103585928.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 21:01:04.149394: step 40380, loss = 102760384.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:01:14.560168: step 40390, loss = 101941408.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:01:25.216977: step 40400, loss = 101128976.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 21:01:35.548270: step 40410, loss = 100323016.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:01:45.887359: step 40420, loss = 99523472.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:01:56.216295: step 40430, loss = 98730296.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:02:06.623048: step 40440, loss = 97943448.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:02:16.986975: step 40450, loss = 97162880.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:02:27.310151: step 40460, loss = 96388520.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 21:02:37.624158: step 40470, loss = 95620320.00 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 21:02:47.959455: step 40480, loss = 94858264.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:02:58.303064: step 40490, loss = 94102272.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 21:03:09.077698: step 40500, loss = 93352296.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 21:03:19.417667: step 40510, loss = 92608344.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:03:29.751630: step 40520, loss = 91870288.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:03:40.059283: step 40530, loss = 91138104.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 21:03:50.398893: step 40540, loss = 90411736.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:04:00.750461: step 40550, loss = 89691160.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:04:11.154795: step 40560, loss = 88976360.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 21:04:21.486165: step 40570, loss = 88267272.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:04:31.840005: step 40580, loss = 87563816.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:04:42.155038: step 40590, loss = 86865944.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 21:04:52.790024: step 40600, loss = 86173688.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 21:05:03.157385: step 40610, loss = 85486880.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 21:05:13.581882: step 40620, loss = 84805600.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:05:23.937706: step 40630, loss = 84129728.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:05:34.253468: step 40640, loss = 83459216.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 21:05:44.604223: step 40650, loss = 82794080.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:05:54.980126: step 40660, loss = 82134248.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 21:06:05.367171: step 40670, loss = 81479672.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:06:15.776676: step 40680, loss = 80830312.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:06:26.153713: step 40690, loss = 80186112.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:06:36.849363: step 40700, loss = 79547056.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 21:06:47.146410: step 40710, loss = 78913096.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 21:06:57.490650: step 40720, loss = 78284184.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 21:07:07.921826: step 40730, loss = 77660296.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:07:18.316042: step 40740, loss = 77041352.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 21:07:28.641031: step 40750, loss = 76427368.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 21:07:38.985314: step 40760, loss = 75818264.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 21:07:49.352581: step 40770, loss = 75214016.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 21:07:59.705372: step 40780, loss = 74614576.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:08:10.124879: step 40790, loss = 74019928.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:08:20.840028: step 40800, loss = 73430016.00 (119.5 examples/sec; 1.072 sec/batch)
2018-04-09 21:08:31.247533: step 40810, loss = 72844800.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:08:41.649433: step 40820, loss = 72264256.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:08:52.008729: step 40830, loss = 71688336.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:09:02.451801: step 40840, loss = 71117008.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:09:13.101040: step 40850, loss = 70550224.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 21:09:23.822105: step 40860, loss = 69987968.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 21:09:34.473858: step 40870, loss = 69430200.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 21:09:45.148246: step 40880, loss = 68876872.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 21:09:55.829530: step 40890, loss = 68327944.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 21:10:06.913874: step 40900, loss = 67783400.00 (115.5 examples/sec; 1.108 sec/batch)
2018-04-09 21:10:17.591089: step 40910, loss = 67243184.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 21:10:28.233227: step 40920, loss = 66707276.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 21:10:38.875608: step 40930, loss = 66175644.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 21:10:49.540995: step 40940, loss = 65648244.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 21:11:00.273918: step 40950, loss = 65125048.00 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 21:11:10.964295: step 40960, loss = 64606028.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 21:11:21.717346: step 40970, loss = 64091132.00 (119.0 examples/sec; 1.075 sec/batch)
2018-04-09 21:11:32.330397: step 40980, loss = 63580360.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 21:11:43.010456: step 40990, loss = 63073644.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 21:11:53.983261: step 41000, loss = 62570976.00 (116.7 examples/sec; 1.097 sec/batch)
2018-04-09 21:12:04.688431: step 41010, loss = 62072316.00 (119.6 examples/sec; 1.071 sec/batch)
2018-04-09 21:12:15.428147: step 41020, loss = 61577612.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 21:12:26.114609: step 41030, loss = 61086864.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 21:12:36.754599: step 41040, loss = 60600024.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 21:12:47.384339: step 41050, loss = 60117060.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 21:12:58.030064: step 41060, loss = 59637956.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 21:13:08.652479: step 41070, loss = 59162656.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 21:13:19.030800: step 41080, loss = 58691148.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:13:29.383546: step 41090, loss = 58223400.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:13:40.044896: step 41100, loss = 57759388.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 21:13:50.368519: step 41110, loss = 57299048.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 21:14:00.753648: step 41120, loss = 56842396.00 (123.3 examples/sec; 1.039 sec/batch)
2018-04-09 21:14:11.157998: step 41130, loss = 56389376.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 21:14:21.541689: step 41140, loss = 55939968.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:14:31.910238: step 41150, loss = 55494144.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 21:14:42.396186: step 41160, loss = 55051884.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 21:14:53.189586: step 41170, loss = 54613144.00 (118.6 examples/sec; 1.079 sec/batch)
2018-04-09 21:15:04.152678: step 41180, loss = 54177892.00 (116.8 examples/sec; 1.096 sec/batch)
2018-04-09 21:15:14.837904: step 41190, loss = 53746112.00 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 21:15:25.711075: step 41200, loss = 53317780.00 (117.7 examples/sec; 1.087 sec/batch)
2018-04-09 21:15:36.278276: step 41210, loss = 52892852.00 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 21:15:46.892823: step 41220, loss = 52471312.00 (120.6 examples/sec; 1.061 sec/batch)
2018-04-09 21:15:57.553688: step 41230, loss = 52053132.00 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 21:16:08.292396: step 41240, loss = 51638276.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 21:16:18.981739: step 41250, loss = 51226732.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 21:16:29.783302: step 41260, loss = 50818488.00 (118.5 examples/sec; 1.080 sec/batch)
2018-04-09 21:16:40.545930: step 41270, loss = 50413480.00 (118.9 examples/sec; 1.076 sec/batch)
2018-04-09 21:16:51.100153: step 41280, loss = 50011688.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 21:17:01.480058: step 41290, loss = 49613112.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:17:12.312632: step 41300, loss = 49217716.00 (118.2 examples/sec; 1.083 sec/batch)
2018-04-09 21:17:22.673953: step 41310, loss = 48825472.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:17:33.058806: step 41320, loss = 48436340.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:17:43.442660: step 41330, loss = 48050328.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:17:53.856649: step 41340, loss = 47667388.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:18:04.302786: step 41350, loss = 47287492.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:18:14.806371: step 41360, loss = 46910628.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 21:18:25.199451: step 41370, loss = 46536764.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:18:35.620650: step 41380, loss = 46165888.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:18:46.084537: step 41390, loss = 45797956.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 21:18:56.995450: step 41400, loss = 45432964.00 (117.3 examples/sec; 1.091 sec/batch)
2018-04-09 21:19:07.454030: step 41410, loss = 45070888.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:19:17.915230: step 41420, loss = 44711688.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:19:28.362347: step 41430, loss = 44355356.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:19:38.775939: step 41440, loss = 44001856.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:19:49.233869: step 41450, loss = 43651176.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:19:59.694366: step 41460, loss = 43303296.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:20:10.286669: step 41470, loss = 42958176.00 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 21:20:20.752177: step 41480, loss = 42615812.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 21:20:31.186214: step 41490, loss = 42276176.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:20:41.947664: step 41500, loss = 41939248.00 (118.9 examples/sec; 1.076 sec/batch)
2018-04-09 21:20:52.370829: step 41510, loss = 41605016.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:21:02.898664: step 41520, loss = 41273428.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 21:21:13.404166: step 41530, loss = 40944492.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 21:21:23.893040: step 41540, loss = 40618180.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 21:21:34.365356: step 41550, loss = 40294472.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 21:21:44.824142: step 41560, loss = 39973344.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:21:55.258168: step 41570, loss = 39654764.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:22:05.778334: step 41580, loss = 39338728.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 21:22:16.299833: step 41590, loss = 39025216.00 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 21:22:27.131981: step 41600, loss = 38714196.00 (118.2 examples/sec; 1.083 sec/batch)
2018-04-09 21:22:37.575650: step 41610, loss = 38405656.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:22:48.043004: step 41620, loss = 38099576.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 21:22:58.506168: step 41630, loss = 37795936.00 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 21:23:08.984429: step 41640, loss = 37494716.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 21:23:19.454046: step 41650, loss = 37195896.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 21:23:29.914449: step 41660, loss = 36899452.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:23:40.335477: step 41670, loss = 36605376.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:23:50.788329: step 41680, loss = 36313644.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:24:01.242553: step 41690, loss = 36024232.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 21:24:12.098087: step 41700, loss = 35737136.00 (117.9 examples/sec; 1.086 sec/batch)
2018-04-09 21:24:22.527866: step 41710, loss = 35452324.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:24:32.967433: step 41720, loss = 35169776.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:24:43.363946: step 41730, loss = 34889488.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:24:53.727520: step 41740, loss = 34611436.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:25:04.137151: step 41750, loss = 34335596.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:25:14.596930: step 41760, loss = 34061952.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:25:24.997790: step 41770, loss = 33790492.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:25:35.436602: step 41780, loss = 33521194.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:25:45.850608: step 41790, loss = 33254036.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:25:56.606892: step 41800, loss = 32989012.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 21:26:07.089808: step 41810, loss = 32726102.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:26:17.513276: step 41820, loss = 32465284.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:26:27.942388: step 41830, loss = 32206548.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:26:38.353748: step 41840, loss = 31949872.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:26:48.791581: step 41850, loss = 31695238.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:26:59.205582: step 41860, loss = 31442640.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:27:09.678681: step 41870, loss = 31192052.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 21:27:20.106362: step 41880, loss = 30943466.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 21:27:30.510562: step 41890, loss = 30696858.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 21:27:41.200978: step 41900, loss = 30452218.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 21:27:51.580638: step 41910, loss = 30209518.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:28:01.961746: step 41920, loss = 29968756.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:28:12.339834: step 41930, loss = 29729916.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:28:22.669103: step 41940, loss = 29492978.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:28:33.015378: step 41950, loss = 29257930.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:28:43.366015: step 41960, loss = 29024752.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:28:53.689863: step 41970, loss = 28793436.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 21:29:04.077083: step 41980, loss = 28563962.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:29:14.463337: step 41990, loss = 28336322.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:29:25.141702: step 42000, loss = 28110484.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 21:29:35.486488: step 42010, loss = 27886452.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 21:29:45.834701: step 42020, loss = 27664206.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:29:56.183549: step 42030, loss = 27443734.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:30:06.734960: step 42040, loss = 27225020.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 21:30:17.094612: step 42050, loss = 27008044.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:30:27.462788: step 42060, loss = 26792798.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 21:30:37.815082: step 42070, loss = 26579270.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:30:48.123117: step 42080, loss = 26367440.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 21:30:58.466407: step 42090, loss = 26157302.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:31:09.261377: step 42100, loss = 25948836.00 (118.6 examples/sec; 1.079 sec/batch)
2018-04-09 21:31:19.703236: step 42110, loss = 25742034.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:31:30.089921: step 42120, loss = 25536880.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:31:40.480682: step 42130, loss = 25333360.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:31:50.856631: step 42140, loss = 25131456.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 21:32:01.255609: step 42150, loss = 24931166.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:32:11.665921: step 42160, loss = 24732476.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:32:22.039897: step 42170, loss = 24535366.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 21:32:32.520464: step 42180, loss = 24339830.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:32:42.943183: step 42190, loss = 24145846.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:32:53.682855: step 42200, loss = 23953414.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 21:33:04.108360: step 42210, loss = 23762514.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 21:33:14.584432: step 42220, loss = 23573138.00 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 21:33:25.011216: step 42230, loss = 23385254.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 21:33:35.450385: step 42240, loss = 23198894.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:33:45.934326: step 42250, loss = 23014010.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:33:56.383300: step 42260, loss = 22830596.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:34:06.921616: step 42270, loss = 22648638.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 21:34:17.416737: step 42280, loss = 22468136.00 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 21:34:27.877002: step 42290, loss = 22289072.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:34:38.649738: step 42300, loss = 22111442.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 21:34:49.052013: step 42310, loss = 21935220.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:34:59.776212: step 42320, loss = 21760396.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 21:35:10.543273: step 42330, loss = 21586982.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 21:35:21.266273: step 42340, loss = 21414934.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 21:35:31.983280: step 42350, loss = 21244272.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 21:35:42.742863: step 42360, loss = 21074956.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 21:35:53.432453: step 42370, loss = 20907000.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 21:36:04.178295: step 42380, loss = 20740378.00 (119.1 examples/sec; 1.075 sec/batch)
2018-04-09 21:36:14.662252: step 42390, loss = 20575086.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:36:25.421470: step 42400, loss = 20411108.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 21:36:35.822495: step 42410, loss = 20248440.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:36:46.221323: step 42420, loss = 20087066.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:36:56.610567: step 42430, loss = 19926978.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:37:07.042382: step 42440, loss = 19768170.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:37:17.462467: step 42450, loss = 19610622.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:37:27.809073: step 42460, loss = 19454332.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:37:38.179404: step 42470, loss = 19299290.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 21:37:48.541553: step 42480, loss = 19145480.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:37:58.917937: step 42490, loss = 18992896.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 21:38:09.675390: step 42500, loss = 18841528.00 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 21:38:20.229932: step 42510, loss = 18691368.00 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 21:38:30.641292: step 42520, loss = 18542400.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:38:41.120890: step 42530, loss = 18394624.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:38:51.540530: step 42540, loss = 18248024.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:39:01.988057: step 42550, loss = 18102600.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:39:12.400105: step 42560, loss = 17958324.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 21:39:22.725415: step 42570, loss = 17815202.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 21:39:33.069027: step 42580, loss = 17673220.00 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 21:39:43.551415: step 42590, loss = 17532372.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:39:54.413525: step 42600, loss = 17392642.00 (117.8 examples/sec; 1.086 sec/batch)
2018-04-09 21:40:05.031767: step 42610, loss = 17254030.00 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 21:40:15.594868: step 42620, loss = 17116522.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 21:40:25.987786: step 42630, loss = 16980108.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:40:36.406729: step 42640, loss = 16844784.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 21:40:46.862419: step 42650, loss = 16710533.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:40:57.370166: step 42660, loss = 16577357.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 21:41:08.149335: step 42670, loss = 16445241.00 (118.7 examples/sec; 1.078 sec/batch)
2018-04-09 21:41:18.885379: step 42680, loss = 16314176.00 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 21:41:29.560942: step 42690, loss = 16184159.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 21:41:40.227173: step 42700, loss = 16055175.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 21:41:50.545511: step 42710, loss = 15927223.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 21:42:00.891804: step 42720, loss = 15800286.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:42:11.314163: step 42730, loss = 15674362.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:42:21.647920: step 42740, loss = 15549442.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:42:31.996020: step 42750, loss = 15425519.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:42:42.389957: step 42760, loss = 15302582.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 21:42:52.722581: step 42770, loss = 15180625.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:43:03.157020: step 42780, loss = 15059639.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:43:13.578054: step 42790, loss = 14939620.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:43:24.275725: step 42800, loss = 14820556.00 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 21:43:34.623780: step 42810, loss = 14702440.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:43:45.004949: step 42820, loss = 14585265.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:43:55.359224: step 42830, loss = 14469023.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:44:05.792057: step 42840, loss = 14353708.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:44:16.211978: step 42850, loss = 14239316.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:44:26.787670: step 42860, loss = 14125835.00 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 21:44:37.469576: step 42870, loss = 14013253.00 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 21:44:48.168620: step 42880, loss = 13901572.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 21:44:58.846253: step 42890, loss = 13790782.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 21:45:09.890035: step 42900, loss = 13680871.00 (115.9 examples/sec; 1.104 sec/batch)
2018-04-09 21:45:20.447740: step 42910, loss = 13571837.00 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 21:45:30.835985: step 42920, loss = 13463676.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:45:41.319913: step 42930, loss = 13356375.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:45:51.666038: step 42940, loss = 13249928.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:46:02.072258: step 42950, loss = 13144331.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:46:12.493365: step 42960, loss = 13039575.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:46:22.840237: step 42970, loss = 12935658.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:46:33.232888: step 42980, loss = 12832567.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:46:43.585418: step 42990, loss = 12730295.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:46:54.256961: step 43000, loss = 12628838.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 21:47:04.684713: step 43010, loss = 12528194.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:47:15.094342: step 43020, loss = 12428348.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:47:25.467350: step 43030, loss = 12329297.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 21:47:36.097847: step 43040, loss = 12231040.00 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 21:47:46.529869: step 43050, loss = 12133563.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:47:56.952001: step 43060, loss = 12036861.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 21:48:07.410755: step 43070, loss = 11940931.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:48:17.847514: step 43080, loss = 11845766.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:48:28.187729: step 43090, loss = 11751359.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:48:38.908514: step 43100, loss = 11657705.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 21:48:49.272906: step 43110, loss = 11564794.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:48:59.630850: step 43120, loss = 11472626.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:49:10.026323: step 43130, loss = 11381194.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:49:20.381571: step 43140, loss = 11290490.00 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:49:30.700381: step 43150, loss = 11200509.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 21:49:41.036036: step 43160, loss = 11111243.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:49:51.374551: step 43170, loss = 11022690.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:50:01.767732: step 43180, loss = 10934843.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:50:12.271633: step 43190, loss = 10847694.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 21:50:22.964399: step 43200, loss = 10761244.00 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 21:50:33.292346: step 43210, loss = 10675478.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:50:43.683117: step 43220, loss = 10590399.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:50:54.033907: step 43230, loss = 10505996.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:51:04.491858: step 43240, loss = 10422267.00 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 21:51:14.897151: step 43250, loss = 10339203.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 21:51:25.290988: step 43260, loss = 10256805.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 21:51:35.661997: step 43270, loss = 10175064.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 21:51:45.994434: step 43280, loss = 10093972.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:51:56.333198: step 43290, loss = 10013528.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:52:07.036916: step 43300, loss = 9933722.00 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 21:52:17.384285: step 43310, loss = 9854554.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:52:27.714358: step 43320, loss = 9776016.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:52:38.062271: step 43330, loss = 9698105.00 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 21:52:48.394481: step 43340, loss = 9620812.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:52:58.748912: step 43350, loss = 9544138.00 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:53:09.247031: step 43360, loss = 9468076.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 21:53:19.731122: step 43370, loss = 9392618.00 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 21:53:30.091552: step 43380, loss = 9317762.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:53:40.524356: step 43390, loss = 9243501.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:53:51.200167: step 43400, loss = 9169832.00 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 21:54:01.569278: step 43410, loss = 9096752.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 21:54:12.058087: step 43420, loss = 9024256.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 21:54:22.565925: step 43430, loss = 8952336.00 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 21:54:32.983893: step 43440, loss = 8880986.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 21:54:43.401607: step 43450, loss = 8810205.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 21:54:53.890342: step 43460, loss = 8739989.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 21:55:04.376315: step 43470, loss = 8670335.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 21:55:14.827707: step 43480, loss = 8601235.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:55:25.211265: step 43490, loss = 8532687.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:55:35.882780: step 43500, loss = 8464682.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 21:55:46.224180: step 43510, loss = 8397223.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 21:55:56.591999: step 43520, loss = 8330300.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 21:56:07.041272: step 43530, loss = 8263909.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 21:56:17.430851: step 43540, loss = 8198050.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 21:56:27.810345: step 43550, loss = 8132713.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:56:38.181226: step 43560, loss = 8067900.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 21:56:48.535900: step 43570, loss = 8003600.50 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 21:56:58.893529: step 43580, loss = 7939813.50 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:57:09.330561: step 43590, loss = 7876536.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 21:57:20.041394: step 43600, loss = 7813763.50 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 21:57:30.339359: step 43610, loss = 7751489.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 21:57:40.702913: step 43620, loss = 7689712.50 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 21:57:51.131960: step 43630, loss = 7628427.50 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 21:58:01.509455: step 43640, loss = 7567631.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 21:58:11.936008: step 43650, loss = 7507319.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 21:58:22.294816: step 43660, loss = 7447487.50 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 21:58:32.623033: step 43670, loss = 7388134.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:58:42.954704: step 43680, loss = 7329253.50 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 21:58:53.299091: step 43690, loss = 7270842.50 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 21:59:04.035696: step 43700, loss = 7212895.50 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 21:59:14.431827: step 43710, loss = 7155410.50 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 21:59:24.796995: step 43720, loss = 7098384.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 21:59:35.325018: step 43730, loss = 7041811.50 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 21:59:45.812380: step 43740, loss = 6985691.00 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 21:59:56.276172: step 43750, loss = 6930017.50 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 22:00:06.925958: step 43760, loss = 6874788.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 22:00:17.686598: step 43770, loss = 6819997.50 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 22:00:28.266934: step 43780, loss = 6765645.50 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 22:00:39.037444: step 43790, loss = 6711725.00 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 22:00:49.805726: step 43800, loss = 6658235.00 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 22:01:00.283402: step 43810, loss = 6605171.50 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 22:01:10.922556: step 43820, loss = 6552530.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 22:01:21.498307: step 43830, loss = 6500308.50 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 22:01:31.949699: step 43840, loss = 6448502.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:01:42.312687: step 43850, loss = 6397110.50 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:01:52.726896: step 43860, loss = 6346127.50 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:02:03.156059: step 43870, loss = 6295550.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:02:13.629189: step 43880, loss = 6245377.50 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 22:02:23.979009: step 43890, loss = 6195604.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 22:02:34.731056: step 43900, loss = 6146227.00 (119.0 examples/sec; 1.075 sec/batch)
2018-04-09 22:02:45.130936: step 43910, loss = 6097243.50 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:02:55.536400: step 43920, loss = 6048650.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:03:05.955334: step 43930, loss = 6000446.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:03:16.427866: step 43940, loss = 5952622.00 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 22:03:26.805456: step 43950, loss = 5905183.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:03:37.205659: step 43960, loss = 5858119.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:03:47.600452: step 43970, loss = 5811433.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 22:03:58.008644: step 43980, loss = 5765119.50 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:04:08.476655: step 43990, loss = 5719174.00 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:04:19.283154: step 44000, loss = 5673593.50 (118.4 examples/sec; 1.081 sec/batch)
2018-04-09 22:04:29.739600: step 44010, loss = 5628376.50 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 22:04:40.182315: step 44020, loss = 5583520.50 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:04:50.636011: step 44030, loss = 5539023.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 22:05:01.151032: step 44040, loss = 5494880.50 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 22:05:11.675512: step 44050, loss = 5451086.50 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 22:05:22.137888: step 44060, loss = 5407643.50 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 22:05:32.487652: step 44070, loss = 5364546.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 22:05:42.892350: step 44080, loss = 5321793.50 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:05:53.288262: step 44090, loss = 5279379.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:06:04.102712: step 44100, loss = 5237304.50 (118.4 examples/sec; 1.081 sec/batch)
2018-04-09 22:06:14.598803: step 44110, loss = 5195566.00 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 22:06:25.247209: step 44120, loss = 5154159.00 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 22:06:35.657642: step 44130, loss = 5113082.50 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:06:46.106038: step 44140, loss = 5072333.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:06:56.526602: step 44150, loss = 5031908.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:07:06.980213: step 44160, loss = 4991805.00 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 22:07:17.410822: step 44170, loss = 4952021.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:07:27.817406: step 44180, loss = 4912556.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:07:38.193652: step 44190, loss = 4873404.00 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:07:48.908247: step 44200, loss = 4834564.00 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 22:07:59.311402: step 44210, loss = 4796034.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:08:09.749400: step 44220, loss = 4757811.50 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:08:20.196020: step 44230, loss = 4719892.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:08:30.568325: step 44240, loss = 4682276.50 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:08:40.944439: step 44250, loss = 4644960.50 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:08:51.310379: step 44260, loss = 4607941.50 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 22:09:01.801420: step 44270, loss = 4571217.50 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 22:09:12.331481: step 44280, loss = 4534787.00 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 22:09:22.779653: step 44290, loss = 4498646.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:09:33.528160: step 44300, loss = 4462793.00 (119.1 examples/sec; 1.075 sec/batch)
2018-04-09 22:09:43.903794: step 44310, loss = 4427226.50 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:09:54.314170: step 44320, loss = 4391943.50 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:10:04.759825: step 44330, loss = 4356941.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:10:15.346877: step 44340, loss = 4322216.00 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 22:10:25.774369: step 44350, loss = 4287769.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 22:10:36.203018: step 44360, loss = 4253598.00 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:10:46.856670: step 44370, loss = 4219697.50 (120.1 examples/sec; 1.065 sec/batch)
2018-04-09 22:10:57.349543: step 44380, loss = 4186068.75 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 22:11:07.929303: step 44390, loss = 4152707.25 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 22:11:18.790518: step 44400, loss = 4119611.50 (117.9 examples/sec; 1.086 sec/batch)
2018-04-09 22:11:29.216869: step 44410, loss = 4086779.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 22:11:39.607455: step 44420, loss = 4054208.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:11:50.015995: step 44430, loss = 4021898.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:12:00.453424: step 44440, loss = 3989844.25 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:12:10.928992: step 44450, loss = 3958046.25 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 22:12:21.370463: step 44460, loss = 3926502.75 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:12:31.772001: step 44470, loss = 3895210.50 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:12:42.181624: step 44480, loss = 3864166.50 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:12:52.596567: step 44490, loss = 3833371.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:13:03.394111: step 44500, loss = 3802819.75 (118.5 examples/sec; 1.080 sec/batch)
2018-04-09 22:13:13.822588: step 44510, loss = 3772512.50 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:13:24.196198: step 44520, loss = 3742447.25 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:13:34.525306: step 44530, loss = 3712621.50 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 22:13:44.863360: step 44540, loss = 3683032.25 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:13:55.185523: step 44550, loss = 3653680.00 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 22:14:05.601260: step 44560, loss = 3624561.25 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:14:16.005753: step 44570, loss = 3595674.25 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:14:26.376172: step 44580, loss = 3567018.25 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:14:36.712637: step 44590, loss = 3538590.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:14:47.396734: step 44600, loss = 3510388.75 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 22:14:57.740792: step 44610, loss = 3482412.25 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 22:15:08.145240: step 44620, loss = 3454659.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:15:18.564303: step 44630, loss = 3427126.50 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:15:28.933755: step 44640, loss = 3399813.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:15:39.322127: step 44650, loss = 3372717.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:15:49.714762: step 44660, loss = 3345838.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:16:00.105093: step 44670, loss = 3319173.25 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:16:10.611263: step 44680, loss = 3292720.25 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 22:16:21.048070: step 44690, loss = 3266479.25 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:16:31.791767: step 44700, loss = 3240446.25 (119.1 examples/sec; 1.074 sec/batch)
2018-04-09 22:16:42.197514: step 44710, loss = 3214620.25 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:16:52.573113: step 44720, loss = 3188999.50 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:17:03.008722: step 44730, loss = 3163583.25 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 22:17:13.459747: step 44740, loss = 3138370.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:17:23.849136: step 44750, loss = 3113357.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:17:34.239336: step 44760, loss = 3088545.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:17:44.663113: step 44770, loss = 3063930.50 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:17:55.140756: step 44780, loss = 3039512.50 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 22:18:05.590482: step 44790, loss = 3015288.75 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:18:16.338332: step 44800, loss = 2991258.00 (119.1 examples/sec; 1.075 sec/batch)
2018-04-09 22:18:26.725198: step 44810, loss = 2967418.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:18:37.108388: step 44820, loss = 2943769.25 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:18:47.476784: step 44830, loss = 2920308.00 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 22:18:57.878529: step 44840, loss = 2897034.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:19:08.303290: step 44850, loss = 2873945.50 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:19:18.699483: step 44860, loss = 2851041.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:19:29.107627: step 44870, loss = 2828319.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:19:39.462138: step 44880, loss = 2805777.75 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:19:49.822986: step 44890, loss = 2783417.25 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:20:00.491758: step 44900, loss = 2761234.75 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 22:20:11.021391: step 44910, loss = 2739228.25 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 22:20:21.415608: step 44920, loss = 2717397.50 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 22:20:31.796490: step 44930, loss = 2695740.25 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:20:42.124113: step 44940, loss = 2674257.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 22:20:52.506189: step 44950, loss = 2652944.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:21:02.945356: step 44960, loss = 2631801.00 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:21:13.368210: step 44970, loss = 2610826.50 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:21:23.797507: step 44980, loss = 2590018.25 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:21:34.336216: step 44990, loss = 2569377.00 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 22:21:45.393791: step 45000, loss = 2548900.00 (115.8 examples/sec; 1.106 sec/batch)
2018-04-09 22:21:56.068699: step 45010, loss = 2528586.00 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 22:22:06.668059: step 45020, loss = 2508434.25 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 22:22:17.103425: step 45030, loss = 2488442.75 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 22:22:27.484967: step 45040, loss = 2468610.75 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:22:37.891967: step 45050, loss = 2448937.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:22:48.264473: step 45060, loss = 2429419.75 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:22:58.622257: step 45070, loss = 2410057.75 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 22:23:09.063139: step 45080, loss = 2390850.50 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:23:19.432105: step 45090, loss = 2371796.50 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:23:30.140595: step 45100, loss = 2352894.25 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 22:23:40.478929: step 45110, loss = 2334142.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:23:50.830941: step 45120, loss = 2315539.75 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:24:01.182789: step 45130, loss = 2297085.75 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:24:11.704881: step 45140, loss = 2278778.50 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 22:24:22.128989: step 45150, loss = 2260617.50 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:24:32.524526: step 45160, loss = 2242601.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:24:42.915105: step 45170, loss = 2224728.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:24:53.322529: step 45180, loss = 2206998.50 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:25:03.795276: step 45190, loss = 2189409.50 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 22:25:14.500925: step 45200, loss = 2171960.50 (119.6 examples/sec; 1.071 sec/batch)
2018-04-09 22:25:24.841230: step 45210, loss = 2154650.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:25:35.210576: step 45220, loss = 2137478.75 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:25:45.537747: step 45230, loss = 2120444.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 22:25:55.932631: step 45240, loss = 2103545.00 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 22:26:06.416987: step 45250, loss = 2086780.38 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 22:26:16.869984: step 45260, loss = 2070149.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:26:27.247237: step 45270, loss = 2053651.00 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:26:37.658867: step 45280, loss = 2037284.38 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:26:48.080124: step 45290, loss = 2021047.75 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:26:58.769358: step 45300, loss = 2004940.75 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 22:27:09.199907: step 45310, loss = 1988961.75 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:27:19.590967: step 45320, loss = 1973110.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:27:29.975845: step 45330, loss = 1957385.62 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:27:40.375474: step 45340, loss = 1941785.88 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:27:50.772387: step 45350, loss = 1926310.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:28:01.194043: step 45360, loss = 1910959.00 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:28:11.635916: step 45370, loss = 1895728.88 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:28:22.052140: step 45380, loss = 1880620.50 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:28:32.472764: step 45390, loss = 1865632.38 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:28:43.191723: step 45400, loss = 1850763.88 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 22:28:53.576138: step 45410, loss = 1836013.88 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:29:04.028268: step 45420, loss = 1821381.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:29:14.552596: step 45430, loss = 1806865.75 (121.6 examples/sec; 1.052 sec/batch)
2018-04-09 22:29:25.026711: step 45440, loss = 1792465.50 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 22:29:35.448981: step 45450, loss = 1778180.88 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:29:45.806451: step 45460, loss = 1764009.12 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 22:29:56.151276: step 45470, loss = 1749950.25 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 22:30:06.604324: step 45480, loss = 1736004.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:30:17.186702: step 45490, loss = 1722168.38 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 22:30:28.003475: step 45500, loss = 1708443.12 (118.3 examples/sec; 1.082 sec/batch)
2018-04-09 22:30:38.468812: step 45510, loss = 1694827.50 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:30:48.957672: step 45520, loss = 1681320.50 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 22:30:59.463915: step 45530, loss = 1667921.38 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 22:31:10.017824: step 45540, loss = 1654628.25 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 22:31:20.479394: step 45550, loss = 1641441.88 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 22:31:31.182125: step 45560, loss = 1628359.62 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 22:31:41.820070: step 45570, loss = 1615382.00 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 22:31:52.222831: step 45580, loss = 1602508.00 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:32:02.726591: step 45590, loss = 1589736.62 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 22:32:13.481179: step 45600, loss = 1577067.25 (119.0 examples/sec; 1.075 sec/batch)
2018-04-09 22:32:23.798174: step 45610, loss = 1564498.12 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 22:32:34.226114: step 45620, loss = 1552029.75 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:32:44.629741: step 45630, loss = 1539660.62 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:32:55.041027: step 45640, loss = 1527389.75 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:33:05.506844: step 45650, loss = 1515217.25 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:33:16.148667: step 45660, loss = 1503141.50 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 22:33:26.809985: step 45670, loss = 1491161.62 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 22:33:37.481039: step 45680, loss = 1479278.00 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 22:33:47.954327: step 45690, loss = 1467488.50 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 22:33:58.670429: step 45700, loss = 1455792.75 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 22:34:09.117089: step 45710, loss = 1444191.12 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:34:19.556861: step 45720, loss = 1432681.38 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:34:29.954703: step 45730, loss = 1421263.75 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:34:40.409447: step 45740, loss = 1409936.62 (122.4 examples/sec; 1.045 sec/batch)
2018-04-09 22:34:50.798778: step 45750, loss = 1398699.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:35:01.219535: step 45760, loss = 1387553.38 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:35:11.662879: step 45770, loss = 1376495.50 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:35:22.086345: step 45780, loss = 1365524.38 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:35:32.485419: step 45790, loss = 1354642.00 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:35:43.265235: step 45800, loss = 1343845.88 (118.7 examples/sec; 1.078 sec/batch)
2018-04-09 22:35:53.634276: step 45810, loss = 1333136.00 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:36:04.093137: step 45820, loss = 1322511.12 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 22:36:14.713236: step 45830, loss = 1311970.88 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 22:36:25.090886: step 45840, loss = 1301515.12 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:36:35.555356: step 45850, loss = 1291142.75 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 22:36:46.022782: step 45860, loss = 1280852.62 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:36:56.442353: step 45870, loss = 1270644.62 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:37:06.894513: step 45880, loss = 1260518.12 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:37:17.460737: step 45890, loss = 1250472.38 (121.1 examples/sec; 1.057 sec/batch)
2018-04-09 22:37:28.182602: step 45900, loss = 1240506.00 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 22:37:38.537114: step 45910, loss = 1230619.62 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:37:48.921403: step 45920, loss = 1220812.12 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:37:59.309082: step 45930, loss = 1211082.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:38:09.798079: step 45940, loss = 1201431.00 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 22:38:20.231198: step 45950, loss = 1191856.12 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:38:30.653895: step 45960, loss = 1182357.25 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:38:41.051917: step 45970, loss = 1172934.25 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:38:51.441222: step 45980, loss = 1163586.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:39:01.875406: step 45990, loss = 1154313.12 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:39:12.651121: step 46000, loss = 1145113.25 (118.8 examples/sec; 1.078 sec/batch)
2018-04-09 22:39:23.062125: step 46010, loss = 1135987.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:39:33.455156: step 46020, loss = 1126933.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:39:43.879214: step 46030, loss = 1117952.12 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:39:54.264920: step 46040, loss = 1109042.88 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:40:04.695566: step 46050, loss = 1100204.12 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:40:15.254768: step 46060, loss = 1091435.88 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 22:40:25.635885: step 46070, loss = 1082737.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:40:36.033827: step 46080, loss = 1074108.88 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:40:46.445751: step 46090, loss = 1065548.38 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:40:57.178842: step 46100, loss = 1057056.25 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 22:41:07.630642: step 46110, loss = 1048631.75 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:41:18.096701: step 46120, loss = 1040274.56 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:41:28.499699: step 46130, loss = 1031983.94 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:41:38.915551: step 46140, loss = 1023759.69 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:41:49.313654: step 46150, loss = 1015600.69 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:41:59.684795: step 46160, loss = 1007506.56 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:42:10.122434: step 46170, loss = 999476.94 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:42:20.805996: step 46180, loss = 991511.56 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 22:42:31.171058: step 46190, loss = 983609.62 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 22:42:41.884251: step 46200, loss = 975770.69 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 22:42:52.235225: step 46210, loss = 967994.31 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 22:43:02.660671: step 46220, loss = 960279.50 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 22:43:13.076775: step 46230, loss = 952626.44 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:43:23.533987: step 46240, loss = 945034.44 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 22:43:33.909226: step 46250, loss = 937502.88 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:43:44.274636: step 46260, loss = 930031.38 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 22:43:54.614593: step 46270, loss = 922619.31 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:44:05.013029: step 46280, loss = 915266.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:44:15.420522: step 46290, loss = 907972.00 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:44:26.107477: step 46300, loss = 900735.44 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 22:44:36.422907: step 46310, loss = 893557.25 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 22:44:46.798469: step 46320, loss = 886435.88 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:44:57.163125: step 46330, loss = 879371.50 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:45:07.621157: step 46340, loss = 872362.94 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 22:45:18.046106: step 46350, loss = 865410.44 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:45:28.419642: step 46360, loss = 858513.69 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:45:38.782244: step 46370, loss = 851671.56 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:45:49.180414: step 46380, loss = 844884.25 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:45:59.601980: step 46390, loss = 838150.75 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:46:10.371267: step 46400, loss = 831470.88 (118.9 examples/sec; 1.077 sec/batch)
2018-04-09 22:46:20.717787: step 46410, loss = 824844.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 22:46:31.061079: step 46420, loss = 818270.69 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:46:41.416081: step 46430, loss = 811749.44 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 22:46:51.762033: step 46440, loss = 805280.06 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 22:47:02.211392: step 46450, loss = 798862.44 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 22:47:12.688481: step 46460, loss = 792495.94 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 22:47:23.084413: step 46470, loss = 786179.81 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:47:33.553683: step 46480, loss = 779914.62 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:47:43.902604: step 46490, loss = 773698.94 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 22:47:54.589768: step 46500, loss = 767532.50 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 22:48:04.977793: step 46510, loss = 761415.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:48:15.379840: step 46520, loss = 755347.56 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:48:25.742461: step 46530, loss = 749327.88 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:48:36.166894: step 46540, loss = 743355.88 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:48:46.545182: step 46550, loss = 737431.62 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:48:56.898200: step 46560, loss = 731554.31 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:49:07.310384: step 46570, loss = 725724.50 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:49:17.692961: step 46580, loss = 719940.69 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:49:28.036848: step 46590, loss = 714202.88 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 22:49:38.723291: step 46600, loss = 708511.06 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 22:49:49.075592: step 46610, loss = 702864.50 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:49:59.483259: step 46620, loss = 697262.69 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 22:50:09.904455: step 46630, loss = 691705.69 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 22:50:20.430367: step 46640, loss = 686193.38 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 22:50:30.808768: step 46650, loss = 680724.56 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:50:41.211805: step 46660, loss = 675299.50 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:50:51.579671: step 46670, loss = 669917.69 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 22:51:02.013363: step 46680, loss = 664578.62 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:51:12.430192: step 46690, loss = 659281.88 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:51:23.185284: step 46700, loss = 654027.75 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 22:51:33.512456: step 46710, loss = 648815.44 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 22:51:43.904755: step 46720, loss = 643644.44 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:51:54.294819: step 46730, loss = 638515.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:52:04.724218: step 46740, loss = 633426.31 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:52:15.164119: step 46750, loss = 628377.94 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:52:25.855721: step 46760, loss = 623370.12 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 22:52:36.519707: step 46770, loss = 618402.19 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 22:52:47.189302: step 46780, loss = 613473.44 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 22:52:57.863988: step 46790, loss = 608584.38 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 22:53:08.709298: step 46800, loss = 603734.19 (118.0 examples/sec; 1.085 sec/batch)
2018-04-09 22:53:19.124531: step 46810, loss = 598922.62 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 22:53:29.520964: step 46820, loss = 594149.50 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:53:39.889897: step 46830, loss = 589414.31 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 22:53:50.254237: step 46840, loss = 584716.69 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:54:00.640931: step 46850, loss = 580056.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:54:11.102066: step 46860, loss = 575433.94 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 22:54:21.456876: step 46870, loss = 570847.94 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:54:31.833047: step 46880, loss = 566298.62 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 22:54:42.145964: step 46890, loss = 561785.25 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 22:54:52.788372: step 46900, loss = 557308.12 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 22:55:03.151549: step 46910, loss = 552866.62 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:55:13.580109: step 46920, loss = 548460.44 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:55:23.981034: step 46930, loss = 544089.38 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:55:34.321196: step 46940, loss = 539752.88 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:55:44.682337: step 46950, loss = 535451.62 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 22:55:55.197016: step 46960, loss = 531184.19 (121.7 examples/sec; 1.051 sec/batch)
2018-04-09 22:56:05.631949: step 46970, loss = 526950.56 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 22:56:16.013077: step 46980, loss = 522751.16 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:56:26.348306: step 46990, loss = 518584.94 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 22:56:37.058099: step 47000, loss = 514451.91 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 22:56:47.402685: step 47010, loss = 510351.81 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 22:56:57.729169: step 47020, loss = 506284.78 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 22:57:08.133375: step 47030, loss = 502249.75 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 22:57:18.530873: step 47040, loss = 498246.94 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 22:57:28.884982: step 47050, loss = 494276.16 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:57:39.274236: step 47060, loss = 490336.69 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:57:49.768636: step 47070, loss = 486429.06 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 22:58:00.240996: step 47080, loss = 482552.53 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 22:58:10.774722: step 47090, loss = 478706.62 (121.5 examples/sec; 1.053 sec/batch)
2018-04-09 22:58:21.502667: step 47100, loss = 474891.38 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 22:58:31.855737: step 47110, loss = 471106.81 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 22:58:42.244325: step 47120, loss = 467352.22 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 22:58:52.627361: step 47130, loss = 463627.66 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 22:59:03.067094: step 47140, loss = 459932.81 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 22:59:13.536429: step 47150, loss = 456267.41 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 22:59:23.947165: step 47160, loss = 452631.12 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:59:34.302444: step 47170, loss = 449023.84 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 22:59:44.716696: step 47180, loss = 445445.22 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 22:59:55.077357: step 47190, loss = 441895.19 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:00:05.832823: step 47200, loss = 438373.50 (119.0 examples/sec; 1.076 sec/batch)
2018-04-09 23:00:16.315994: step 47210, loss = 434879.59 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 23:00:26.649075: step 47220, loss = 431413.84 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:00:36.976177: step 47230, loss = 427975.94 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:00:47.324523: step 47240, loss = 424564.81 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:00:57.680801: step 47250, loss = 421181.19 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:01:08.138890: step 47260, loss = 417824.53 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:01:18.557853: step 47270, loss = 414494.59 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 23:01:28.933735: step 47280, loss = 411191.09 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 23:01:39.309713: step 47290, loss = 407914.12 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 23:01:50.012095: step 47300, loss = 404663.12 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 23:02:00.393070: step 47310, loss = 401438.41 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:02:10.871700: step 47320, loss = 398238.84 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 23:02:21.229839: step 47330, loss = 395065.06 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:02:31.587432: step 47340, loss = 391916.28 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:02:42.006838: step 47350, loss = 388793.06 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:02:52.393153: step 47360, loss = 385694.41 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:03:02.825562: step 47370, loss = 382620.50 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:03:13.403870: step 47380, loss = 379571.28 (121.0 examples/sec; 1.058 sec/batch)
2018-04-09 23:03:23.897450: step 47390, loss = 376546.16 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 23:03:34.692601: step 47400, loss = 373545.28 (118.6 examples/sec; 1.080 sec/batch)
2018-04-09 23:03:45.106012: step 47410, loss = 370568.06 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:03:55.480722: step 47420, loss = 367615.09 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:04:05.927792: step 47430, loss = 364685.00 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:04:16.408823: step 47440, loss = 361778.84 (122.1 examples/sec; 1.048 sec/batch)
2018-04-09 23:04:26.915007: step 47450, loss = 358895.75 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:04:37.408345: step 47460, loss = 356035.38 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 23:04:47.909120: step 47470, loss = 353197.94 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 23:04:58.406833: step 47480, loss = 350383.25 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 23:05:08.969082: step 47490, loss = 347590.66 (121.2 examples/sec; 1.056 sec/batch)
2018-04-09 23:05:19.843572: step 47500, loss = 344820.81 (117.7 examples/sec; 1.087 sec/batch)
2018-04-09 23:05:30.312546: step 47510, loss = 342072.56 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 23:05:40.829246: step 47520, loss = 339346.44 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 23:05:51.363943: step 47530, loss = 336641.94 (121.5 examples/sec; 1.053 sec/batch)
2018-04-09 23:06:01.920074: step 47540, loss = 333958.88 (121.3 examples/sec; 1.056 sec/batch)
2018-04-09 23:06:12.513130: step 47550, loss = 331297.44 (120.8 examples/sec; 1.059 sec/batch)
2018-04-09 23:06:23.020271: step 47560, loss = 328657.12 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:06:33.515696: step 47570, loss = 326037.72 (122.0 examples/sec; 1.050 sec/batch)
2018-04-09 23:06:44.033802: step 47580, loss = 323439.50 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 23:06:55.021099: step 47590, loss = 320861.91 (116.5 examples/sec; 1.099 sec/batch)
2018-04-09 23:07:06.556656: step 47600, loss = 318304.53 (111.0 examples/sec; 1.154 sec/batch)
2018-04-09 23:07:17.719212: step 47610, loss = 315767.78 (114.7 examples/sec; 1.116 sec/batch)
2018-04-09 23:07:28.240749: step 47620, loss = 313251.31 (121.7 examples/sec; 1.052 sec/batch)
2018-04-09 23:07:38.769800: step 47630, loss = 310754.81 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 23:07:49.408658: step 47640, loss = 308278.22 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 23:07:59.918927: step 47650, loss = 305821.16 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:08:10.521701: step 47660, loss = 303384.19 (120.7 examples/sec; 1.060 sec/batch)
2018-04-09 23:08:20.897435: step 47670, loss = 300966.28 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 23:08:31.230048: step 47680, loss = 298567.78 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:08:41.592658: step 47690, loss = 296188.03 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:08:52.279128: step 47700, loss = 293827.78 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 23:09:02.678713: step 47710, loss = 291485.91 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:09:13.120118: step 47720, loss = 289162.91 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:09:23.495513: step 47730, loss = 286858.53 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 23:09:33.895853: step 47740, loss = 284572.34 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:09:44.275644: step 47750, loss = 282304.38 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:09:54.861380: step 47760, loss = 280054.44 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 23:10:05.603066: step 47770, loss = 277822.53 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 23:10:16.474508: step 47780, loss = 275608.19 (117.7 examples/sec; 1.087 sec/batch)
2018-04-09 23:10:27.179289: step 47790, loss = 273411.69 (119.6 examples/sec; 1.070 sec/batch)
2018-04-09 23:10:38.203892: step 47800, loss = 271232.75 (116.1 examples/sec; 1.102 sec/batch)
2018-04-09 23:10:48.843077: step 47810, loss = 269071.22 (120.3 examples/sec; 1.064 sec/batch)
2018-04-09 23:10:59.645875: step 47820, loss = 266926.81 (118.5 examples/sec; 1.080 sec/batch)
2018-04-09 23:11:10.451043: step 47830, loss = 264799.78 (118.5 examples/sec; 1.081 sec/batch)
2018-04-09 23:11:21.225610: step 47840, loss = 262689.19 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 23:11:31.909104: step 47850, loss = 260595.61 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 23:11:42.636152: step 47860, loss = 258518.77 (119.3 examples/sec; 1.073 sec/batch)
2018-04-09 23:11:53.267088: step 47870, loss = 256458.33 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:12:03.672410: step 47880, loss = 254414.56 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:12:14.120756: step 47890, loss = 252386.91 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:12:24.817920: step 47900, loss = 250375.42 (119.7 examples/sec; 1.070 sec/batch)
2018-04-09 23:12:35.194847: step 47910, loss = 248380.06 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 23:12:45.601069: step 47920, loss = 246400.86 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:12:55.985192: step 47930, loss = 244436.72 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:13:06.425228: step 47940, loss = 242488.80 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:13:16.877870: step 47950, loss = 240556.34 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:13:27.236347: step 47960, loss = 238639.08 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:13:37.591972: step 47970, loss = 236737.12 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:13:47.950292: step 47980, loss = 234850.52 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:13:58.276414: step 47990, loss = 232979.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 23:14:09.000091: step 48000, loss = 231121.92 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 23:14:19.372737: step 48010, loss = 229279.97 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:14:29.709963: step 48020, loss = 227453.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:14:40.068019: step 48030, loss = 225639.92 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:14:50.429394: step 48040, loss = 223841.92 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:15:00.795555: step 48050, loss = 222057.89 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 23:15:11.243891: step 48060, loss = 220288.41 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:15:21.656265: step 48070, loss = 218532.56 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:15:31.987517: step 48080, loss = 216791.00 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:15:42.325167: step 48090, loss = 215063.34 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:15:53.014520: step 48100, loss = 213349.14 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 23:16:03.441268: step 48110, loss = 211648.92 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 23:16:13.843059: step 48120, loss = 209962.08 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:16:24.225111: step 48130, loss = 208288.69 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:16:34.623910: step 48140, loss = 206628.92 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:16:45.033835: step 48150, loss = 204982.02 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:16:55.406492: step 48160, loss = 203348.64 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:17:05.804431: step 48170, loss = 201727.83 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:17:16.205577: step 48180, loss = 200120.19 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:17:26.544783: step 48190, loss = 198525.38 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:17:37.216047: step 48200, loss = 196942.94 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 23:17:47.572924: step 48210, loss = 195373.45 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:17:57.995266: step 48220, loss = 193816.27 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:18:08.367150: step 48230, loss = 192271.80 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:18:18.726799: step 48240, loss = 190739.33 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:18:29.076422: step 48250, loss = 189219.25 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:18:39.436882: step 48260, loss = 187711.33 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:18:49.750234: step 48270, loss = 186215.16 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 23:19:00.067251: step 48280, loss = 184731.34 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 23:19:10.461043: step 48290, loss = 183259.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:19:21.178350: step 48300, loss = 181798.75 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 23:19:31.494928: step 48310, loss = 180349.77 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 23:19:41.852269: step 48320, loss = 178912.36 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:19:52.182727: step 48330, loss = 177486.56 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:20:02.594901: step 48340, loss = 176072.00 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:20:13.136511: step 48350, loss = 174668.92 (121.4 examples/sec; 1.054 sec/batch)
2018-04-09 23:20:23.434090: step 48360, loss = 173276.86 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 23:20:33.758891: step 48370, loss = 171895.75 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 23:20:44.113219: step 48380, loss = 170525.86 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 23:20:54.497768: step 48390, loss = 169166.67 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:21:05.329396: step 48400, loss = 167818.69 (118.2 examples/sec; 1.083 sec/batch)
2018-04-09 23:21:15.808031: step 48410, loss = 166481.03 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 23:21:26.280386: step 48420, loss = 165154.17 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 23:21:36.720936: step 48430, loss = 163838.06 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:21:47.077530: step 48440, loss = 162532.31 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:21:57.436147: step 48450, loss = 161236.88 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:22:07.881285: step 48460, loss = 159952.27 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:22:18.322053: step 48470, loss = 158677.33 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:22:28.797544: step 48480, loss = 157412.75 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 23:22:39.252688: step 48490, loss = 156158.28 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:22:50.045153: step 48500, loss = 154913.92 (118.6 examples/sec; 1.079 sec/batch)
2018-04-09 23:23:00.553368: step 48510, loss = 153679.06 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:23:11.026831: step 48520, loss = 152454.42 (122.2 examples/sec; 1.047 sec/batch)
2018-04-09 23:23:21.479843: step 48530, loss = 151239.28 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:23:31.888778: step 48540, loss = 150034.12 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:23:42.298572: step 48550, loss = 148838.30 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:23:52.711939: step 48560, loss = 147651.92 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:24:03.133133: step 48570, loss = 146475.61 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:24:13.585961: step 48580, loss = 145308.14 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:24:24.043821: step 48590, loss = 144149.97 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:24:34.996020: step 48600, loss = 143001.28 (116.9 examples/sec; 1.095 sec/batch)
2018-04-09 23:24:45.627082: step 48610, loss = 141861.56 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:24:56.262482: step 48620, loss = 140731.05 (120.4 examples/sec; 1.064 sec/batch)
2018-04-09 23:25:06.925199: step 48630, loss = 139609.33 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 23:25:17.594401: step 48640, loss = 138496.77 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 23:25:28.263351: step 48650, loss = 137393.03 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 23:25:38.914637: step 48660, loss = 136298.27 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 23:25:49.530482: step 48670, loss = 135211.80 (120.6 examples/sec; 1.062 sec/batch)
2018-04-09 23:26:00.182488: step 48680, loss = 134134.27 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 23:26:10.897913: step 48690, loss = 133065.38 (119.5 examples/sec; 1.072 sec/batch)
2018-04-09 23:26:21.848643: step 48700, loss = 132004.86 (116.9 examples/sec; 1.095 sec/batch)
2018-04-09 23:26:32.447074: step 48710, loss = 130952.85 (120.8 examples/sec; 1.060 sec/batch)
2018-04-09 23:26:43.081199: step 48720, loss = 129909.02 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:26:53.704895: step 48730, loss = 128873.84 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 23:27:04.414905: step 48740, loss = 127846.65 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 23:27:15.093445: step 48750, loss = 126827.59 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 23:27:25.716899: step 48760, loss = 125816.98 (120.5 examples/sec; 1.062 sec/batch)
2018-04-09 23:27:36.344854: step 48770, loss = 124814.26 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:27:47.000299: step 48780, loss = 123819.63 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 23:27:57.626649: step 48790, loss = 122832.80 (120.5 examples/sec; 1.063 sec/batch)
2018-04-09 23:28:08.716862: step 48800, loss = 121853.82 (115.4 examples/sec; 1.109 sec/batch)
2018-04-09 23:28:19.436850: step 48810, loss = 120882.49 (119.4 examples/sec; 1.072 sec/batch)
2018-04-09 23:28:30.119566: step 48820, loss = 119919.50 (119.8 examples/sec; 1.068 sec/batch)
2018-04-09 23:28:40.782624: step 48830, loss = 118963.74 (120.0 examples/sec; 1.066 sec/batch)
2018-04-09 23:28:51.416279: step 48840, loss = 118015.53 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:29:02.123248: step 48850, loss = 117074.95 (119.5 examples/sec; 1.071 sec/batch)
2018-04-09 23:29:12.875007: step 48860, loss = 116142.02 (119.1 examples/sec; 1.075 sec/batch)
2018-04-09 23:29:23.410035: step 48870, loss = 115216.34 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 23:29:33.803362: step 48880, loss = 114298.01 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:29:44.168537: step 48890, loss = 113387.54 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 23:29:54.854791: step 48900, loss = 112483.57 (119.8 examples/sec; 1.069 sec/batch)
2018-04-09 23:30:05.280820: step 48910, loss = 111587.20 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 23:30:15.829545: step 48920, loss = 110697.98 (121.3 examples/sec; 1.055 sec/batch)
2018-04-09 23:30:26.204054: step 48930, loss = 109815.69 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:30:36.645637: step 48940, loss = 108940.34 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:30:47.096690: step 48950, loss = 108072.34 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:30:57.508308: step 48960, loss = 107210.99 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:31:07.928483: step 48970, loss = 106356.32 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:31:18.321493: step 48980, loss = 105508.88 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:31:28.745041: step 48990, loss = 104668.07 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:31:39.410140: step 49000, loss = 103833.91 (120.0 examples/sec; 1.067 sec/batch)
2018-04-09 23:31:49.746304: step 49010, loss = 103006.39 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:32:00.178367: step 49020, loss = 102185.38 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:32:10.637608: step 49030, loss = 101370.99 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:32:21.044590: step 49040, loss = 100563.22 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:32:31.398576: step 49050, loss = 99761.84 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 23:32:41.769744: step 49060, loss = 98966.74 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:32:52.113709: step 49070, loss = 98178.12 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 23:33:02.496823: step 49080, loss = 97395.39 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:33:12.917515: step 49090, loss = 96619.36 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:33:23.579020: step 49100, loss = 95849.38 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 23:33:33.929776: step 49110, loss = 95085.53 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:33:44.281340: step 49120, loss = 94327.69 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:33:54.602157: step 49130, loss = 93575.91 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 23:34:04.999873: step 49140, loss = 92830.15 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:34:15.430144: step 49150, loss = 92090.40 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:34:25.781236: step 49160, loss = 91356.54 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:34:36.118460: step 49170, loss = 90628.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:34:46.446978: step 49180, loss = 89906.15 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:34:56.778805: step 49190, loss = 89189.60 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:35:07.589353: step 49200, loss = 88479.00 (118.4 examples/sec; 1.081 sec/batch)
2018-04-09 23:35:18.020223: step 49210, loss = 87773.80 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:35:28.393145: step 49220, loss = 87074.30 (123.4 examples/sec; 1.037 sec/batch)
2018-04-09 23:35:38.739104: step 49230, loss = 86380.37 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:35:49.083675: step 49240, loss = 85691.84 (123.7 examples/sec; 1.034 sec/batch)
2018-04-09 23:35:59.583287: step 49250, loss = 85009.00 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 23:36:10.033619: step 49260, loss = 84331.36 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:36:20.446309: step 49270, loss = 83659.41 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:36:30.814000: step 49280, loss = 82992.67 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 23:36:41.148846: step 49290, loss = 82331.45 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:36:51.806414: step 49300, loss = 81675.20 (120.1 examples/sec; 1.066 sec/batch)
2018-04-09 23:37:02.135099: step 49310, loss = 81024.20 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:37:12.558669: step 49320, loss = 80378.47 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:37:22.902039: step 49330, loss = 79737.99 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:37:33.257683: step 49340, loss = 79102.24 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:37:43.624833: step 49350, loss = 78472.09 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 23:37:53.988813: step 49360, loss = 77846.41 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:38:04.579484: step 49370, loss = 77226.18 (120.9 examples/sec; 1.059 sec/batch)
2018-04-09 23:38:15.213967: step 49380, loss = 76610.71 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:38:25.715447: step 49390, loss = 76000.24 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 23:38:36.393148: step 49400, loss = 75394.36 (119.9 examples/sec; 1.068 sec/batch)
2018-04-09 23:38:46.690314: step 49410, loss = 74793.50 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 23:38:57.007817: step 49420, loss = 74197.70 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 23:39:07.427629: step 49430, loss = 73606.33 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:39:17.808667: step 49440, loss = 73019.60 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:39:28.141622: step 49450, loss = 72437.83 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:39:38.507146: step 49460, loss = 71860.63 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 23:39:48.843381: step 49470, loss = 71287.68 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:39:59.181928: step 49480, loss = 70719.80 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:40:09.576483: step 49490, loss = 70155.95 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 23:40:20.355302: step 49500, loss = 69596.94 (118.8 examples/sec; 1.078 sec/batch)
2018-04-09 23:40:30.674038: step 49510, loss = 69042.39 (124.0 examples/sec; 1.032 sec/batch)
2018-04-09 23:40:41.020979: step 49520, loss = 68492.10 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:40:51.352691: step 49530, loss = 67946.10 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:41:01.775925: step 49540, loss = 67404.76 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:41:12.167873: step 49550, loss = 66867.71 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:41:22.496721: step 49560, loss = 66334.53 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:41:32.814551: step 49570, loss = 65805.71 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 23:41:43.167299: step 49580, loss = 65281.58 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 23:41:53.474162: step 49590, loss = 64761.45 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 23:42:04.120055: step 49600, loss = 64244.96 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 23:42:14.471017: step 49610, loss = 63733.17 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:42:24.797267: step 49620, loss = 63225.19 (124.0 examples/sec; 1.033 sec/batch)
2018-04-09 23:42:35.136476: step 49630, loss = 62721.31 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:42:45.435830: step 49640, loss = 62221.42 (124.3 examples/sec; 1.030 sec/batch)
2018-04-09 23:42:55.763787: step 49650, loss = 61725.70 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:43:06.167958: step 49660, loss = 61233.63 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 23:43:16.552603: step 49670, loss = 60745.73 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:43:26.964602: step 49680, loss = 60261.50 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:43:37.396427: step 49690, loss = 59781.21 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:43:48.208398: step 49700, loss = 59304.85 (118.4 examples/sec; 1.081 sec/batch)
2018-04-09 23:43:58.659980: step 49710, loss = 58832.42 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:44:09.151496: step 49720, loss = 58363.30 (122.0 examples/sec; 1.049 sec/batch)
2018-04-09 23:44:19.610246: step 49730, loss = 57898.23 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:44:30.045848: step 49740, loss = 57436.93 (122.7 examples/sec; 1.044 sec/batch)
2018-04-09 23:44:40.509895: step 49750, loss = 56978.96 (122.3 examples/sec; 1.046 sec/batch)
2018-04-09 23:44:50.899774: step 49760, loss = 56524.96 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:45:01.248124: step 49770, loss = 56074.60 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:45:11.655150: step 49780, loss = 55627.61 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:45:22.009175: step 49790, loss = 55184.44 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 23:45:32.639833: step 49800, loss = 54744.85 (120.4 examples/sec; 1.063 sec/batch)
2018-04-09 23:45:42.947574: step 49810, loss = 54308.25 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 23:45:53.284562: step 49820, loss = 53875.58 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:46:03.693286: step 49830, loss = 53446.15 (123.0 examples/sec; 1.041 sec/batch)
2018-04-09 23:46:14.143914: step 49840, loss = 53020.25 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:46:24.562539: step 49850, loss = 52597.78 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 23:46:34.963094: step 49860, loss = 52178.54 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:46:45.404234: step 49870, loss = 51762.61 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:46:55.830488: step 49880, loss = 51350.11 (122.8 examples/sec; 1.043 sec/batch)
2018-04-09 23:47:06.265025: step 49890, loss = 50940.92 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:47:17.086794: step 49900, loss = 50534.81 (118.3 examples/sec; 1.082 sec/batch)
2018-04-09 23:47:27.532036: step 49910, loss = 50132.06 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:47:37.975702: step 49920, loss = 49732.60 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:47:48.406726: step 49930, loss = 49336.25 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:47:58.847715: step 49940, loss = 48943.11 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:48:09.358419: step 49950, loss = 48553.12 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:48:19.863840: step 49960, loss = 48166.10 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:48:30.293073: step 49970, loss = 47782.31 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:48:40.739026: step 49980, loss = 47401.59 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:48:51.152310: step 49990, loss = 47023.84 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:49:01.970329: step 50000, loss = 46648.91 (118.3 examples/sec; 1.082 sec/batch)
2018-04-09 23:49:12.437617: step 50010, loss = 46277.30 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 23:49:22.780252: step 50020, loss = 45908.48 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:49:33.098649: step 50030, loss = 45542.46 (124.1 examples/sec; 1.032 sec/batch)
2018-04-09 23:49:43.511873: step 50040, loss = 45179.70 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:49:53.977710: step 50050, loss = 44819.73 (122.3 examples/sec; 1.047 sec/batch)
2018-04-09 23:50:04.507321: step 50060, loss = 44462.38 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 23:50:15.198368: step 50070, loss = 44108.21 (119.7 examples/sec; 1.069 sec/batch)
2018-04-09 23:50:25.595851: step 50080, loss = 43756.77 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:50:36.018779: step 50090, loss = 43407.91 (122.8 examples/sec; 1.042 sec/batch)
2018-04-09 23:50:46.756149: step 50100, loss = 43061.86 (119.2 examples/sec; 1.074 sec/batch)
2018-04-09 23:50:57.172365: step 50110, loss = 42718.86 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 23:51:07.677661: step 50120, loss = 42378.22 (121.8 examples/sec; 1.051 sec/batch)
2018-04-09 23:51:18.153757: step 50130, loss = 42040.52 (122.2 examples/sec; 1.048 sec/batch)
2018-04-09 23:51:28.639629: step 50140, loss = 41705.66 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 23:51:39.175665: step 50150, loss = 41373.39 (121.5 examples/sec; 1.054 sec/batch)
2018-04-09 23:51:49.676420: step 50160, loss = 41043.29 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 23:52:00.132098: step 50170, loss = 40716.57 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:52:10.631810: step 50180, loss = 40391.95 (121.9 examples/sec; 1.050 sec/batch)
2018-04-09 23:52:21.062923: step 50190, loss = 40069.84 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:52:31.833913: step 50200, loss = 39750.76 (118.8 examples/sec; 1.077 sec/batch)
2018-04-09 23:52:42.280986: step 50210, loss = 39434.05 (122.5 examples/sec; 1.045 sec/batch)
2018-04-09 23:52:52.766410: step 50220, loss = 39119.62 (122.1 examples/sec; 1.049 sec/batch)
2018-04-09 23:53:03.292516: step 50230, loss = 38807.85 (121.6 examples/sec; 1.053 sec/batch)
2018-04-09 23:53:13.734508: step 50240, loss = 38498.60 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:53:24.151456: step 50250, loss = 38191.68 (122.9 examples/sec; 1.042 sec/batch)
2018-04-09 23:53:34.595375: step 50260, loss = 37887.27 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:53:44.974503: step 50270, loss = 37585.60 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:53:55.335563: step 50280, loss = 37285.82 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:54:05.739866: step 50290, loss = 36989.04 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 23:54:16.503614: step 50300, loss = 36694.04 (118.9 examples/sec; 1.076 sec/batch)
2018-04-09 23:54:26.959218: step 50310, loss = 36401.55 (122.4 examples/sec; 1.046 sec/batch)
2018-04-09 23:54:37.388496: step 50320, loss = 36111.38 (122.7 examples/sec; 1.043 sec/batch)
2018-04-09 23:54:47.766012: step 50330, loss = 35823.84 (123.3 examples/sec; 1.038 sec/batch)
2018-04-09 23:54:58.141549: step 50340, loss = 35538.09 (123.4 examples/sec; 1.038 sec/batch)
2018-04-09 23:55:08.554413: step 50350, loss = 35254.98 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:55:18.965222: step 50360, loss = 34974.20 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:55:29.317573: step 50370, loss = 34695.29 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 23:55:39.684465: step 50380, loss = 34418.71 (123.5 examples/sec; 1.037 sec/batch)
2018-04-09 23:55:50.033335: step 50390, loss = 34144.69 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:56:00.738650: step 50400, loss = 33872.39 (119.6 examples/sec; 1.071 sec/batch)
2018-04-09 23:56:11.130572: step 50410, loss = 33602.29 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:56:21.516707: step 50420, loss = 33334.85 (123.2 examples/sec; 1.039 sec/batch)
2018-04-09 23:56:31.880055: step 50430, loss = 33068.95 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:56:42.219336: step 50440, loss = 32805.33 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:56:52.559723: step 50450, loss = 32543.99 (123.8 examples/sec; 1.034 sec/batch)
2018-04-09 23:57:02.953802: step 50460, loss = 32284.91 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 23:57:13.366974: step 50470, loss = 32027.32 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:57:23.714131: step 50480, loss = 31772.36 (123.7 examples/sec; 1.035 sec/batch)
2018-04-09 23:57:34.066585: step 50490, loss = 31518.83 (123.6 examples/sec; 1.035 sec/batch)
2018-04-09 23:57:44.740351: step 50500, loss = 31267.58 (119.9 examples/sec; 1.067 sec/batch)
2018-04-09 23:57:55.049126: step 50510, loss = 31018.53 (124.2 examples/sec; 1.031 sec/batch)
2018-04-09 23:58:05.451746: step 50520, loss = 30771.20 (123.0 examples/sec; 1.040 sec/batch)
2018-04-09 23:58:15.846181: step 50530, loss = 30525.96 (123.1 examples/sec; 1.039 sec/batch)
2018-04-09 23:58:26.173346: step 50540, loss = 30283.04 (123.9 examples/sec; 1.033 sec/batch)
2018-04-09 23:58:36.571468: step 50550, loss = 30041.84 (123.1 examples/sec; 1.040 sec/batch)
2018-04-09 23:58:46.984688: step 50560, loss = 29802.23 (122.9 examples/sec; 1.041 sec/batch)
2018-04-09 23:58:57.349484: step 50570, loss = 29564.63 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:59:07.788354: step 50580, loss = 29328.93 (122.6 examples/sec; 1.044 sec/batch)
2018-04-09 23:59:18.152165: step 50590, loss = 29095.28 (123.5 examples/sec; 1.036 sec/batch)
2018-04-09 23:59:28.798411: step 50600, loss = 28863.53 (120.2 examples/sec; 1.065 sec/batch)
2018-04-09 23:59:39.113042: step 50610, loss = 28633.65 (124.1 examples/sec; 1.031 sec/batch)
2018-04-09 23:59:49.468776: step 50620, loss = 28405.18 (123.6 examples/sec; 1.036 sec/batch)
2018-04-09 23:59:59.782467: step 50630, loss = 28178.88 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:00:10.261886: step 50640, loss = 27954.45 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 00:00:20.738809: step 50650, loss = 27731.62 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 00:00:31.082814: step 50660, loss = 27510.50 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:00:41.448531: step 50670, loss = 27291.36 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 00:00:51.824518: step 50680, loss = 27073.79 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 00:01:02.234432: step 50690, loss = 26858.04 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 00:01:12.959136: step 50700, loss = 26644.04 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 00:01:23.285135: step 50710, loss = 26431.59 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 00:01:33.646600: step 50720, loss = 26221.15 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:01:44.096043: step 50730, loss = 26012.12 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 00:01:54.694875: step 50740, loss = 25804.78 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 00:02:05.404667: step 50750, loss = 25599.08 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 00:02:16.061597: step 50760, loss = 25395.43 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 00:02:26.690449: step 50770, loss = 25192.62 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 00:02:37.258689: step 50780, loss = 24991.96 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 00:02:47.880940: step 50790, loss = 24792.88 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 00:02:58.805681: step 50800, loss = 24595.38 (117.2 examples/sec; 1.092 sec/batch)
2018-04-10 00:03:09.449202: step 50810, loss = 24399.27 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 00:03:20.148883: step 50820, loss = 24204.63 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 00:03:30.719927: step 50830, loss = 24011.92 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 00:03:41.325413: step 50840, loss = 23820.41 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 00:03:51.873725: step 50850, loss = 23630.56 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 00:04:02.466418: step 50860, loss = 23442.66 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 00:04:13.119357: step 50870, loss = 23255.51 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 00:04:23.699952: step 50880, loss = 23070.18 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 00:04:34.259499: step 50890, loss = 22886.50 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 00:04:45.159296: step 50900, loss = 22704.16 (117.4 examples/sec; 1.090 sec/batch)
2018-04-10 00:04:55.703283: step 50910, loss = 22523.01 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 00:05:06.350517: step 50920, loss = 22343.67 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 00:05:17.014970: step 50930, loss = 22165.62 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 00:05:27.681175: step 50940, loss = 21988.79 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 00:05:38.274909: step 50950, loss = 21813.83 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 00:05:48.849079: step 50960, loss = 21640.00 (121.0 examples/sec; 1.057 sec/batch)
2018-04-10 00:05:59.421287: step 50970, loss = 21467.35 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 00:06:10.105095: step 50980, loss = 21296.39 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 00:06:20.723612: step 50990, loss = 21126.91 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 00:06:31.600858: step 51000, loss = 20958.41 (117.7 examples/sec; 1.088 sec/batch)
2018-04-10 00:06:42.129504: step 51010, loss = 20791.09 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 00:06:52.724580: step 51020, loss = 20625.62 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 00:07:03.326891: step 51030, loss = 20461.21 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 00:07:13.971261: step 51040, loss = 20298.29 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 00:07:24.523389: step 51050, loss = 20136.39 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 00:07:35.106243: step 51060, loss = 19976.03 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 00:07:45.660804: step 51070, loss = 19816.55 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 00:07:56.240498: step 51080, loss = 19658.86 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 00:08:06.861600: step 51090, loss = 19502.16 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 00:08:17.771820: step 51100, loss = 19346.81 (117.3 examples/sec; 1.091 sec/batch)
2018-04-10 00:08:28.301321: step 51110, loss = 19192.80 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 00:08:38.873398: step 51120, loss = 19039.51 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 00:08:49.462017: step 51130, loss = 18887.99 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 00:09:00.018876: step 51140, loss = 18737.54 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 00:09:10.700995: step 51150, loss = 18587.96 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 00:09:21.288581: step 51160, loss = 18439.85 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 00:09:31.851665: step 51170, loss = 18292.98 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 00:09:42.403297: step 51180, loss = 18147.34 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 00:09:52.775896: step 51190, loss = 18002.38 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 00:10:03.482722: step 51200, loss = 17859.28 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 00:10:14.020082: step 51210, loss = 17716.75 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 00:10:24.506245: step 51220, loss = 17575.48 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 00:10:34.950501: step 51230, loss = 17435.46 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 00:10:45.514058: step 51240, loss = 17296.56 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 00:10:56.101311: step 51250, loss = 17158.79 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 00:11:06.723288: step 51260, loss = 17022.22 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 00:11:17.213360: step 51270, loss = 16886.40 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 00:11:27.579479: step 51280, loss = 16751.96 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 00:11:37.909685: step 51290, loss = 16618.28 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:11:48.564642: step 51300, loss = 16485.99 (120.1 examples/sec; 1.065 sec/batch)
2018-04-10 00:11:58.898170: step 51310, loss = 16354.54 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:12:09.297594: step 51320, loss = 16224.13 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 00:12:19.638415: step 51330, loss = 16094.89 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:12:29.938760: step 51340, loss = 15966.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:12:40.261001: step 51350, loss = 15839.33 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:12:50.639580: step 51360, loss = 15713.39 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:13:01.092400: step 51370, loss = 15588.03 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 00:13:11.735835: step 51380, loss = 15463.64 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 00:13:22.289818: step 51390, loss = 15340.65 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 00:13:33.164186: step 51400, loss = 15218.19 (117.7 examples/sec; 1.087 sec/batch)
2018-04-10 00:13:43.712173: step 51410, loss = 15096.69 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 00:13:54.130229: step 51420, loss = 14976.81 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 00:14:04.529404: step 51430, loss = 14857.32 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 00:14:14.947560: step 51440, loss = 14739.00 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 00:14:25.298956: step 51450, loss = 14621.66 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:14:35.636601: step 51460, loss = 14505.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:14:45.964255: step 51470, loss = 14389.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:14:56.380413: step 51480, loss = 14274.73 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 00:15:06.768769: step 51490, loss = 14161.05 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:15:17.472450: step 51500, loss = 14048.03 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 00:15:27.826210: step 51510, loss = 13936.38 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 00:15:38.175146: step 51520, loss = 13825.25 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:15:48.517235: step 51530, loss = 13714.92 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:15:58.857653: step 51540, loss = 13605.70 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:16:09.276317: step 51550, loss = 13497.15 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 00:16:19.717435: step 51560, loss = 13389.69 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 00:16:30.121138: step 51570, loss = 13282.94 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 00:16:40.512427: step 51580, loss = 13177.16 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:16:50.856148: step 51590, loss = 13072.17 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:17:01.505810: step 51600, loss = 12967.86 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 00:17:11.871756: step 51610, loss = 12864.79 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 00:17:22.243409: step 51620, loss = 12762.15 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 00:17:32.605780: step 51630, loss = 12660.25 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:17:42.949328: step 51640, loss = 12559.60 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:17:53.356472: step 51650, loss = 12459.37 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 00:18:03.763701: step 51660, loss = 12360.15 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 00:18:14.140014: step 51670, loss = 12261.69 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 00:18:24.491552: step 51680, loss = 12163.89 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:18:34.802578: step 51690, loss = 12066.67 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:18:45.411325: step 51700, loss = 11970.95 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 00:18:55.732457: step 51710, loss = 11875.49 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:19:06.085362: step 51720, loss = 11780.71 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 00:19:16.434329: step 51730, loss = 11686.90 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:19:26.755064: step 51740, loss = 11593.74 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:19:37.061321: step 51750, loss = 11501.31 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:19:47.374900: step 51760, loss = 11409.98 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:19:57.682525: step 51770, loss = 11319.06 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:20:08.225906: step 51780, loss = 11228.41 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 00:20:18.667085: step 51790, loss = 11139.23 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 00:20:29.316185: step 51800, loss = 11050.53 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 00:20:39.600449: step 51810, loss = 10962.26 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 00:20:49.937147: step 51820, loss = 10874.98 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:21:00.254808: step 51830, loss = 10788.46 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 00:21:10.647676: step 51840, loss = 10702.31 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:21:20.992232: step 51850, loss = 10617.15 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:21:31.297887: step 51860, loss = 10532.68 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:21:41.608794: step 51870, loss = 10448.55 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:21:51.910217: step 51880, loss = 10365.39 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:22:02.220047: step 51890, loss = 10282.64 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:22:12.902345: step 51900, loss = 10200.86 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 00:22:23.221191: step 51910, loss = 10119.52 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:22:33.524178: step 51920, loss = 10038.92 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 00:22:43.848457: step 51930, loss = 9958.81 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:22:54.159074: step 51940, loss = 9879.27 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:23:04.537680: step 51950, loss = 9800.90 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:23:14.965999: step 51960, loss = 9722.73 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 00:23:25.357101: step 51970, loss = 9645.06 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:23:35.693013: step 51980, loss = 9568.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:23:46.020040: step 51990, loss = 9492.09 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:23:56.695289: step 52000, loss = 9416.12 (119.9 examples/sec; 1.068 sec/batch)
2018-04-10 00:24:07.059175: step 52010, loss = 9341.59 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:24:17.471002: step 52020, loss = 9267.16 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 00:24:27.824911: step 52030, loss = 9193.23 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 00:24:38.117432: step 52040, loss = 9119.83 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:24:48.446554: step 52050, loss = 9047.22 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:24:58.837936: step 52060, loss = 8975.26 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:25:09.230150: step 52070, loss = 8903.64 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:25:19.576189: step 52080, loss = 8832.83 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:25:29.888109: step 52090, loss = 8762.37 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:25:40.523271: step 52100, loss = 8692.41 (120.4 examples/sec; 1.064 sec/batch)
2018-04-10 00:25:50.835249: step 52110, loss = 8623.20 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:26:01.182100: step 52120, loss = 8554.54 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:26:11.527692: step 52130, loss = 8486.22 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:26:21.870787: step 52140, loss = 8418.63 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:26:32.197733: step 52150, loss = 8351.78 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:26:42.507587: step 52160, loss = 8285.07 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:26:52.802703: step 52170, loss = 8219.34 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:27:03.102591: step 52180, loss = 8153.68 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:27:13.444621: step 52190, loss = 8088.66 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:27:24.118255: step 52200, loss = 8024.19 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 00:27:34.396813: step 52210, loss = 7959.95 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 00:27:44.740089: step 52220, loss = 7896.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:27:55.014266: step 52230, loss = 7833.90 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 00:28:05.345816: step 52240, loss = 7771.62 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:28:15.706187: step 52250, loss = 7709.43 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:28:26.093784: step 52260, loss = 7648.22 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:28:36.406461: step 52270, loss = 7587.20 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:28:46.745859: step 52280, loss = 7526.66 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:28:57.075778: step 52290, loss = 7466.62 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:29:07.797074: step 52300, loss = 7407.27 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 00:29:18.135160: step 52310, loss = 7348.10 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:29:28.458540: step 52320, loss = 7289.61 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:29:38.796553: step 52330, loss = 7231.53 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:29:49.134592: step 52340, loss = 7174.24 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:29:59.450538: step 52350, loss = 7116.79 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 00:30:09.837896: step 52360, loss = 7060.00 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:30:20.346513: step 52370, loss = 7003.94 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 00:30:30.814027: step 52380, loss = 6947.99 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 00:30:41.192056: step 52390, loss = 6892.72 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:30:51.915300: step 52400, loss = 6837.60 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 00:31:02.351559: step 52410, loss = 6783.31 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 00:31:12.847107: step 52420, loss = 6729.16 (122.0 examples/sec; 1.050 sec/batch)
2018-04-10 00:31:23.217397: step 52430, loss = 6675.78 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 00:31:33.632825: step 52440, loss = 6622.39 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 00:31:44.010277: step 52450, loss = 6569.65 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:31:54.384371: step 52460, loss = 6517.20 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 00:32:04.845634: step 52470, loss = 6465.26 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 00:32:15.309447: step 52480, loss = 6413.95 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 00:32:25.701254: step 52490, loss = 6362.82 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:32:36.394925: step 52500, loss = 6311.90 (119.7 examples/sec; 1.069 sec/batch)
2018-04-10 00:32:46.769367: step 52510, loss = 6261.72 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 00:32:57.169638: step 52520, loss = 6211.83 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 00:33:07.604191: step 52530, loss = 6162.37 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 00:33:18.090555: step 52540, loss = 6113.26 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 00:33:28.484647: step 52550, loss = 6064.63 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 00:33:38.864591: step 52560, loss = 6016.06 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:33:49.210621: step 52570, loss = 5968.35 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:33:59.554179: step 52580, loss = 5920.78 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:34:10.013021: step 52590, loss = 5873.51 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 00:34:20.685005: step 52600, loss = 5826.86 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 00:34:31.017724: step 52610, loss = 5780.45 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:34:41.295718: step 52620, loss = 5734.35 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 00:34:51.567785: step 52630, loss = 5688.61 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 00:35:01.894514: step 52640, loss = 5643.26 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 00:35:12.221045: step 52650, loss = 5598.35 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 00:35:22.569481: step 52660, loss = 5553.81 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:35:32.913980: step 52670, loss = 5509.65 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:35:43.234159: step 52680, loss = 5465.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:35:53.522295: step 52690, loss = 5422.02 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:36:04.236266: step 52700, loss = 5378.81 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 00:36:14.703502: step 52710, loss = 5336.05 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 00:36:25.007301: step 52720, loss = 5293.37 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 00:36:35.326460: step 52730, loss = 5251.33 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:36:45.637861: step 52740, loss = 5209.48 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:36:55.954232: step 52750, loss = 5168.00 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 00:37:06.310895: step 52760, loss = 5126.82 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 00:37:16.658878: step 52770, loss = 5085.94 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 00:37:26.980825: step 52780, loss = 5045.42 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:37:37.294969: step 52790, loss = 5005.28 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:37:47.913878: step 52800, loss = 4965.35 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 00:37:58.206767: step 52810, loss = 4925.59 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:38:08.568387: step 52820, loss = 4886.50 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:38:18.926903: step 52830, loss = 4847.86 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 00:38:29.250375: step 52840, loss = 4808.84 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:38:39.569433: step 52850, loss = 4770.75 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:38:49.906924: step 52860, loss = 4732.78 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:39:00.194689: step 52870, loss = 4694.78 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:39:10.624867: step 52880, loss = 4657.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 00:39:20.953054: step 52890, loss = 4620.57 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:39:31.549957: step 52900, loss = 4583.59 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 00:39:41.847033: step 52910, loss = 4547.19 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:39:52.135845: step 52920, loss = 4511.04 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:40:02.448964: step 52930, loss = 4474.94 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:40:12.815257: step 52940, loss = 4439.19 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 00:40:23.230072: step 52950, loss = 4403.94 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 00:40:33.596278: step 52960, loss = 4368.65 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 00:40:43.902407: step 52970, loss = 4334.08 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:40:54.207195: step 52980, loss = 4299.58 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 00:41:04.566831: step 52990, loss = 4265.30 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 00:41:15.229876: step 53000, loss = 4231.33 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 00:41:25.510264: step 53010, loss = 4197.69 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 00:41:35.821103: step 53020, loss = 4164.05 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:41:46.118020: step 53030, loss = 4130.84 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:41:56.407017: step 53040, loss = 4098.12 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:42:06.743007: step 53050, loss = 4065.42 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:42:17.087566: step 53060, loss = 4032.81 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 00:42:27.401784: step 53070, loss = 4000.81 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:42:37.730257: step 53080, loss = 3969.13 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:42:48.051681: step 53090, loss = 3937.19 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:42:58.666901: step 53100, loss = 3905.80 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 00:43:09.007854: step 53110, loss = 3875.04 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:43:19.398957: step 53120, loss = 3844.12 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:43:29.694976: step 53130, loss = 3813.34 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:43:39.996673: step 53140, loss = 3783.02 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:43:50.305595: step 53150, loss = 3753.01 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:44:00.632075: step 53160, loss = 3722.92 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 00:44:11.034766: step 53170, loss = 3693.38 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 00:44:21.424461: step 53180, loss = 3663.69 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:44:31.688367: step 53190, loss = 3634.66 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 00:44:42.278828: step 53200, loss = 3605.80 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 00:44:52.508401: step 53210, loss = 3577.08 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 00:45:02.837846: step 53220, loss = 3548.66 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:45:13.173177: step 53230, loss = 3520.18 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:45:23.473797: step 53240, loss = 3492.15 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:45:33.785897: step 53250, loss = 3464.52 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:45:44.140428: step 53260, loss = 3436.82 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 00:45:54.499241: step 53270, loss = 3409.49 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 00:46:04.890018: step 53280, loss = 3382.28 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 00:46:15.252412: step 53290, loss = 3355.34 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:46:25.884775: step 53300, loss = 3328.46 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 00:46:36.187678: step 53310, loss = 3301.99 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 00:46:46.548167: step 53320, loss = 3275.63 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 00:46:56.876445: step 53330, loss = 3249.51 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:47:07.250799: step 53340, loss = 3223.76 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 00:47:17.632062: step 53350, loss = 3198.27 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:47:28.011242: step 53360, loss = 3172.66 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:47:38.336446: step 53370, loss = 3147.41 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 00:47:48.637656: step 53380, loss = 3122.32 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:47:58.949146: step 53390, loss = 3097.41 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:48:09.597857: step 53400, loss = 3072.66 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 00:48:19.919963: step 53410, loss = 3048.16 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:48:30.245467: step 53420, loss = 3023.95 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 00:48:40.555454: step 53430, loss = 2999.73 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:48:50.854161: step 53440, loss = 2975.95 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:49:01.207889: step 53450, loss = 2952.34 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 00:49:11.604525: step 53460, loss = 2928.75 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 00:49:21.926029: step 53470, loss = 2905.45 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:49:32.220644: step 53480, loss = 2882.35 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 00:49:42.501801: step 53490, loss = 2859.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 00:49:53.092792: step 53500, loss = 2836.51 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 00:50:03.410768: step 53510, loss = 2813.97 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 00:50:13.744587: step 53520, loss = 2791.62 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:50:24.357421: step 53530, loss = 2769.34 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 00:50:34.680104: step 53540, loss = 2747.26 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:50:44.970532: step 53550, loss = 2725.43 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:50:55.269759: step 53560, loss = 2703.60 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:51:05.628017: step 53570, loss = 2682.15 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 00:51:15.962578: step 53580, loss = 2660.65 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 00:51:26.254773: step 53590, loss = 2639.56 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:51:36.838888: step 53600, loss = 2618.53 (120.9 examples/sec; 1.058 sec/batch)
2018-04-10 00:51:47.134919: step 53610, loss = 2597.68 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:51:57.488319: step 53620, loss = 2576.87 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 00:52:07.868364: step 53630, loss = 2556.51 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:52:18.247706: step 53640, loss = 2535.85 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 00:52:28.512780: step 53650, loss = 2515.92 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 00:52:38.775344: step 53660, loss = 2495.80 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 00:52:49.034747: step 53670, loss = 2476.02 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 00:52:59.348463: step 53680, loss = 2456.40 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:53:09.667900: step 53690, loss = 2436.74 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:53:20.257084: step 53700, loss = 2417.19 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 00:53:30.481520: step 53710, loss = 2398.07 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 00:53:40.744215: step 53720, loss = 2378.83 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 00:53:50.988515: step 53730, loss = 2360.11 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 00:54:01.258591: step 53740, loss = 2341.10 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 00:54:11.556940: step 53750, loss = 2322.43 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:54:21.855814: step 53760, loss = 2303.95 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:54:32.132005: step 53770, loss = 2285.72 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 00:54:42.405263: step 53780, loss = 2267.36 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 00:54:52.676465: step 53790, loss = 2249.41 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 00:55:03.274561: step 53800, loss = 2231.63 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 00:55:13.576015: step 53810, loss = 2213.71 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:55:23.874006: step 53820, loss = 2196.25 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 00:55:34.155161: step 53830, loss = 2178.93 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 00:55:44.620865: step 53840, loss = 2161.17 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 00:55:55.085790: step 53850, loss = 2144.10 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 00:56:05.621501: step 53860, loss = 2127.03 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 00:56:16.097325: step 53870, loss = 2109.88 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 00:56:26.414580: step 53880, loss = 2093.10 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 00:56:36.737484: step 53890, loss = 2076.55 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:56:47.301509: step 53900, loss = 2060.17 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 00:56:57.556435: step 53910, loss = 2043.62 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 00:57:07.871551: step 53920, loss = 2027.33 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 00:57:18.180197: step 53930, loss = 2011.34 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:57:28.433349: step 53940, loss = 1995.16 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 00:57:38.701339: step 53950, loss = 1979.37 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 00:57:49.015386: step 53960, loss = 1963.63 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 00:57:59.290206: step 53970, loss = 1947.89 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 00:58:09.704687: step 53980, loss = 1932.29 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 00:58:20.006648: step 53990, loss = 1916.92 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 00:58:30.577511: step 54000, loss = 1901.44 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 00:58:40.828003: step 54010, loss = 1886.56 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 00:58:51.134909: step 54020, loss = 1871.48 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 00:59:01.455704: step 54030, loss = 1856.77 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 00:59:11.797694: step 54040, loss = 1841.95 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:59:22.135650: step 54050, loss = 1827.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 00:59:32.494941: step 54060, loss = 1812.75 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 00:59:42.783859: step 54070, loss = 1798.26 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 00:59:53.085567: step 54080, loss = 1783.90 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:00:03.423009: step 54090, loss = 1769.71 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:00:14.061483: step 54100, loss = 1755.44 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 01:00:24.404390: step 54110, loss = 1741.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:00:34.724549: step 54120, loss = 1727.75 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:00:45.046508: step 54130, loss = 1714.08 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:00:55.357286: step 54140, loss = 1700.41 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:01:05.808024: step 54150, loss = 1686.71 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 01:01:16.136604: step 54160, loss = 1673.21 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:01:26.427742: step 54170, loss = 1660.25 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:01:36.752983: step 54180, loss = 1646.74 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 01:01:47.030202: step 54190, loss = 1633.63 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:01:57.652374: step 54200, loss = 1621.02 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 01:02:08.109163: step 54210, loss = 1607.78 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 01:02:18.492779: step 54220, loss = 1595.02 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 01:02:28.838936: step 54230, loss = 1582.33 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:02:39.212430: step 54240, loss = 1569.60 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 01:02:49.650339: step 54250, loss = 1557.32 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 01:03:00.012140: step 54260, loss = 1544.95 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:03:10.504977: step 54270, loss = 1532.67 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 01:03:20.963912: step 54280, loss = 1520.21 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 01:03:31.339034: step 54290, loss = 1508.13 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 01:03:42.002765: step 54300, loss = 1496.20 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 01:03:52.348282: step 54310, loss = 1484.40 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:04:02.715614: step 54320, loss = 1472.42 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 01:04:13.072910: step 54330, loss = 1460.95 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 01:04:23.374638: step 54340, loss = 1449.06 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:04:33.694533: step 54350, loss = 1437.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:04:44.000664: step 54360, loss = 1426.11 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:04:54.302042: step 54370, loss = 1414.72 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:05:04.794684: step 54380, loss = 1403.40 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 01:05:15.213111: step 54390, loss = 1392.50 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 01:05:25.883922: step 54400, loss = 1381.16 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 01:05:36.195374: step 54410, loss = 1370.29 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:05:46.555562: step 54420, loss = 1359.58 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:05:56.925590: step 54430, loss = 1348.43 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 01:06:07.355976: step 54440, loss = 1337.71 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 01:06:17.766864: step 54450, loss = 1327.32 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 01:06:28.077326: step 54460, loss = 1316.73 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:06:38.363370: step 54470, loss = 1305.81 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:06:48.674311: step 54480, loss = 1295.83 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:06:58.959651: step 54490, loss = 1285.50 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:07:09.625073: step 54500, loss = 1275.47 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 01:07:19.913317: step 54510, loss = 1265.19 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:07:30.214246: step 54520, loss = 1254.96 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:07:40.553393: step 54530, loss = 1245.06 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:07:50.935514: step 54540, loss = 1235.04 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 01:08:01.310047: step 54550, loss = 1225.20 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 01:08:11.729478: step 54560, loss = 1215.45 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 01:08:22.056019: step 54570, loss = 1205.80 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 01:08:32.331062: step 54580, loss = 1196.27 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 01:08:42.614140: step 54590, loss = 1186.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:08:53.160914: step 54600, loss = 1177.22 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 01:09:03.447568: step 54610, loss = 1167.89 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:09:13.789465: step 54620, loss = 1158.45 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:09:24.140611: step 54630, loss = 1149.38 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:09:34.439579: step 54640, loss = 1140.24 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:09:44.780624: step 54650, loss = 1131.06 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:09:55.078675: step 54660, loss = 1122.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:10:05.387402: step 54670, loss = 1113.26 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:10:15.738462: step 54680, loss = 1104.24 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:10:26.108052: step 54690, loss = 1095.72 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 01:10:36.655877: step 54700, loss = 1086.86 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 01:10:46.896792: step 54710, loss = 1078.09 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:10:57.184626: step 54720, loss = 1069.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:11:07.546842: step 54730, loss = 1061.08 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:11:17.925680: step 54740, loss = 1052.74 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 01:11:28.217125: step 54750, loss = 1044.15 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:11:38.508135: step 54760, loss = 1036.02 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:11:48.778611: step 54770, loss = 1027.71 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:11:59.059644: step 54780, loss = 1019.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:12:09.433916: step 54790, loss = 1011.48 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 01:12:20.135796: step 54800, loss = 1003.30 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 01:12:30.421262: step 54810, loss = 995.27 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:12:40.711604: step 54820, loss = 987.53 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:12:50.997759: step 54830, loss = 979.55 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:13:01.346343: step 54840, loss = 971.63 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:13:11.678396: step 54850, loss = 964.21 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:13:22.039168: step 54860, loss = 956.39 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:13:32.354261: step 54870, loss = 948.90 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:13:42.750530: step 54880, loss = 941.27 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 01:13:53.141192: step 54890, loss = 934.05 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 01:14:03.827630: step 54900, loss = 926.44 (119.8 examples/sec; 1.069 sec/batch)
2018-04-10 01:14:14.149021: step 54910, loss = 918.79 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:14:24.462299: step 54920, loss = 911.82 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:14:34.738522: step 54930, loss = 904.45 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 01:14:45.075873: step 54940, loss = 896.96 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:14:55.363641: step 54950, loss = 890.24 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:15:05.700094: step 54960, loss = 883.00 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:15:16.043998: step 54970, loss = 875.72 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 01:15:26.359459: step 54980, loss = 869.01 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:15:36.655767: step 54990, loss = 862.23 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:15:47.233312: step 55000, loss = 855.35 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 01:15:57.472978: step 55010, loss = 848.38 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:16:07.825511: step 55020, loss = 841.62 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 01:16:18.162884: step 55030, loss = 834.86 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:16:28.502632: step 55040, loss = 828.24 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:16:38.856266: step 55050, loss = 821.88 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 01:16:49.153613: step 55060, loss = 815.22 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:16:59.479090: step 55070, loss = 808.78 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 01:17:09.843152: step 55080, loss = 802.36 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:17:20.159999: step 55090, loss = 795.82 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:17:30.755208: step 55100, loss = 789.71 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 01:17:41.077556: step 55110, loss = 783.37 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:17:51.571026: step 55120, loss = 777.06 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 01:18:02.038883: step 55130, loss = 770.84 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 01:18:12.502598: step 55140, loss = 764.74 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 01:18:22.946803: step 55150, loss = 758.52 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 01:18:33.360849: step 55160, loss = 752.48 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 01:18:43.691288: step 55170, loss = 746.86 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:18:54.020806: step 55180, loss = 740.67 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:19:04.339320: step 55190, loss = 734.95 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:19:14.983922: step 55200, loss = 729.08 (120.2 examples/sec; 1.064 sec/batch)
2018-04-10 01:19:25.257978: step 55210, loss = 723.12 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:19:35.529953: step 55220, loss = 717.29 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:19:45.821132: step 55230, loss = 711.60 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:19:56.122478: step 55240, loss = 705.96 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:20:06.451011: step 55250, loss = 700.29 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:20:16.783240: step 55260, loss = 694.91 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:20:27.165846: step 55270, loss = 689.30 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 01:20:37.436891: step 55280, loss = 683.90 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:20:47.703480: step 55290, loss = 678.45 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:20:58.290238: step 55300, loss = 672.85 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 01:21:08.588265: step 55310, loss = 667.62 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:21:18.898184: step 55320, loss = 662.40 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:21:29.166932: step 55330, loss = 657.18 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:21:39.437854: step 55340, loss = 651.83 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:21:49.728548: step 55350, loss = 646.68 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:22:00.026655: step 55360, loss = 641.77 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:22:10.343250: step 55370, loss = 636.60 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:22:20.656270: step 55380, loss = 631.39 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:22:30.919659: step 55390, loss = 626.48 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:22:41.466294: step 55400, loss = 621.36 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 01:22:51.705215: step 55410, loss = 616.36 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:23:02.023686: step 55420, loss = 611.65 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:23:12.335780: step 55430, loss = 606.65 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:23:22.605872: step 55440, loss = 601.70 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:23:32.874260: step 55450, loss = 597.07 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:23:43.160873: step 55460, loss = 592.42 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:23:53.461230: step 55470, loss = 587.39 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:24:03.791849: step 55480, loss = 583.01 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:24:14.105946: step 55490, loss = 578.26 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:24:24.686328: step 55500, loss = 573.56 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 01:24:34.951689: step 55510, loss = 569.10 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:24:45.226728: step 55520, loss = 564.69 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 01:24:55.567456: step 55530, loss = 560.07 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:25:05.888662: step 55540, loss = 555.60 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:25:16.198104: step 55550, loss = 551.36 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:25:26.512832: step 55560, loss = 546.79 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:25:36.768675: step 55570, loss = 542.73 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 01:25:47.021262: step 55580, loss = 538.25 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 01:25:57.263832: step 55590, loss = 534.08 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:26:07.860401: step 55600, loss = 529.61 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 01:26:18.122586: step 55610, loss = 525.53 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:26:28.405564: step 55620, loss = 521.43 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:26:38.673653: step 55630, loss = 517.07 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:26:48.941897: step 55640, loss = 513.20 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:26:59.211605: step 55650, loss = 509.05 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:27:09.537128: step 55660, loss = 504.98 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 01:27:19.845983: step 55670, loss = 501.00 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:27:30.194176: step 55680, loss = 496.96 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:27:40.530812: step 55690, loss = 493.01 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:27:51.092595: step 55700, loss = 489.09 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 01:28:01.329504: step 55710, loss = 485.22 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:28:11.633645: step 55720, loss = 481.41 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 01:28:21.914043: step 55730, loss = 477.48 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:28:32.172728: step 55740, loss = 473.74 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 01:28:42.443862: step 55750, loss = 469.81 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:28:52.705175: step 55760, loss = 466.31 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:29:03.002746: step 55770, loss = 462.43 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:29:13.415604: step 55780, loss = 458.92 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 01:29:23.809653: step 55790, loss = 455.09 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 01:29:34.493222: step 55800, loss = 451.68 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 01:29:44.849505: step 55810, loss = 448.15 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 01:29:55.191733: step 55820, loss = 444.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:30:05.530908: step 55830, loss = 441.14 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:30:15.837290: step 55840, loss = 437.33 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:30:26.181420: step 55850, loss = 433.83 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 01:30:36.496415: step 55860, loss = 430.56 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:30:46.740096: step 55870, loss = 427.16 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:30:57.044487: step 55880, loss = 423.80 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 01:31:07.392818: step 55890, loss = 420.29 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:31:18.074279: step 55900, loss = 416.93 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 01:31:28.364229: step 55910, loss = 413.74 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:31:38.629004: step 55920, loss = 410.52 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:31:48.892105: step 55930, loss = 406.98 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:31:59.169619: step 55940, loss = 403.72 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:32:09.491639: step 55950, loss = 400.75 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:32:19.805395: step 55960, loss = 397.62 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:32:30.069817: step 55970, loss = 394.35 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:32:40.331719: step 55980, loss = 391.11 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:32:50.616324: step 55990, loss = 388.10 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:33:01.195817: step 56000, loss = 384.89 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 01:33:11.508016: step 56010, loss = 382.08 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:33:21.806547: step 56020, loss = 379.07 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:33:32.078108: step 56030, loss = 376.06 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:33:42.369816: step 56040, loss = 372.92 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:33:52.669904: step 56050, loss = 370.08 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:34:03.004459: step 56060, loss = 366.96 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:34:13.334375: step 56070, loss = 364.24 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:34:23.623922: step 56080, loss = 361.42 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:34:33.907392: step 56090, loss = 358.44 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:34:44.484999: step 56100, loss = 355.40 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 01:34:54.761100: step 56110, loss = 352.71 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 01:35:05.150375: step 56120, loss = 349.87 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 01:35:15.511650: step 56130, loss = 347.10 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:35:25.821602: step 56140, loss = 344.42 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:35:36.157340: step 56150, loss = 341.82 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:35:46.481912: step 56160, loss = 338.78 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:35:56.812350: step 56170, loss = 336.41 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:36:07.208088: step 56180, loss = 333.46 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 01:36:17.568306: step 56190, loss = 330.84 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:36:28.192764: step 56200, loss = 328.39 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 01:36:38.484320: step 56210, loss = 325.62 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:36:48.805350: step 56220, loss = 323.17 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:36:59.124205: step 56230, loss = 320.59 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:37:09.474888: step 56240, loss = 318.02 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:37:19.802997: step 56250, loss = 315.41 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 01:37:30.091240: step 56260, loss = 312.85 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:37:40.352217: step 56270, loss = 310.50 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:37:50.615905: step 56280, loss = 308.00 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:38:00.861615: step 56290, loss = 305.61 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 01:38:11.465234: step 56300, loss = 303.21 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 01:38:21.745832: step 56310, loss = 300.91 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:38:32.009608: step 56320, loss = 298.63 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:38:42.283824: step 56330, loss = 296.09 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:38:52.577814: step 56340, loss = 293.55 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 01:39:03.012528: step 56350, loss = 291.11 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 01:39:13.366558: step 56360, loss = 289.02 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 01:39:23.650368: step 56370, loss = 286.71 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:39:33.940744: step 56380, loss = 284.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:39:44.210142: step 56390, loss = 282.36 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:39:54.788620: step 56400, loss = 279.92 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 01:40:05.103815: step 56410, loss = 277.77 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:40:15.464899: step 56420, loss = 275.53 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:40:25.832406: step 56430, loss = 273.38 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 01:40:36.134120: step 56440, loss = 271.20 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:40:46.400718: step 56450, loss = 269.22 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:40:56.719278: step 56460, loss = 266.99 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:41:07.079003: step 56470, loss = 264.68 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 01:41:17.387594: step 56480, loss = 262.65 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:41:27.661721: step 56490, loss = 260.52 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:41:38.244181: step 56500, loss = 258.38 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 01:41:48.512688: step 56510, loss = 256.58 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:41:58.810512: step 56520, loss = 254.42 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:42:09.152899: step 56530, loss = 252.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:42:19.493385: step 56540, loss = 250.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 01:42:29.802981: step 56550, loss = 248.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:42:40.120538: step 56560, loss = 246.40 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:42:50.433063: step 56570, loss = 244.56 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:43:00.748991: step 56580, loss = 242.54 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:43:11.092552: step 56590, loss = 240.76 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 01:43:21.725964: step 56600, loss = 238.76 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 01:43:31.979066: step 56610, loss = 236.93 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 01:43:42.290977: step 56620, loss = 235.10 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:43:52.687723: step 56630, loss = 233.05 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 01:44:03.240635: step 56640, loss = 231.39 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 01:44:13.766015: step 56650, loss = 229.43 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 01:44:24.245033: step 56660, loss = 227.58 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 01:44:34.714578: step 56670, loss = 225.93 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 01:44:45.183725: step 56680, loss = 224.20 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 01:44:55.577540: step 56690, loss = 222.22 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 01:45:06.353810: step 56700, loss = 220.57 (118.8 examples/sec; 1.078 sec/batch)
2018-04-10 01:45:16.878210: step 56710, loss = 218.71 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 01:45:27.312359: step 56720, loss = 217.16 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 01:45:37.703156: step 56730, loss = 215.36 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 01:45:48.162605: step 56740, loss = 213.73 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 01:45:58.577411: step 56750, loss = 211.94 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 01:46:08.950824: step 56760, loss = 210.08 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 01:46:19.220850: step 56770, loss = 208.45 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:46:29.501567: step 56780, loss = 207.10 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:46:39.754070: step 56790, loss = 205.23 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 01:46:50.368345: step 56800, loss = 203.89 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 01:47:00.768123: step 56810, loss = 202.17 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 01:47:11.115365: step 56820, loss = 200.42 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 01:47:21.428396: step 56830, loss = 199.10 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:47:31.977871: step 56840, loss = 197.06 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 01:47:42.256276: step 56850, loss = 195.76 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:47:52.541567: step 56860, loss = 194.24 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:48:02.843505: step 56870, loss = 192.77 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 01:48:13.141461: step 56880, loss = 191.20 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:48:23.408641: step 56890, loss = 189.74 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:48:33.966994: step 56900, loss = 188.09 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 01:48:44.185456: step 56910, loss = 186.69 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 01:48:54.468983: step 56920, loss = 185.23 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:49:04.750158: step 56930, loss = 183.77 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:49:15.027036: step 56940, loss = 182.17 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 01:49:25.278856: step 56950, loss = 180.87 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 01:49:35.545277: step 56960, loss = 179.37 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:49:45.783463: step 56970, loss = 177.95 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:49:56.031919: step 56980, loss = 176.51 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 01:50:06.355602: step 56990, loss = 175.33 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:50:17.026372: step 57000, loss = 173.75 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 01:50:27.352510: step 57010, loss = 172.66 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 01:50:37.601027: step 57020, loss = 171.04 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 01:50:47.855815: step 57030, loss = 169.72 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 01:50:58.147487: step 57040, loss = 168.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:51:08.500720: step 57050, loss = 167.15 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 01:51:18.802820: step 57060, loss = 165.57 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 01:51:29.041611: step 57070, loss = 164.45 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:51:39.308635: step 57080, loss = 163.15 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:51:49.586512: step 57090, loss = 161.81 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:52:00.153534: step 57100, loss = 160.53 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 01:52:10.450400: step 57110, loss = 159.32 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 01:52:20.761009: step 57120, loss = 157.95 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 01:52:31.015804: step 57130, loss = 156.84 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 01:52:41.284819: step 57140, loss = 155.62 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:52:51.551643: step 57150, loss = 154.43 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:53:01.875889: step 57160, loss = 153.07 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 01:53:12.166670: step 57170, loss = 151.89 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:53:22.442016: step 57180, loss = 150.73 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 01:53:32.698898: step 57190, loss = 149.95 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 01:53:43.254501: step 57200, loss = 148.00 (121.3 examples/sec; 1.056 sec/batch)
2018-04-10 01:53:53.527448: step 57210, loss = 147.18 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:54:03.833152: step 57220, loss = 146.12 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 01:54:14.106336: step 57230, loss = 144.87 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:54:24.350905: step 57240, loss = 143.46 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 01:54:34.600168: step 57250, loss = 142.46 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 01:54:44.894201: step 57260, loss = 141.55 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 01:54:55.134288: step 57270, loss = 140.59 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:55:05.438012: step 57280, loss = 139.17 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 01:55:15.712900: step 57290, loss = 138.03 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 01:55:26.307917: step 57300, loss = 136.97 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 01:55:36.560372: step 57310, loss = 135.99 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 01:55:46.854218: step 57320, loss = 134.88 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 01:55:57.139725: step 57330, loss = 134.18 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 01:56:07.507742: step 57340, loss = 132.65 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 01:56:17.884727: step 57350, loss = 131.69 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 01:56:28.169156: step 57360, loss = 130.62 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:56:38.414517: step 57370, loss = 129.57 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 01:56:48.658902: step 57380, loss = 128.54 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 01:56:58.920391: step 57390, loss = 127.46 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:57:09.523173: step 57400, loss = 126.66 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 01:57:19.804060: step 57410, loss = 125.50 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 01:57:30.046678: step 57420, loss = 124.82 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 01:57:40.269109: step 57430, loss = 123.78 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 01:57:50.531545: step 57440, loss = 122.72 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:58:00.789436: step 57450, loss = 121.58 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 01:58:11.105968: step 57460, loss = 120.67 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 01:58:21.372435: step 57470, loss = 119.69 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 01:58:31.635467: step 57480, loss = 118.89 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:58:41.852451: step 57490, loss = 117.85 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 01:58:52.441926: step 57500, loss = 117.03 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 01:59:02.768318: step 57510, loss = 116.24 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 01:59:13.131910: step 57520, loss = 115.19 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 01:59:23.572616: step 57530, loss = 114.03 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 01:59:33.833927: step 57540, loss = 113.44 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 01:59:44.094335: step 57550, loss = 112.44 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 01:59:54.370933: step 57560, loss = 111.54 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:00:04.689096: step 57570, loss = 110.70 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:00:15.002185: step 57580, loss = 109.83 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:00:25.348037: step 57590, loss = 108.99 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:00:35.915493: step 57600, loss = 108.07 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 02:00:46.208764: step 57610, loss = 107.24 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:00:56.480565: step 57620, loss = 106.36 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:01:06.811721: step 57630, loss = 105.53 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:01:17.095025: step 57640, loss = 105.09 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:01:27.376445: step 57650, loss = 103.97 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:01:37.648879: step 57660, loss = 103.06 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:01:47.900669: step 57670, loss = 102.47 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:01:58.166322: step 57680, loss = 101.44 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:02:08.490529: step 57690, loss = 100.69 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:02:19.092654: step 57700, loss = 100.04 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 02:02:29.344304: step 57710, loss = 99.02 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:02:39.616187: step 57720, loss = 98.24 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:02:49.893486: step 57730, loss = 97.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:03:00.190059: step 57740, loss = 96.76 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:03:10.528492: step 57750, loss = 95.81 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:03:20.832388: step 57760, loss = 95.37 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:03:31.088851: step 57770, loss = 94.44 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:03:41.365949: step 57780, loss = 93.68 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:03:51.609407: step 57790, loss = 93.07 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 02:04:02.186435: step 57800, loss = 92.61 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 02:04:12.473838: step 57810, loss = 91.66 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:04:22.775219: step 57820, loss = 90.99 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:04:33.030111: step 57830, loss = 90.35 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 02:04:43.274186: step 57840, loss = 89.59 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 02:04:53.555949: step 57850, loss = 88.59 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:05:03.883020: step 57860, loss = 88.16 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:05:14.244432: step 57870, loss = 87.35 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:05:24.549349: step 57880, loss = 86.68 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:05:34.850562: step 57890, loss = 86.04 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:05:45.436050: step 57900, loss = 85.31 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 02:05:55.731146: step 57910, loss = 84.87 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:06:06.139180: step 57920, loss = 83.97 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 02:06:16.466820: step 57930, loss = 83.30 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:06:26.781906: step 57940, loss = 82.68 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:06:37.086819: step 57950, loss = 82.07 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:06:47.387602: step 57960, loss = 81.46 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:06:57.681840: step 57970, loss = 80.52 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 02:07:08.038852: step 57980, loss = 80.23 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:07:18.377270: step 57990, loss = 79.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:07:29.020215: step 58000, loss = 78.97 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 02:07:39.319796: step 58010, loss = 78.44 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:07:49.600432: step 58020, loss = 77.59 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:07:59.891557: step 58030, loss = 77.09 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:08:10.233536: step 58040, loss = 76.43 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:08:20.563773: step 58050, loss = 75.84 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:08:30.836481: step 58060, loss = 75.39 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:08:41.083741: step 58070, loss = 74.80 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:08:51.334842: step 58080, loss = 74.17 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:09:01.710921: step 58090, loss = 73.49 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 02:09:12.347232: step 58100, loss = 72.95 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 02:09:22.638211: step 58110, loss = 72.35 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:09:32.926712: step 58120, loss = 71.62 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:09:43.195008: step 58130, loss = 71.10 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:09:53.506514: step 58140, loss = 70.70 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:10:03.866792: step 58150, loss = 70.46 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:10:14.248678: step 58160, loss = 69.67 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 02:10:24.761262: step 58170, loss = 69.10 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 02:10:35.161220: step 58180, loss = 68.45 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 02:10:45.518157: step 58190, loss = 68.02 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:10:56.126544: step 58200, loss = 67.42 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 02:11:06.516444: step 58210, loss = 66.82 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 02:11:16.871916: step 58220, loss = 66.27 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:11:27.156548: step 58230, loss = 65.99 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:11:37.499926: step 58240, loss = 65.41 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:11:47.810926: step 58250, loss = 64.76 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:11:58.119002: step 58260, loss = 64.25 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:12:08.453733: step 58270, loss = 64.08 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:12:18.812338: step 58280, loss = 63.27 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:12:29.124799: step 58290, loss = 62.87 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:12:39.724731: step 58300, loss = 62.62 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 02:12:49.980145: step 58310, loss = 61.85 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:13:00.307878: step 58320, loss = 61.45 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:13:10.630104: step 58330, loss = 61.20 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:13:20.954588: step 58340, loss = 60.49 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:13:31.247857: step 58350, loss = 59.91 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:13:41.537645: step 58360, loss = 59.62 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:13:51.812696: step 58370, loss = 58.82 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:14:02.133838: step 58380, loss = 58.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:14:12.432538: step 58390, loss = 58.07 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:14:23.041488: step 58400, loss = 57.56 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 02:14:33.271513: step 58410, loss = 57.08 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 02:14:43.556302: step 58420, loss = 56.70 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:14:53.826793: step 58430, loss = 56.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:15:04.141421: step 58440, loss = 55.76 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:15:14.453635: step 58450, loss = 55.45 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:15:24.761571: step 58460, loss = 54.91 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:15:35.044677: step 58470, loss = 54.62 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:15:45.382856: step 58480, loss = 54.22 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:15:55.681080: step 58490, loss = 53.83 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:16:06.354274: step 58500, loss = 53.50 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 02:16:16.653535: step 58510, loss = 53.01 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:16:27.014286: step 58520, loss = 52.49 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:16:37.414183: step 58530, loss = 52.14 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 02:16:47.800441: step 58540, loss = 51.76 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 02:16:58.317208: step 58550, loss = 51.35 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 02:17:08.786713: step 58560, loss = 51.02 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 02:17:19.111403: step 58570, loss = 50.51 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:17:29.400804: step 58580, loss = 50.25 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:17:39.679994: step 58590, loss = 49.73 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:17:50.290387: step 58600, loss = 49.46 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 02:18:00.546706: step 58610, loss = 49.05 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:18:10.902565: step 58620, loss = 48.61 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:18:21.212560: step 58630, loss = 48.11 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:18:31.490119: step 58640, loss = 48.01 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:18:41.775788: step 58650, loss = 47.51 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:18:52.057560: step 58660, loss = 47.25 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:19:02.373586: step 58670, loss = 46.84 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:19:12.676270: step 58680, loss = 46.44 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:19:22.964107: step 58690, loss = 46.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:19:33.533392: step 58700, loss = 45.79 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 02:19:43.796300: step 58710, loss = 45.34 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:19:54.077159: step 58720, loss = 44.88 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:20:04.418317: step 58730, loss = 44.71 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:20:14.756380: step 58740, loss = 44.14 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:20:25.173554: step 58750, loss = 43.88 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 02:20:35.493244: step 58760, loss = 43.65 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:20:45.796733: step 58770, loss = 43.31 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:20:56.061976: step 58780, loss = 42.77 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:21:06.404667: step 58790, loss = 42.60 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:21:17.047571: step 58800, loss = 42.38 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 02:21:27.358151: step 58810, loss = 41.72 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:21:37.658009: step 58820, loss = 41.76 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:21:47.941314: step 58830, loss = 41.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:21:58.249343: step 58840, loss = 40.98 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:22:08.622143: step 58850, loss = 40.43 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 02:22:18.951497: step 58860, loss = 40.35 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:22:29.273350: step 58870, loss = 40.07 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:22:39.594976: step 58880, loss = 39.75 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:22:49.885453: step 58890, loss = 39.56 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:23:00.478299: step 58900, loss = 39.09 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 02:23:10.790702: step 58910, loss = 38.96 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:23:21.107180: step 58920, loss = 38.63 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:23:31.436398: step 58930, loss = 38.41 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:23:41.714918: step 58940, loss = 37.90 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:23:51.984500: step 58950, loss = 37.89 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:24:02.320723: step 58960, loss = 37.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:24:12.646962: step 58970, loss = 37.00 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 02:24:22.946129: step 58980, loss = 36.79 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:24:33.205773: step 58990, loss = 36.66 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:24:43.773707: step 59000, loss = 36.16 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 02:24:53.997059: step 59010, loss = 35.99 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 02:25:04.300575: step 59020, loss = 35.67 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:25:14.598943: step 59030, loss = 35.32 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:25:24.930655: step 59040, loss = 35.24 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:25:35.185872: step 59050, loss = 34.81 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:25:45.492820: step 59060, loss = 34.55 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:25:55.821343: step 59070, loss = 34.30 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:26:06.165941: step 59080, loss = 34.13 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 02:26:16.500275: step 59090, loss = 33.81 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:26:27.109778: step 59100, loss = 33.48 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 02:26:37.374805: step 59110, loss = 33.37 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:26:47.652152: step 59120, loss = 33.25 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:26:57.917423: step 59130, loss = 32.80 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:27:08.246663: step 59140, loss = 32.65 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:27:18.560332: step 59150, loss = 32.42 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:27:28.906003: step 59160, loss = 32.16 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:27:39.218538: step 59170, loss = 31.87 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:27:49.496184: step 59180, loss = 31.57 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:27:59.763453: step 59190, loss = 31.48 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:28:10.378350: step 59200, loss = 31.20 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 02:28:20.649345: step 59210, loss = 30.90 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:28:30.955497: step 59220, loss = 30.58 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:28:41.209316: step 59230, loss = 30.59 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 02:28:51.495160: step 59240, loss = 30.19 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:29:01.827406: step 59250, loss = 29.99 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:29:12.189025: step 59260, loss = 29.75 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:29:22.496275: step 59270, loss = 29.64 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:29:32.800205: step 59280, loss = 29.33 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:29:43.107456: step 59290, loss = 29.16 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:29:53.708249: step 59300, loss = 28.80 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 02:30:04.052943: step 59310, loss = 28.63 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 02:30:14.461433: step 59320, loss = 28.40 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 02:30:24.780643: step 59330, loss = 28.10 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:30:35.085399: step 59340, loss = 27.89 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:30:45.377317: step 59350, loss = 27.75 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:30:55.745290: step 59360, loss = 27.79 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 02:31:06.093947: step 59370, loss = 27.29 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:31:16.460460: step 59380, loss = 27.35 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 02:31:26.791949: step 59390, loss = 26.93 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:31:37.414025: step 59400, loss = 26.56 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 02:31:47.729286: step 59410, loss = 26.30 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:31:58.049491: step 59420, loss = 26.52 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:32:08.410623: step 59430, loss = 26.10 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:32:18.768313: step 59440, loss = 26.04 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:32:29.124333: step 59450, loss = 25.77 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:32:39.464269: step 59460, loss = 25.64 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:32:49.763868: step 59470, loss = 25.20 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:33:00.076192: step 59480, loss = 25.39 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:33:10.447596: step 59490, loss = 24.89 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 02:33:21.088066: step 59500, loss = 24.69 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 02:33:31.386498: step 59510, loss = 24.69 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:33:41.686897: step 59520, loss = 24.39 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:33:51.978320: step 59530, loss = 24.32 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:34:02.309011: step 59540, loss = 24.14 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:34:12.627640: step 59550, loss = 23.84 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:34:22.928345: step 59560, loss = 23.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:34:33.210353: step 59570, loss = 23.65 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:34:43.485658: step 59580, loss = 23.39 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:34:53.750467: step 59590, loss = 23.20 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:35:04.357590: step 59600, loss = 22.86 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 02:35:14.683051: step 59610, loss = 23.01 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 02:35:25.042113: step 59620, loss = 22.77 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:35:35.709358: step 59630, loss = 22.54 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 02:35:46.107766: step 59640, loss = 22.34 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 02:35:56.446890: step 59650, loss = 22.18 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:36:07.010845: step 59660, loss = 21.84 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 02:36:17.375787: step 59670, loss = 21.69 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:36:27.697099: step 59680, loss = 21.67 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:36:38.016360: step 59690, loss = 21.43 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:36:48.610580: step 59700, loss = 21.41 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 02:36:58.887344: step 59710, loss = 20.94 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:37:09.220810: step 59720, loss = 21.25 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:37:19.534309: step 59730, loss = 20.91 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:37:29.829114: step 59740, loss = 20.72 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 02:37:40.148969: step 59750, loss = 20.53 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:37:50.454081: step 59760, loss = 20.65 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:38:00.831531: step 59770, loss = 20.22 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 02:38:11.198582: step 59780, loss = 19.96 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 02:38:21.535353: step 59790, loss = 20.06 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:38:32.136097: step 59800, loss = 19.85 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 02:38:42.410659: step 59810, loss = 19.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:38:52.721214: step 59820, loss = 19.52 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:39:03.067250: step 59830, loss = 19.30 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:39:13.398380: step 59840, loss = 19.08 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:39:23.743272: step 59850, loss = 19.11 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 02:39:34.091853: step 59860, loss = 18.97 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:39:44.377426: step 59870, loss = 18.91 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:39:54.757018: step 59880, loss = 18.87 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 02:40:05.087213: step 59890, loss = 18.74 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:40:15.696967: step 59900, loss = 18.38 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 02:40:26.052106: step 59910, loss = 18.32 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 02:40:36.360567: step 59920, loss = 18.21 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:40:46.636208: step 59930, loss = 18.12 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:40:56.952189: step 59940, loss = 17.80 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:41:07.377237: step 59950, loss = 17.85 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 02:41:17.749928: step 59960, loss = 17.81 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 02:41:28.058663: step 59970, loss = 17.48 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:41:38.331146: step 59980, loss = 17.44 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:41:48.621031: step 59990, loss = 17.40 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:41:59.178444: step 60000, loss = 17.15 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 02:42:09.485982: step 60010, loss = 17.03 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:42:19.816531: step 60020, loss = 16.93 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:42:30.086320: step 60030, loss = 16.70 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:42:40.401049: step 60040, loss = 16.58 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:42:50.666158: step 60050, loss = 16.38 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:43:00.938231: step 60060, loss = 16.12 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:43:11.300240: step 60070, loss = 16.32 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 02:43:21.616933: step 60080, loss = 16.24 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:43:31.880577: step 60090, loss = 16.15 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:43:42.434205: step 60100, loss = 15.77 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 02:43:52.694173: step 60110, loss = 15.73 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:44:03.061976: step 60120, loss = 15.55 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 02:44:13.368527: step 60130, loss = 15.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:44:23.655547: step 60140, loss = 15.59 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:44:33.929569: step 60150, loss = 15.17 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:44:44.233131: step 60160, loss = 15.38 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:44:54.488775: step 60170, loss = 15.39 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:45:04.779157: step 60180, loss = 15.04 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:45:15.098776: step 60190, loss = 14.97 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:45:25.705951: step 60200, loss = 14.91 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 02:45:35.977231: step 60210, loss = 14.75 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:45:46.274345: step 60220, loss = 14.70 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:45:56.577389: step 60230, loss = 14.64 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:46:07.019364: step 60240, loss = 14.38 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 02:46:17.356519: step 60250, loss = 14.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:46:27.659120: step 60260, loss = 14.01 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:46:37.917736: step 60270, loss = 14.02 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:46:48.172193: step 60280, loss = 13.97 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 02:46:58.466126: step 60290, loss = 13.95 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 02:47:09.093664: step 60300, loss = 13.88 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 02:47:19.374990: step 60310, loss = 13.81 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:47:29.694523: step 60320, loss = 13.59 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:47:39.978475: step 60330, loss = 13.61 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:47:50.260002: step 60340, loss = 13.30 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:48:00.583799: step 60350, loss = 13.19 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:48:10.927426: step 60360, loss = 13.37 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 02:48:21.246588: step 60370, loss = 12.94 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:48:31.551264: step 60380, loss = 12.94 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:48:41.846703: step 60390, loss = 12.94 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:48:52.424739: step 60400, loss = 12.90 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 02:49:02.725968: step 60410, loss = 12.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:49:13.030045: step 60420, loss = 12.63 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:49:23.331910: step 60430, loss = 12.57 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 02:49:33.608130: step 60440, loss = 12.55 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:49:43.888332: step 60450, loss = 12.50 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:49:54.170774: step 60460, loss = 12.30 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:50:04.500012: step 60470, loss = 12.24 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:50:14.849495: step 60480, loss = 12.27 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:50:25.177231: step 60490, loss = 12.03 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 02:50:35.739624: step 60500, loss = 11.81 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 02:50:45.931234: step 60510, loss = 11.76 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 02:50:56.192347: step 60520, loss = 11.76 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:51:06.510781: step 60530, loss = 11.77 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 02:51:16.848222: step 60540, loss = 11.62 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 02:51:27.132018: step 60550, loss = 11.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:51:37.444652: step 60560, loss = 11.47 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 02:51:47.729226: step 60570, loss = 11.35 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:51:57.992827: step 60580, loss = 11.62 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:52:08.318210: step 60590, loss = 11.29 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 02:52:18.903678: step 60600, loss = 11.27 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 02:52:29.150576: step 60610, loss = 11.19 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:52:39.415175: step 60620, loss = 11.01 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:52:49.660948: step 60630, loss = 10.74 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:52:59.932519: step 60640, loss = 10.78 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:53:10.241961: step 60650, loss = 10.73 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:53:20.547664: step 60660, loss = 10.52 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 02:53:30.798712: step 60670, loss = 10.62 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:53:41.045959: step 60680, loss = 10.79 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:53:51.325555: step 60690, loss = 10.37 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:54:01.953395: step 60700, loss = 10.34 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 02:54:12.200963: step 60710, loss = 10.38 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:54:22.457741: step 60720, loss = 10.31 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:54:32.689631: step 60730, loss = 10.11 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 02:54:42.917096: step 60740, loss = 10.12 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 02:54:53.162396: step 60750, loss = 10.02 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:55:03.538778: step 60760, loss = 9.97 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 02:55:13.806585: step 60770, loss = 10.01 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 02:55:24.083604: step 60780, loss = 9.77 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:55:34.357825: step 60790, loss = 9.72 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:55:44.921892: step 60800, loss = 9.63 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 02:55:55.159111: step 60810, loss = 9.63 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 02:56:05.512383: step 60820, loss = 9.48 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 02:56:15.860665: step 60830, loss = 9.49 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 02:56:26.144464: step 60840, loss = 9.70 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 02:56:36.419455: step 60850, loss = 9.25 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:56:46.709088: step 60860, loss = 9.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 02:56:56.971024: step 60870, loss = 9.19 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 02:57:07.289122: step 60880, loss = 9.38 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 02:57:17.546158: step 60890, loss = 9.25 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:57:28.089448: step 60900, loss = 9.18 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 02:57:38.302805: step 60910, loss = 8.94 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 02:57:48.546571: step 60920, loss = 8.98 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 02:57:58.777860: step 60930, loss = 8.81 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 02:58:09.078459: step 60940, loss = 8.92 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 02:58:19.352033: step 60950, loss = 8.75 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:58:29.621207: step 60960, loss = 8.77 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 02:58:39.847196: step 60970, loss = 8.68 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 02:58:50.073650: step 60980, loss = 8.79 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 02:59:00.497578: step 60990, loss = 8.71 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 02:59:11.158115: step 61000, loss = 8.54 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 02:59:21.405812: step 61010, loss = 8.50 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 02:59:31.681951: step 61020, loss = 8.34 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 02:59:41.938016: step 61030, loss = 8.29 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 02:59:52.200404: step 61040, loss = 8.22 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:00:02.485869: step 61050, loss = 8.15 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:00:12.848600: step 61060, loss = 8.13 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 03:00:23.232512: step 61070, loss = 8.25 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 03:00:33.494637: step 61080, loss = 8.11 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:00:43.735373: step 61090, loss = 7.97 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:00:54.283015: step 61100, loss = 7.97 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 03:01:04.610602: step 61110, loss = 7.99 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:01:14.953427: step 61120, loss = 7.91 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:01:25.223567: step 61130, loss = 7.89 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:01:35.511835: step 61140, loss = 7.97 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:01:45.782484: step 61150, loss = 7.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:01:56.044532: step 61160, loss = 7.57 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:02:06.371143: step 61170, loss = 7.70 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 03:02:16.695868: step 61180, loss = 7.71 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:02:26.960538: step 61190, loss = 7.42 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:02:37.531386: step 61200, loss = 7.52 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 03:02:47.816421: step 61210, loss = 7.48 (124.5 examples/sec; 1.029 sec/batch)
2018-04-10 03:02:58.093388: step 61220, loss = 7.34 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:03:08.433354: step 61230, loss = 7.23 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:03:18.780046: step 61240, loss = 7.36 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:03:29.111053: step 61250, loss = 7.18 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:03:39.449405: step 61260, loss = 7.18 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:03:49.771893: step 61270, loss = 7.42 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:04:00.078559: step 61280, loss = 6.97 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:04:10.452850: step 61290, loss = 7.10 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 03:04:21.072543: step 61300, loss = 7.26 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 03:04:31.340406: step 61310, loss = 6.86 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:04:41.633742: step 61320, loss = 6.83 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:04:51.900530: step 61330, loss = 7.03 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:05:02.210798: step 61340, loss = 6.76 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:05:12.512990: step 61350, loss = 6.70 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:05:22.801247: step 61360, loss = 7.00 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:05:33.060614: step 61370, loss = 6.63 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:05:43.303930: step 61380, loss = 6.76 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:05:53.567759: step 61390, loss = 6.73 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:06:04.180584: step 61400, loss = 6.57 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 03:06:14.493084: step 61410, loss = 6.34 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:06:24.772894: step 61420, loss = 6.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:06:35.044920: step 61430, loss = 6.48 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:06:45.314640: step 61440, loss = 6.41 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:06:55.607738: step 61450, loss = 6.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:07:05.918427: step 61460, loss = 6.37 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:07:16.243788: step 61470, loss = 6.27 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 03:07:26.516610: step 61480, loss = 6.25 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:07:36.769374: step 61490, loss = 6.05 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 03:07:47.369854: step 61500, loss = 6.13 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 03:07:57.609258: step 61510, loss = 6.14 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:08:07.934862: step 61520, loss = 6.28 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 03:08:18.193081: step 61530, loss = 6.02 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:08:28.460252: step 61540, loss = 6.11 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:08:38.747427: step 61550, loss = 5.97 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:08:49.026656: step 61560, loss = 6.02 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:08:59.409816: step 61570, loss = 5.86 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 03:09:09.821382: step 61580, loss = 6.07 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 03:09:20.126266: step 61590, loss = 5.97 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:09:30.710976: step 61600, loss = 5.86 (120.9 examples/sec; 1.058 sec/batch)
2018-04-10 03:09:40.975728: step 61610, loss = 5.96 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:09:51.247355: step 61620, loss = 5.80 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:10:01.524324: step 61630, loss = 5.81 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:10:11.837086: step 61640, loss = 5.77 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:10:22.296207: step 61650, loss = 5.66 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 03:10:32.517625: step 61660, loss = 5.52 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:10:42.776008: step 61670, loss = 5.87 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:10:53.060870: step 61680, loss = 5.62 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:11:03.401110: step 61690, loss = 5.37 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:11:14.017169: step 61700, loss = 5.49 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 03:11:24.314661: step 61710, loss = 5.44 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:11:34.621227: step 61720, loss = 5.58 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:11:44.882560: step 61730, loss = 5.74 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:11:55.167567: step 61740, loss = 5.47 (124.5 examples/sec; 1.029 sec/batch)
2018-04-10 03:12:05.488060: step 61750, loss = 5.24 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:12:15.838121: step 61760, loss = 5.39 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:12:26.104272: step 61770, loss = 5.28 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:12:36.384449: step 61780, loss = 5.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:12:46.724471: step 61790, loss = 5.03 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:12:57.327282: step 61800, loss = 5.25 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 03:13:07.591818: step 61810, loss = 5.05 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:13:17.919197: step 61820, loss = 5.12 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:13:28.187330: step 61830, loss = 5.28 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:13:38.452752: step 61840, loss = 5.08 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:13:48.716199: step 61850, loss = 5.06 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:13:59.021630: step 61860, loss = 4.97 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:14:09.360987: step 61870, loss = 5.01 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:14:19.674781: step 61880, loss = 5.03 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:14:30.032158: step 61890, loss = 4.78 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 03:14:40.602095: step 61900, loss = 4.99 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 03:14:50.857872: step 61910, loss = 4.87 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:15:01.123711: step 61920, loss = 4.79 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:15:11.465410: step 61930, loss = 4.69 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:15:21.782869: step 61940, loss = 4.70 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 03:15:32.058601: step 61950, loss = 4.58 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:15:42.374781: step 61960, loss = 4.86 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 03:15:52.646172: step 61970, loss = 4.64 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:16:02.978769: step 61980, loss = 4.77 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:16:13.268628: step 61990, loss = 4.76 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:16:23.902997: step 62000, loss = 4.79 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 03:16:34.254515: step 62010, loss = 4.79 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:16:44.603840: step 62020, loss = 4.66 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:16:54.881342: step 62030, loss = 4.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:17:05.229617: step 62040, loss = 4.51 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:17:15.536214: step 62050, loss = 4.54 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:17:25.803593: step 62060, loss = 4.36 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:17:36.063654: step 62070, loss = 4.49 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:17:46.335300: step 62080, loss = 4.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:17:56.633852: step 62090, loss = 4.45 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:18:07.266884: step 62100, loss = 4.50 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 03:18:17.571110: step 62110, loss = 4.32 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:18:27.877376: step 62120, loss = 4.37 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:18:38.171442: step 62130, loss = 4.51 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 03:18:48.437477: step 62140, loss = 4.32 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:18:58.704037: step 62150, loss = 4.20 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:19:09.021874: step 62160, loss = 4.35 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 03:19:19.305808: step 62170, loss = 4.17 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:19:29.581941: step 62180, loss = 4.26 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:19:39.818030: step 62190, loss = 4.15 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:19:50.372942: step 62200, loss = 4.16 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 03:20:00.637997: step 62210, loss = 4.20 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:20:10.972695: step 62220, loss = 4.13 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:20:21.264520: step 62230, loss = 4.06 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:20:31.639498: step 62240, loss = 4.26 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 03:20:41.919283: step 62250, loss = 3.93 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:20:52.199699: step 62260, loss = 4.21 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:21:02.508756: step 62270, loss = 4.14 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:21:12.812007: step 62280, loss = 3.97 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:21:23.119845: step 62290, loss = 4.17 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:21:33.684184: step 62300, loss = 4.15 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 03:21:43.983775: step 62310, loss = 4.00 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:21:54.241763: step 62320, loss = 3.92 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:22:04.544714: step 62330, loss = 3.94 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:22:14.873677: step 62340, loss = 3.90 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:22:25.147571: step 62350, loss = 3.96 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:22:35.434812: step 62360, loss = 3.92 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:22:45.742024: step 62370, loss = 3.87 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:22:56.051791: step 62380, loss = 3.66 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:23:06.353855: step 62390, loss = 3.64 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:23:16.944826: step 62400, loss = 3.74 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 03:23:27.177925: step 62410, loss = 3.79 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 03:23:37.487670: step 62420, loss = 3.91 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:23:47.766314: step 62430, loss = 3.66 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:23:58.051341: step 62440, loss = 3.64 (124.5 examples/sec; 1.029 sec/batch)
2018-04-10 03:24:08.364702: step 62450, loss = 3.63 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:24:18.623380: step 62460, loss = 3.88 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:24:28.898197: step 62470, loss = 3.40 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:24:39.114219: step 62480, loss = 3.82 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 03:24:49.339697: step 62490, loss = 3.51 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 03:24:59.867880: step 62500, loss = 3.61 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 03:25:10.144893: step 62510, loss = 3.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:25:20.433350: step 62520, loss = 3.74 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:25:30.706007: step 62530, loss = 3.45 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:25:40.975628: step 62540, loss = 3.63 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:25:51.224307: step 62550, loss = 3.48 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:26:01.536091: step 62560, loss = 3.34 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:26:11.802218: step 62570, loss = 3.33 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:26:22.064409: step 62580, loss = 3.42 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:26:32.314310: step 62590, loss = 3.43 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:26:42.886818: step 62600, loss = 3.24 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 03:26:53.136882: step 62610, loss = 3.63 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:27:03.412070: step 62620, loss = 3.42 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:27:13.693499: step 62630, loss = 3.40 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:27:23.950145: step 62640, loss = 3.54 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:27:34.211871: step 62650, loss = 3.37 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:27:44.498641: step 62660, loss = 3.37 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:27:54.723118: step 62670, loss = 3.48 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:28:05.035834: step 62680, loss = 3.43 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:28:15.353597: step 62690, loss = 3.45 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 03:28:25.922374: step 62700, loss = 3.17 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 03:28:36.203176: step 62710, loss = 3.41 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:28:46.505751: step 62720, loss = 3.16 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:28:56.788042: step 62730, loss = 3.63 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:29:07.110386: step 62740, loss = 3.35 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:29:17.444893: step 62750, loss = 3.13 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:29:27.770710: step 62760, loss = 3.28 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 03:29:38.092214: step 62770, loss = 3.18 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:29:48.386580: step 62780, loss = 3.17 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 03:29:58.739969: step 62790, loss = 3.21 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 03:30:09.341838: step 62800, loss = 3.27 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 03:30:19.601516: step 62810, loss = 3.31 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:30:29.973789: step 62820, loss = 3.06 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 03:30:40.221923: step 62830, loss = 3.12 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:30:50.482645: step 62840, loss = 3.22 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:31:00.764309: step 62850, loss = 3.03 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:31:11.117386: step 62860, loss = 3.19 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 03:31:21.415664: step 62870, loss = 3.06 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:31:31.724729: step 62880, loss = 2.91 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:31:42.003679: step 62890, loss = 3.04 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:31:52.591714: step 62900, loss = 3.03 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 03:32:02.913206: step 62910, loss = 3.09 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:32:13.234011: step 62920, loss = 3.07 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:32:23.519091: step 62930, loss = 2.99 (124.5 examples/sec; 1.029 sec/batch)
2018-04-10 03:32:33.803350: step 62940, loss = 3.09 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:32:44.074462: step 62950, loss = 3.17 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:32:54.348775: step 62960, loss = 2.99 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:33:04.655060: step 62970, loss = 2.74 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:33:14.965322: step 62980, loss = 2.94 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:33:25.228724: step 62990, loss = 2.97 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:33:35.813315: step 63000, loss = 2.99 (120.9 examples/sec; 1.058 sec/batch)
2018-04-10 03:33:46.101294: step 63010, loss = 2.91 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:33:56.388176: step 63020, loss = 2.82 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:34:06.696714: step 63030, loss = 2.87 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:34:16.984502: step 63040, loss = 2.94 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:34:27.257251: step 63050, loss = 2.93 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:34:37.523719: step 63060, loss = 3.11 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:34:47.756328: step 63070, loss = 2.86 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 03:34:58.010327: step 63080, loss = 3.14 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 03:35:08.316522: step 63090, loss = 2.63 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:35:18.860616: step 63100, loss = 2.87 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 03:35:29.059675: step 63110, loss = 2.83 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 03:35:39.351995: step 63120, loss = 2.78 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:35:49.573904: step 63130, loss = 2.61 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:35:59.815390: step 63140, loss = 2.81 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:36:10.137233: step 63150, loss = 2.65 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:36:20.384213: step 63160, loss = 2.69 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:36:30.594821: step 63170, loss = 2.79 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 03:36:40.854113: step 63180, loss = 2.73 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:36:51.068116: step 63190, loss = 2.65 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 03:37:01.607730: step 63200, loss = 2.91 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 03:37:11.906637: step 63210, loss = 2.57 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:37:22.136589: step 63220, loss = 2.73 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 03:37:32.340591: step 63230, loss = 2.68 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 03:37:42.580324: step 63240, loss = 2.58 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:37:52.804385: step 63250, loss = 2.74 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:38:03.090672: step 63260, loss = 2.79 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:38:13.331890: step 63270, loss = 2.53 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:38:23.556586: step 63280, loss = 2.72 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:38:33.787895: step 63290, loss = 2.56 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 03:38:44.319436: step 63300, loss = 2.56 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 03:38:54.499304: step 63310, loss = 2.64 (125.7 examples/sec; 1.018 sec/batch)
2018-04-10 03:39:04.801041: step 63320, loss = 2.59 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:39:15.044271: step 63330, loss = 2.63 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:39:25.252036: step 63340, loss = 2.56 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 03:39:35.485523: step 63350, loss = 2.49 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 03:39:45.731223: step 63360, loss = 2.67 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:39:55.968458: step 63370, loss = 2.44 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:40:06.270959: step 63380, loss = 2.66 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:40:16.551235: step 63390, loss = 2.80 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:40:27.196816: step 63400, loss = 2.45 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 03:40:37.442578: step 63410, loss = 2.40 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:40:47.645562: step 63420, loss = 2.44 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 03:40:57.869709: step 63430, loss = 2.43 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:41:08.155610: step 63440, loss = 2.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:41:18.416756: step 63450, loss = 2.65 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:41:28.657878: step 63460, loss = 2.41 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:41:38.920291: step 63470, loss = 2.32 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:41:49.164947: step 63480, loss = 2.46 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 03:41:59.419862: step 63490, loss = 2.42 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 03:42:09.988342: step 63500, loss = 2.54 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 03:42:20.278907: step 63510, loss = 2.24 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:42:30.619558: step 63520, loss = 2.35 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:42:40.917038: step 63530, loss = 2.39 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:42:51.195232: step 63540, loss = 2.62 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:43:01.475250: step 63550, loss = 2.50 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:43:11.795393: step 63560, loss = 2.20 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:43:22.018943: step 63570, loss = 2.29 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:43:32.232294: step 63580, loss = 2.35 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 03:43:42.441740: step 63590, loss = 2.33 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 03:43:52.934247: step 63600, loss = 2.48 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 03:44:03.170954: step 63610, loss = 2.39 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:44:13.432186: step 63620, loss = 2.20 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:44:23.674415: step 63630, loss = 2.37 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:44:33.899089: step 63640, loss = 2.43 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:44:44.189443: step 63650, loss = 2.34 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:44:54.471763: step 63660, loss = 2.51 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:45:04.799855: step 63670, loss = 2.59 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:45:15.221461: step 63680, loss = 2.43 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 03:45:25.507369: step 63690, loss = 2.21 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:45:36.106728: step 63700, loss = 2.40 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 03:45:46.374685: step 63710, loss = 2.24 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:45:56.663209: step 63720, loss = 2.13 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:46:07.008737: step 63730, loss = 2.26 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:46:17.336677: step 63740, loss = 2.17 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:46:27.632747: step 63750, loss = 2.36 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:46:37.930710: step 63760, loss = 2.32 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:46:48.200722: step 63770, loss = 2.10 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:46:58.512167: step 63780, loss = 2.21 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:47:08.864911: step 63790, loss = 2.20 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 03:47:19.491424: step 63800, loss = 2.13 (120.5 examples/sec; 1.063 sec/batch)
2018-04-10 03:47:29.810972: step 63810, loss = 2.26 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:47:40.143068: step 63820, loss = 2.34 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:47:50.421255: step 63830, loss = 2.12 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:48:00.705756: step 63840, loss = 2.20 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:48:11.113968: step 63850, loss = 2.21 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 03:48:21.461107: step 63860, loss = 2.22 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:48:31.730280: step 63870, loss = 2.22 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:48:42.022559: step 63880, loss = 2.18 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:48:52.294887: step 63890, loss = 2.22 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:49:02.926466: step 63900, loss = 2.15 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 03:49:13.223506: step 63910, loss = 2.25 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:49:23.497587: step 63920, loss = 2.35 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:49:33.737614: step 63930, loss = 2.12 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:49:43.976871: step 63940, loss = 2.17 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:49:54.237884: step 63950, loss = 2.16 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:50:04.594358: step 63960, loss = 2.22 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 03:50:14.874662: step 63970, loss = 2.13 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:50:25.231742: step 63980, loss = 2.26 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 03:50:35.476188: step 63990, loss = 2.10 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 03:50:46.000106: step 64000, loss = 2.04 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 03:50:56.284414: step 64010, loss = 2.31 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:51:06.615957: step 64020, loss = 2.01 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:51:16.891972: step 64030, loss = 2.00 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:51:27.121752: step 64040, loss = 2.17 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 03:51:37.381377: step 64050, loss = 2.23 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:51:47.672047: step 64060, loss = 2.16 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:51:57.946524: step 64070, loss = 2.15 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:52:08.359621: step 64080, loss = 2.05 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 03:52:18.672192: step 64090, loss = 2.01 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:52:29.248311: step 64100, loss = 2.08 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 03:52:39.507747: step 64110, loss = 2.22 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:52:49.813691: step 64120, loss = 1.99 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:53:00.116193: step 64130, loss = 1.99 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 03:53:10.470914: step 64140, loss = 2.24 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 03:53:20.792021: step 64150, loss = 2.13 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:53:31.074389: step 64160, loss = 1.90 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:53:41.324604: step 64170, loss = 2.18 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:53:51.617567: step 64180, loss = 2.15 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:54:01.938135: step 64190, loss = 1.95 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 03:54:12.509214: step 64200, loss = 2.23 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 03:54:22.772567: step 64210, loss = 1.91 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 03:54:33.041713: step 64220, loss = 1.97 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:54:43.287295: step 64230, loss = 2.07 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 03:54:53.552910: step 64240, loss = 2.02 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 03:55:03.842948: step 64250, loss = 1.94 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:55:14.112757: step 64260, loss = 2.17 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 03:55:24.335534: step 64270, loss = 2.04 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 03:55:34.625572: step 64280, loss = 1.85 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:55:44.850583: step 64290, loss = 1.80 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 03:55:55.363626: step 64300, loss = 1.97 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 03:56:05.680237: step 64310, loss = 2.12 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 03:56:15.958765: step 64320, loss = 1.95 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 03:56:26.173585: step 64330, loss = 1.95 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 03:56:36.416824: step 64340, loss = 1.92 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:56:46.677023: step 64350, loss = 1.80 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 03:56:56.953016: step 64360, loss = 2.05 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:57:07.264044: step 64370, loss = 1.97 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 03:57:17.603263: step 64380, loss = 1.87 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 03:57:27.965803: step 64390, loss = 2.00 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 03:57:39.131461: step 64400, loss = 2.18 (114.6 examples/sec; 1.117 sec/batch)
2018-04-10 03:57:49.559037: step 64410, loss = 2.00 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 03:57:59.854824: step 64420, loss = 1.78 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 03:58:10.228664: step 64430, loss = 1.91 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 03:58:20.574433: step 64440, loss = 1.93 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 03:58:30.901321: step 64450, loss = 2.12 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 03:58:41.208334: step 64460, loss = 2.10 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 03:58:51.499725: step 64470, loss = 1.91 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 03:59:01.859131: step 64480, loss = 2.09 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 03:59:12.212687: step 64490, loss = 1.94 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 03:59:22.776266: step 64500, loss = 2.01 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 03:59:33.016365: step 64510, loss = 2.15 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 03:59:43.292849: step 64520, loss = 2.05 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 03:59:53.557671: step 64530, loss = 1.96 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:00:03.868790: step 64540, loss = 1.79 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 04:00:14.191450: step 64550, loss = 1.89 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:00:24.580539: step 64560, loss = 1.73 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 04:00:34.786885: step 64570, loss = 2.01 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 04:00:45.042057: step 64580, loss = 1.92 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:00:55.281041: step 64590, loss = 1.88 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:01:05.888745: step 64600, loss = 1.79 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 04:01:16.141379: step 64610, loss = 1.96 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:01:26.387786: step 64620, loss = 1.75 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:01:36.637926: step 64630, loss = 1.54 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:01:46.891134: step 64640, loss = 1.97 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:01:57.140804: step 64650, loss = 1.96 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:02:07.493215: step 64660, loss = 1.63 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 04:02:17.846155: step 64670, loss = 1.95 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 04:02:28.110146: step 64680, loss = 1.84 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:02:38.371261: step 64690, loss = 1.89 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:02:48.921213: step 64700, loss = 1.90 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 04:02:59.156523: step 64710, loss = 1.96 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 04:03:09.488202: step 64720, loss = 1.64 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:03:19.767011: step 64730, loss = 1.94 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:03:30.020098: step 64740, loss = 1.91 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:03:40.290429: step 64750, loss = 1.93 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:03:50.555049: step 64760, loss = 1.88 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:04:00.794461: step 64770, loss = 1.82 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:04:11.132366: step 64780, loss = 1.95 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:04:21.379672: step 64790, loss = 2.06 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:04:31.902891: step 64800, loss = 1.83 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 04:04:42.132467: step 64810, loss = 1.83 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:04:52.375776: step 64820, loss = 2.10 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:05:02.679353: step 64830, loss = 1.75 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 04:05:13.008734: step 64840, loss = 1.93 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:05:23.264498: step 64850, loss = 1.63 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:05:33.531940: step 64860, loss = 1.78 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:05:43.814936: step 64870, loss = 2.02 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:05:54.131329: step 64880, loss = 1.63 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 04:06:04.509740: step 64890, loss = 1.81 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 04:06:15.149811: step 64900, loss = 1.90 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 04:06:25.422864: step 64910, loss = 1.62 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:06:35.711734: step 64920, loss = 1.91 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:06:45.963178: step 64930, loss = 1.69 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:06:56.255631: step 64940, loss = 1.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:07:06.592818: step 64950, loss = 1.93 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:07:16.950127: step 64960, loss = 1.76 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 04:07:27.309199: step 64970, loss = 1.79 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 04:07:37.568022: step 64980, loss = 1.85 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:07:47.822514: step 64990, loss = 1.88 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:07:58.364210: step 65000, loss = 1.57 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 04:08:08.693304: step 65010, loss = 1.73 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:08:19.054380: step 65020, loss = 1.82 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 04:08:29.347434: step 65030, loss = 1.83 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:08:39.598654: step 65040, loss = 1.90 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:08:49.845320: step 65050, loss = 1.99 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:09:00.093480: step 65060, loss = 1.68 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:09:10.434727: step 65070, loss = 1.70 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:09:20.729227: step 65080, loss = 1.66 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 04:09:31.018765: step 65090, loss = 1.81 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:09:41.662291: step 65100, loss = 1.82 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 04:09:51.865598: step 65110, loss = 1.78 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 04:10:02.154787: step 65120, loss = 1.78 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:10:12.463193: step 65130, loss = 1.74 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 04:10:22.809423: step 65140, loss = 1.77 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 04:10:33.176139: step 65150, loss = 1.86 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 04:10:43.448853: step 65160, loss = 1.83 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:10:53.677118: step 65170, loss = 1.87 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:11:03.995614: step 65180, loss = 2.01 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:11:14.280373: step 65190, loss = 1.61 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:11:24.824005: step 65200, loss = 1.77 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 04:11:35.034013: step 65210, loss = 1.85 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 04:11:45.291744: step 65220, loss = 1.69 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:11:55.522992: step 65230, loss = 1.87 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:12:05.847717: step 65240, loss = 1.69 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:12:16.164602: step 65250, loss = 1.76 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 04:12:26.480215: step 65260, loss = 1.78 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 04:12:36.744010: step 65270, loss = 1.91 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:12:47.032749: step 65280, loss = 1.65 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:12:57.318946: step 65290, loss = 1.84 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:13:07.919732: step 65300, loss = 1.58 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 04:13:18.239487: step 65310, loss = 1.60 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:13:28.521618: step 65320, loss = 1.78 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:13:38.790172: step 65330, loss = 1.61 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:13:49.038038: step 65340, loss = 1.69 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:13:59.328524: step 65350, loss = 1.86 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:14:09.650903: step 65360, loss = 1.75 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:14:19.929326: step 65370, loss = 1.74 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:14:30.181655: step 65380, loss = 1.69 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:14:40.463381: step 65390, loss = 1.81 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:14:51.036388: step 65400, loss = 1.66 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 04:15:01.255864: step 65410, loss = 1.64 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 04:15:11.594359: step 65420, loss = 2.15 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:15:21.890076: step 65430, loss = 1.84 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:15:32.117466: step 65440, loss = 1.68 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 04:15:42.396365: step 65450, loss = 1.75 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:15:52.657346: step 65460, loss = 1.75 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:16:03.129416: step 65470, loss = 1.80 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 04:16:13.458898: step 65480, loss = 1.84 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:16:23.748262: step 65490, loss = 1.67 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:16:34.323571: step 65500, loss = 1.57 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 04:16:44.584286: step 65510, loss = 1.72 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:16:54.827866: step 65520, loss = 1.66 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:17:05.103164: step 65530, loss = 1.51 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:17:15.388907: step 65540, loss = 1.63 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:17:25.631415: step 65550, loss = 1.81 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:17:35.912552: step 65560, loss = 1.65 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:17:46.153144: step 65570, loss = 1.75 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:17:56.429430: step 65580, loss = 1.70 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:18:06.730588: step 65590, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:18:17.317856: step 65600, loss = 1.72 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 04:18:27.545956: step 65610, loss = 1.49 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:18:37.822856: step 65620, loss = 1.48 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:18:48.062557: step 65630, loss = 1.63 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:18:58.324467: step 65640, loss = 1.65 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:19:08.645928: step 65650, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:19:18.929760: step 65660, loss = 1.50 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:19:29.171430: step 65670, loss = 1.61 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:19:39.414241: step 65680, loss = 1.64 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:19:49.652373: step 65690, loss = 1.39 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:20:00.206049: step 65700, loss = 1.76 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 04:20:10.526385: step 65710, loss = 1.76 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:20:20.850745: step 65720, loss = 1.60 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:20:31.176631: step 65730, loss = 1.68 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 04:20:41.434063: step 65740, loss = 1.74 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:20:51.663627: step 65750, loss = 1.71 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:21:01.951724: step 65760, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:21:12.249207: step 65770, loss = 1.67 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:21:22.539849: step 65780, loss = 1.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:21:32.774797: step 65790, loss = 1.68 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:21:43.322600: step 65800, loss = 1.63 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 04:21:53.570685: step 65810, loss = 1.49 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:22:03.877882: step 65820, loss = 1.70 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 04:22:14.182303: step 65830, loss = 1.69 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 04:22:24.486716: step 65840, loss = 1.55 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 04:22:34.808137: step 65850, loss = 1.73 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:22:45.094086: step 65860, loss = 1.73 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:22:55.339113: step 65870, loss = 1.76 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:23:05.665579: step 65880, loss = 1.56 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 04:23:15.952433: step 65890, loss = 1.68 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:23:26.510082: step 65900, loss = 1.75 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 04:23:36.743033: step 65910, loss = 1.76 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:23:47.023812: step 65920, loss = 1.68 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:23:57.247862: step 65930, loss = 1.72 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 04:24:07.535186: step 65940, loss = 1.66 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:24:17.804653: step 65950, loss = 1.67 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:24:28.069136: step 65960, loss = 1.74 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:24:38.330984: step 65970, loss = 1.68 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:24:48.571533: step 65980, loss = 1.74 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:24:58.784336: step 65990, loss = 1.56 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 04:25:09.334631: step 66000, loss = 1.39 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 04:25:19.562943: step 66010, loss = 1.73 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:25:29.861223: step 66020, loss = 1.66 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:25:40.090286: step 66030, loss = 1.69 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:25:50.302710: step 66040, loss = 1.58 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 04:26:00.552775: step 66050, loss = 1.73 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:26:10.840617: step 66060, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:26:21.101476: step 66070, loss = 1.63 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:26:31.382505: step 66080, loss = 1.68 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:26:41.596550: step 66090, loss = 1.57 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 04:26:52.100220: step 66100, loss = 1.55 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 04:27:02.357768: step 66110, loss = 1.70 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:27:12.640769: step 66120, loss = 1.55 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:27:22.852798: step 66130, loss = 1.49 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 04:27:33.104914: step 66140, loss = 1.70 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:27:43.366486: step 66150, loss = 1.55 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:27:53.612980: step 66160, loss = 1.48 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:28:03.882066: step 66170, loss = 1.44 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:28:14.152571: step 66180, loss = 1.57 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:28:24.401881: step 66190, loss = 1.71 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:28:34.953036: step 66200, loss = 1.83 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 04:28:45.161023: step 66210, loss = 1.47 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 04:28:55.386139: step 66220, loss = 1.59 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 04:29:05.676355: step 66230, loss = 1.69 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:29:15.966300: step 66240, loss = 1.54 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:29:26.202669: step 66250, loss = 1.65 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:29:36.455919: step 66260, loss = 1.72 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:29:46.706455: step 66270, loss = 1.72 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:29:56.929064: step 66280, loss = 1.42 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 04:30:07.191363: step 66290, loss = 1.60 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:30:17.766546: step 66300, loss = 1.87 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 04:30:28.080856: step 66310, loss = 1.67 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 04:30:38.364655: step 66320, loss = 1.61 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:30:48.603416: step 66330, loss = 1.58 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:30:58.910324: step 66340, loss = 1.63 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 04:31:09.237457: step 66350, loss = 1.82 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:31:19.548271: step 66360, loss = 1.62 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 04:31:29.839397: step 66370, loss = 1.69 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:31:40.182286: step 66380, loss = 1.49 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:31:50.484083: step 66390, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:32:01.120108: step 66400, loss = 1.62 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 04:32:11.461941: step 66410, loss = 1.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:32:21.790141: step 66420, loss = 1.71 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:32:32.091971: step 66430, loss = 1.55 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 04:32:42.388410: step 66440, loss = 1.67 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:32:52.715806: step 66450, loss = 1.74 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:33:03.017491: step 66460, loss = 1.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:33:13.320878: step 66470, loss = 1.61 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 04:33:23.597189: step 66480, loss = 1.49 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:33:33.868454: step 66490, loss = 1.63 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:33:44.418857: step 66500, loss = 1.70 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 04:33:54.644770: step 66510, loss = 1.63 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 04:34:04.946546: step 66520, loss = 1.73 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:34:15.222141: step 66530, loss = 1.52 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:34:25.491714: step 66540, loss = 1.64 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:34:35.779922: step 66550, loss = 1.61 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:34:46.043688: step 66560, loss = 1.56 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:34:56.283599: step 66570, loss = 1.63 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:35:06.625863: step 66580, loss = 1.48 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:35:16.952662: step 66590, loss = 1.51 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:35:27.494662: step 66600, loss = 1.66 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 04:35:37.806216: step 66610, loss = 1.68 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 04:35:48.346371: step 66620, loss = 1.62 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 04:35:59.233444: step 66630, loss = 1.42 (117.6 examples/sec; 1.089 sec/batch)
2018-04-10 04:36:09.568657: step 66640, loss = 1.63 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:36:19.857055: step 66650, loss = 1.38 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:36:30.125000: step 66660, loss = 1.57 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:36:40.402344: step 66670, loss = 1.71 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:36:50.850432: step 66680, loss = 1.42 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 04:37:01.151477: step 66690, loss = 1.52 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:37:11.807825: step 66700, loss = 1.57 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 04:37:22.088492: step 66710, loss = 1.71 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:37:32.365255: step 66720, loss = 1.52 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:37:42.647045: step 66730, loss = 1.65 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:37:52.986483: step 66740, loss = 1.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:38:03.294955: step 66750, loss = 1.48 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 04:38:13.642264: step 66760, loss = 1.59 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 04:38:23.896854: step 66770, loss = 1.67 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:38:34.159248: step 66780, loss = 1.52 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:38:44.547710: step 66790, loss = 1.47 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 04:38:55.082611: step 66800, loss = 1.82 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 04:39:05.404400: step 66810, loss = 1.30 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:39:15.711538: step 66820, loss = 1.55 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 04:39:25.999147: step 66830, loss = 1.42 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:39:36.266573: step 66840, loss = 1.63 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:39:46.535207: step 66850, loss = 1.36 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:39:56.788962: step 66860, loss = 1.81 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:40:07.076636: step 66870, loss = 1.51 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:40:17.396754: step 66880, loss = 1.79 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:40:27.754932: step 66890, loss = 1.57 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 04:40:38.402613: step 66900, loss = 1.60 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 04:40:48.752521: step 66910, loss = 1.63 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 04:40:59.028993: step 66920, loss = 1.72 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:41:09.438054: step 66930, loss = 1.51 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 04:41:19.759105: step 66940, loss = 1.44 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:41:30.020185: step 66950, loss = 1.68 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:41:40.369240: step 66960, loss = 1.57 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 04:41:50.650489: step 66970, loss = 1.51 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:42:00.969485: step 66980, loss = 1.49 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:42:11.321782: step 66990, loss = 1.51 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 04:42:21.906524: step 67000, loss = 1.49 (120.9 examples/sec; 1.058 sec/batch)
2018-04-10 04:42:32.146042: step 67010, loss = 1.46 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:42:42.474240: step 67020, loss = 1.65 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:42:52.774879: step 67030, loss = 1.65 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:43:03.106494: step 67040, loss = 1.58 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:43:13.417405: step 67050, loss = 1.80 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 04:43:23.708430: step 67060, loss = 1.38 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:43:33.953759: step 67070, loss = 1.64 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:43:44.217146: step 67080, loss = 1.74 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:43:54.444820: step 67090, loss = 1.44 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 04:44:05.019766: step 67100, loss = 1.58 (121.0 examples/sec; 1.057 sec/batch)
2018-04-10 04:44:15.296760: step 67110, loss = 1.56 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:44:25.561560: step 67120, loss = 1.34 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:44:35.811930: step 67130, loss = 1.46 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:44:46.086545: step 67140, loss = 1.73 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:44:56.338059: step 67150, loss = 1.46 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:45:06.833934: step 67160, loss = 1.48 (122.0 examples/sec; 1.050 sec/batch)
2018-04-10 04:45:17.165716: step 67170, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 04:45:27.442310: step 67180, loss = 1.34 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:45:37.721847: step 67190, loss = 1.33 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:45:48.280246: step 67200, loss = 1.58 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 04:45:58.522977: step 67210, loss = 1.64 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:46:08.840252: step 67220, loss = 1.36 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 04:46:19.160870: step 67230, loss = 1.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:46:29.421983: step 67240, loss = 1.67 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:46:39.714739: step 67250, loss = 1.28 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:46:49.980303: step 67260, loss = 1.63 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:47:00.278284: step 67270, loss = 1.60 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:47:10.631502: step 67280, loss = 1.59 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 04:47:20.914022: step 67290, loss = 1.47 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:47:31.528841: step 67300, loss = 1.50 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 04:47:41.818181: step 67310, loss = 1.43 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:47:52.113857: step 67320, loss = 1.53 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:48:02.453731: step 67330, loss = 1.37 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:48:12.758652: step 67340, loss = 1.57 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 04:48:23.053812: step 67350, loss = 1.68 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:48:33.318209: step 67360, loss = 1.71 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:48:43.577929: step 67370, loss = 1.65 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:48:53.841243: step 67380, loss = 1.70 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:49:04.142832: step 67390, loss = 1.43 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:49:14.724174: step 67400, loss = 1.63 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 04:49:24.945247: step 67410, loss = 1.48 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 04:49:35.204526: step 67420, loss = 1.69 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:49:45.457459: step 67430, loss = 1.60 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:49:55.778918: step 67440, loss = 1.49 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:50:06.124075: step 67450, loss = 1.69 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 04:50:16.436029: step 67460, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 04:50:26.952287: step 67470, loss = 1.53 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 04:50:37.299771: step 67480, loss = 1.54 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 04:50:47.687869: step 67490, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 04:50:58.380235: step 67500, loss = 1.36 (119.7 examples/sec; 1.069 sec/batch)
2018-04-10 04:51:08.767668: step 67510, loss = 1.66 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 04:51:19.063116: step 67520, loss = 1.50 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:51:29.344949: step 67530, loss = 1.33 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:51:39.630480: step 67540, loss = 1.49 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:51:49.878611: step 67550, loss = 1.56 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:52:00.151547: step 67560, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:52:10.475345: step 67570, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:52:20.768497: step 67580, loss = 1.58 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:52:31.013427: step 67590, loss = 1.60 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 04:52:41.570068: step 67600, loss = 1.58 (121.3 examples/sec; 1.056 sec/batch)
2018-04-10 04:52:51.790036: step 67610, loss = 1.44 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 04:53:02.115770: step 67620, loss = 1.55 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 04:53:12.422507: step 67630, loss = 1.59 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 04:53:22.677036: step 67640, loss = 1.43 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:53:32.923763: step 67650, loss = 1.37 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:53:43.203702: step 67660, loss = 1.49 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 04:53:53.460680: step 67670, loss = 1.60 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 04:54:03.815682: step 67680, loss = 1.51 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 04:54:14.089754: step 67690, loss = 1.39 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:54:24.617839: step 67700, loss = 1.46 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 04:54:34.851355: step 67710, loss = 1.67 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:54:45.112507: step 67720, loss = 1.44 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:54:55.346671: step 67730, loss = 1.46 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 04:55:05.633485: step 67740, loss = 1.59 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:55:15.925531: step 67750, loss = 1.33 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 04:55:26.178150: step 67760, loss = 1.59 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 04:55:36.418882: step 67770, loss = 1.71 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:55:46.690612: step 67780, loss = 1.53 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:55:56.929327: step 67790, loss = 1.42 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:56:07.544383: step 67800, loss = 1.37 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 04:56:17.818707: step 67810, loss = 1.53 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:56:28.095397: step 67820, loss = 1.52 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:56:38.340127: step 67830, loss = 1.54 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 04:56:48.591982: step 67840, loss = 1.34 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 04:56:58.853861: step 67850, loss = 1.49 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 04:57:09.292071: step 67860, loss = 1.46 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 04:57:19.613762: step 67870, loss = 1.48 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:57:29.912835: step 67880, loss = 1.41 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:57:40.185726: step 67890, loss = 1.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 04:57:50.735164: step 67900, loss = 1.65 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 04:58:00.972277: step 67910, loss = 1.57 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 04:58:11.342559: step 67920, loss = 1.68 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 04:58:21.617966: step 67930, loss = 1.38 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 04:58:31.942665: step 67940, loss = 1.35 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 04:58:42.208084: step 67950, loss = 1.47 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 04:58:52.545686: step 67960, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 04:59:02.945222: step 67970, loss = 1.44 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 04:59:13.300414: step 67980, loss = 1.55 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 04:59:23.599190: step 67990, loss = 1.53 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 04:59:34.112454: step 68000, loss = 1.35 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 04:59:44.322109: step 68010, loss = 1.72 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 04:59:54.547455: step 68020, loss = 1.75 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:00:04.863048: step 68030, loss = 1.38 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 05:00:15.243686: step 68040, loss = 1.64 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 05:00:25.559747: step 68050, loss = 1.59 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 05:00:35.917985: step 68060, loss = 1.29 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 05:00:46.146050: step 68070, loss = 1.56 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:00:56.394170: step 68080, loss = 1.59 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:01:06.726978: step 68090, loss = 1.57 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:01:17.302835: step 68100, loss = 1.47 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 05:01:27.525332: step 68110, loss = 1.64 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:01:37.815342: step 68120, loss = 1.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:01:48.027596: step 68130, loss = 1.56 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 05:01:58.250133: step 68140, loss = 1.63 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:02:08.545205: step 68150, loss = 1.63 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:02:18.837197: step 68160, loss = 1.56 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:02:29.067728: step 68170, loss = 1.78 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:02:39.355771: step 68180, loss = 1.39 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:02:49.599902: step 68190, loss = 1.28 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 05:03:00.123359: step 68200, loss = 1.60 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 05:03:10.448798: step 68210, loss = 1.67 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 05:03:20.781547: step 68220, loss = 1.48 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:03:31.020418: step 68230, loss = 1.54 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:03:41.273629: step 68240, loss = 1.49 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:03:51.578736: step 68250, loss = 1.64 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 05:04:01.868346: step 68260, loss = 1.60 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:04:12.176218: step 68270, loss = 1.50 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 05:04:22.443159: step 68280, loss = 1.58 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:04:32.693813: step 68290, loss = 1.46 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:04:43.243068: step 68300, loss = 1.65 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 05:04:53.485326: step 68310, loss = 1.40 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:05:03.808375: step 68320, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:05:14.099493: step 68330, loss = 1.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:05:24.352059: step 68340, loss = 1.53 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:05:34.634438: step 68350, loss = 1.59 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:05:44.923634: step 68360, loss = 1.60 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:05:55.168960: step 68370, loss = 1.55 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:06:05.514836: step 68380, loss = 1.47 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 05:06:15.816408: step 68390, loss = 1.63 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:06:26.353276: step 68400, loss = 1.47 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 05:06:36.601254: step 68410, loss = 1.37 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:06:46.857584: step 68420, loss = 1.62 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:06:57.088338: step 68430, loss = 1.55 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:07:07.429078: step 68440, loss = 1.39 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:07:17.799552: step 68450, loss = 1.64 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 05:07:28.122226: step 68460, loss = 1.56 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:07:38.432528: step 68470, loss = 1.54 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 05:07:48.791183: step 68480, loss = 1.62 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 05:07:59.151526: step 68490, loss = 1.53 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 05:08:09.801682: step 68500, loss = 1.56 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 05:08:20.124847: step 68510, loss = 1.79 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:08:30.405035: step 68520, loss = 1.44 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:08:40.703265: step 68530, loss = 1.49 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:08:51.039132: step 68540, loss = 1.49 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:09:01.310541: step 68550, loss = 1.54 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:09:11.721452: step 68560, loss = 1.43 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 05:09:22.011616: step 68570, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:09:32.261095: step 68580, loss = 1.56 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:09:42.561567: step 68590, loss = 1.24 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:09:53.121460: step 68600, loss = 1.71 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 05:10:03.435492: step 68610, loss = 1.67 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 05:10:13.782675: step 68620, loss = 1.58 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 05:10:24.078859: step 68630, loss = 1.45 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:10:34.532744: step 68640, loss = 1.58 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 05:10:44.876672: step 68650, loss = 1.54 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 05:10:55.188285: step 68660, loss = 1.47 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 05:11:05.528413: step 68670, loss = 1.49 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:11:15.855379: step 68680, loss = 1.45 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:11:26.122874: step 68690, loss = 1.48 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:11:36.661986: step 68700, loss = 1.45 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 05:11:46.881581: step 68710, loss = 1.40 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:11:57.108512: step 68720, loss = 1.45 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:12:07.383898: step 68730, loss = 1.59 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 05:12:17.684128: step 68740, loss = 1.52 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:12:27.914509: step 68750, loss = 1.46 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:12:38.144396: step 68760, loss = 1.71 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:12:48.368684: step 68770, loss = 1.44 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:12:58.602337: step 68780, loss = 1.45 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:13:08.927167: step 68790, loss = 1.68 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:13:19.601990: step 68800, loss = 1.47 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 05:13:29.914729: step 68810, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 05:13:40.169258: step 68820, loss = 1.53 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:13:50.403733: step 68830, loss = 1.64 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:14:00.638615: step 68840, loss = 1.39 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:14:10.946966: step 68850, loss = 1.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 05:14:21.222400: step 68860, loss = 1.68 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 05:14:31.476275: step 68870, loss = 1.43 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:14:41.742747: step 68880, loss = 1.56 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:14:51.949581: step 68890, loss = 1.73 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 05:15:02.579056: step 68900, loss = 1.50 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 05:15:12.937573: step 68910, loss = 1.61 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 05:15:23.168464: step 68920, loss = 1.52 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:15:33.355559: step 68930, loss = 1.65 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 05:15:43.591094: step 68940, loss = 1.32 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 05:15:53.793677: step 68950, loss = 1.44 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 05:16:04.122084: step 68960, loss = 1.56 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:16:14.396294: step 68970, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:16:24.659903: step 68980, loss = 1.58 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:16:34.914658: step 68990, loss = 1.56 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:16:45.472008: step 69000, loss = 1.53 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 05:16:55.672572: step 69010, loss = 1.51 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 05:17:05.968771: step 69020, loss = 1.36 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:17:16.209990: step 69030, loss = 1.29 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:17:26.428342: step 69040, loss = 1.47 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 05:17:36.658713: step 69050, loss = 1.70 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:17:46.907741: step 69060, loss = 1.43 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:17:57.123888: step 69070, loss = 1.45 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 05:18:07.394132: step 69080, loss = 1.39 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:18:17.634508: step 69090, loss = 1.51 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:18:28.153671: step 69100, loss = 1.41 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 05:18:38.357910: step 69110, loss = 1.56 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 05:18:48.559904: step 69120, loss = 1.57 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 05:18:58.741190: step 69130, loss = 1.70 (125.7 examples/sec; 1.018 sec/batch)
2018-04-10 05:19:09.005079: step 69140, loss = 1.55 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:19:19.279480: step 69150, loss = 1.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:19:29.544178: step 69160, loss = 1.53 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:19:39.849716: step 69170, loss = 1.65 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 05:19:50.091595: step 69180, loss = 1.44 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:20:00.347424: step 69190, loss = 1.40 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:20:10.929115: step 69200, loss = 1.64 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 05:20:21.223013: step 69210, loss = 1.57 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 05:20:31.698415: step 69220, loss = 1.41 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 05:20:41.948858: step 69230, loss = 1.50 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:20:52.187557: step 69240, loss = 1.48 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:21:02.485525: step 69250, loss = 1.38 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:21:12.779807: step 69260, loss = 1.58 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 05:21:23.060336: step 69270, loss = 1.52 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:21:33.339073: step 69280, loss = 1.41 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:21:43.620458: step 69290, loss = 1.60 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:21:54.160667: step 69300, loss = 1.77 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 05:22:04.451961: step 69310, loss = 1.43 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:22:14.794347: step 69320, loss = 1.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:22:25.104872: step 69330, loss = 1.41 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 05:22:35.357183: step 69340, loss = 1.44 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:22:45.659054: step 69350, loss = 1.42 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 05:22:55.950996: step 69360, loss = 1.63 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:23:06.279132: step 69370, loss = 1.70 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:23:16.646321: step 69380, loss = 1.52 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 05:23:26.925167: step 69390, loss = 1.83 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:23:37.533715: step 69400, loss = 1.30 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 05:23:47.793444: step 69410, loss = 1.51 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:23:58.072129: step 69420, loss = 1.64 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:24:08.427822: step 69430, loss = 1.54 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 05:24:18.727291: step 69440, loss = 1.27 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:24:28.999930: step 69450, loss = 1.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:24:39.320647: step 69460, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:24:49.562623: step 69470, loss = 1.54 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:24:59.792820: step 69480, loss = 1.54 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:25:10.090322: step 69490, loss = 1.65 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:25:20.659437: step 69500, loss = 1.29 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 05:25:30.883356: step 69510, loss = 1.57 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:25:41.157601: step 69520, loss = 1.35 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:25:51.412949: step 69530, loss = 1.48 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:26:01.742903: step 69540, loss = 1.54 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:26:12.083416: step 69550, loss = 1.68 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:26:22.424441: step 69560, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:26:32.860117: step 69570, loss = 1.38 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 05:26:43.247360: step 69580, loss = 1.44 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 05:26:53.649414: step 69590, loss = 1.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 05:27:04.338758: step 69600, loss = 1.49 (119.7 examples/sec; 1.069 sec/batch)
2018-04-10 05:27:14.752695: step 69610, loss = 1.74 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 05:27:25.140719: step 69620, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 05:27:35.501274: step 69630, loss = 1.38 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 05:27:45.849807: step 69640, loss = 1.54 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 05:27:56.217013: step 69650, loss = 1.43 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 05:28:06.666727: step 69660, loss = 1.49 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 05:28:17.028580: step 69670, loss = 1.63 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 05:28:27.379484: step 69680, loss = 1.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 05:28:37.736793: step 69690, loss = 1.60 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 05:28:48.359115: step 69700, loss = 1.44 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 05:28:58.733048: step 69710, loss = 1.47 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 05:29:09.186325: step 69720, loss = 1.59 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 05:29:19.599537: step 69730, loss = 1.63 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 05:29:30.012420: step 69740, loss = 1.53 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 05:29:40.254524: step 69750, loss = 1.62 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:29:50.511157: step 69760, loss = 1.82 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:30:00.774054: step 69770, loss = 1.75 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:30:11.098323: step 69780, loss = 1.51 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:30:21.381033: step 69790, loss = 1.55 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:30:32.018333: step 69800, loss = 1.46 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 05:30:42.254169: step 69810, loss = 1.49 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 05:30:52.488346: step 69820, loss = 1.52 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:31:02.741478: step 69830, loss = 1.41 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:31:13.008890: step 69840, loss = 1.36 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:31:23.243583: step 69850, loss = 1.41 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:31:33.484321: step 69860, loss = 1.66 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:31:43.709394: step 69870, loss = 1.23 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:31:53.937492: step 69880, loss = 1.43 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:32:04.221952: step 69890, loss = 1.58 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:32:14.784191: step 69900, loss = 1.28 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 05:32:25.036413: step 69910, loss = 1.70 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:32:35.308171: step 69920, loss = 1.66 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:32:45.577881: step 69930, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:32:55.862637: step 69940, loss = 1.39 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:33:06.122971: step 69950, loss = 1.53 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:33:16.448493: step 69960, loss = 1.51 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 05:33:26.702802: step 69970, loss = 1.50 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 05:33:37.030243: step 69980, loss = 1.62 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:33:47.339695: step 69990, loss = 1.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 05:33:57.858869: step 70000, loss = 1.30 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 05:34:08.183536: step 70010, loss = 1.41 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:34:18.517995: step 70020, loss = 1.63 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:34:28.810790: step 70030, loss = 1.51 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:34:39.091077: step 70040, loss = 1.79 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:34:49.330433: step 70050, loss = 1.58 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:34:59.599847: step 70060, loss = 1.47 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:35:09.870345: step 70070, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:35:20.145917: step 70080, loss = 1.67 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 05:35:30.350915: step 70090, loss = 1.54 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 05:35:40.885906: step 70100, loss = 1.66 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 05:35:51.099989: step 70110, loss = 1.51 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 05:36:01.330795: step 70120, loss = 1.49 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:36:11.613964: step 70130, loss = 1.46 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:36:21.871513: step 70140, loss = 1.66 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:36:32.130474: step 70150, loss = 1.51 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:36:42.447051: step 70160, loss = 1.54 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 05:36:52.663401: step 70170, loss = 1.64 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 05:37:02.944295: step 70180, loss = 1.61 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:37:13.206764: step 70190, loss = 1.41 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:37:23.776913: step 70200, loss = 1.58 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 05:37:33.999370: step 70210, loss = 1.63 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:37:44.271295: step 70220, loss = 1.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:37:54.543148: step 70230, loss = 1.60 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:38:04.843582: step 70240, loss = 1.50 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:38:15.126255: step 70250, loss = 1.64 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:38:25.400247: step 70260, loss = 1.68 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:38:35.665104: step 70270, loss = 1.42 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:38:45.948634: step 70280, loss = 1.33 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:38:56.176007: step 70290, loss = 1.50 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:39:06.764946: step 70300, loss = 1.55 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 05:39:17.032948: step 70310, loss = 1.36 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:39:27.281949: step 70320, loss = 1.52 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:39:37.560147: step 70330, loss = 1.63 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:39:47.846028: step 70340, loss = 1.42 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:39:58.114710: step 70350, loss = 1.47 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:40:08.482460: step 70360, loss = 1.43 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 05:40:18.778975: step 70370, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:40:29.175255: step 70380, loss = 1.49 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 05:40:39.430320: step 70390, loss = 1.50 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:40:49.993128: step 70400, loss = 1.55 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 05:41:00.261404: step 70410, loss = 1.47 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:41:10.591917: step 70420, loss = 1.66 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:41:20.842189: step 70430, loss = 1.38 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:41:31.053188: step 70440, loss = 1.36 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 05:41:41.271781: step 70450, loss = 1.71 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 05:41:51.485306: step 70460, loss = 1.61 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 05:42:01.705325: step 70470, loss = 1.36 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:42:11.961188: step 70480, loss = 1.50 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:42:22.191589: step 70490, loss = 1.36 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:42:32.699955: step 70500, loss = 1.48 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 05:42:42.946446: step 70510, loss = 1.59 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:42:53.208722: step 70520, loss = 1.38 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:43:03.497955: step 70530, loss = 1.30 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:43:13.769936: step 70540, loss = 1.31 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:43:23.995019: step 70550, loss = 1.41 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:43:34.221827: step 70560, loss = 1.49 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:43:44.484644: step 70570, loss = 1.49 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:43:54.754190: step 70580, loss = 1.82 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:44:05.055748: step 70590, loss = 1.53 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:44:15.590188: step 70600, loss = 1.74 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 05:44:25.789970: step 70610, loss = 1.74 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 05:44:35.998985: step 70620, loss = 1.50 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 05:44:46.248696: step 70630, loss = 1.33 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:44:56.462272: step 70640, loss = 1.56 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 05:45:06.724148: step 70650, loss = 1.41 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:45:17.011773: step 70660, loss = 1.53 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:45:27.224213: step 70670, loss = 1.55 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 05:45:37.458805: step 70680, loss = 1.48 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:45:47.701257: step 70690, loss = 1.50 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:45:58.198186: step 70700, loss = 1.61 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 05:46:08.464077: step 70710, loss = 1.35 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:46:18.724616: step 70720, loss = 1.34 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:46:28.930045: step 70730, loss = 1.65 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 05:46:39.130383: step 70740, loss = 1.34 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 05:46:49.355739: step 70750, loss = 1.43 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:46:59.591918: step 70760, loss = 1.38 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:47:09.850950: step 70770, loss = 1.47 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:47:20.116444: step 70780, loss = 1.48 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:47:30.470305: step 70790, loss = 1.53 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 05:47:40.985279: step 70800, loss = 1.59 (121.7 examples/sec; 1.051 sec/batch)
2018-04-10 05:47:51.190619: step 70810, loss = 1.71 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 05:48:01.449773: step 70820, loss = 1.47 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:48:11.729511: step 70830, loss = 1.65 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:48:21.973547: step 70840, loss = 1.57 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:48:32.174429: step 70850, loss = 1.54 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 05:48:42.406748: step 70860, loss = 1.65 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:48:52.631069: step 70870, loss = 1.41 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:49:02.896961: step 70880, loss = 1.63 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:49:13.143532: step 70890, loss = 1.59 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:49:23.656803: step 70900, loss = 1.43 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 05:49:33.878147: step 70910, loss = 1.35 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 05:49:44.104902: step 70920, loss = 1.56 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 05:49:54.337749: step 70930, loss = 1.74 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:50:04.699104: step 70940, loss = 1.55 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 05:50:15.028755: step 70950, loss = 1.54 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:50:25.313273: step 70960, loss = 1.52 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:50:35.683243: step 70970, loss = 1.62 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 05:50:45.982571: step 70980, loss = 1.56 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:50:56.264112: step 70990, loss = 1.60 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:51:06.930235: step 71000, loss = 1.37 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 05:51:17.254466: step 71010, loss = 1.64 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:51:27.536274: step 71020, loss = 1.67 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:51:37.855073: step 71030, loss = 1.46 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:51:48.155646: step 71040, loss = 1.35 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 05:51:58.419268: step 71050, loss = 1.56 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 05:52:08.759215: step 71060, loss = 1.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 05:52:19.053624: step 71070, loss = 1.37 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 05:52:29.322611: step 71080, loss = 1.32 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:52:39.616826: step 71090, loss = 1.51 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 05:52:50.164749: step 71100, loss = 1.42 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 05:53:00.481493: step 71110, loss = 1.44 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 05:53:10.829958: step 71120, loss = 1.54 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 05:53:21.141460: step 71130, loss = 1.64 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 05:53:31.504994: step 71140, loss = 1.59 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 05:53:41.779978: step 71150, loss = 1.54 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:53:52.047179: step 71160, loss = 1.36 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:54:02.425602: step 71170, loss = 1.58 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 05:54:12.720289: step 71180, loss = 1.50 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 05:54:22.978526: step 71190, loss = 1.47 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:54:33.508138: step 71200, loss = 1.60 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 05:54:43.763902: step 71210, loss = 1.48 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:54:54.019871: step 71220, loss = 1.48 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 05:55:04.306666: step 71230, loss = 1.60 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 05:55:14.625685: step 71240, loss = 1.45 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 05:55:24.906876: step 71250, loss = 1.60 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 05:55:35.180429: step 71260, loss = 1.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:55:45.498603: step 71270, loss = 1.61 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 05:55:55.765414: step 71280, loss = 1.56 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:56:06.118076: step 71290, loss = 1.52 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 05:56:16.682212: step 71300, loss = 1.33 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 05:56:26.934034: step 71310, loss = 1.38 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:56:37.200176: step 71320, loss = 1.78 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 05:56:47.450515: step 71330, loss = 1.65 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:56:57.724034: step 71340, loss = 1.33 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:57:08.057371: step 71350, loss = 1.55 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 05:57:18.361333: step 71360, loss = 1.46 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 05:57:28.593651: step 71370, loss = 1.50 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:57:38.837893: step 71380, loss = 1.38 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 05:57:49.142083: step 71390, loss = 1.49 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 05:57:59.698949: step 71400, loss = 1.67 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 05:58:17.541989: step 71410, loss = 1.49 (71.7 examples/sec; 1.784 sec/batch)
2018-04-10 05:58:27.785113: step 71420, loss = 1.63 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:58:38.014706: step 71430, loss = 1.50 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 05:58:48.249845: step 71440, loss = 1.53 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 05:58:58.491587: step 71450, loss = 1.68 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:59:08.859391: step 71460, loss = 1.46 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 05:59:19.133259: step 71470, loss = 1.30 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 05:59:29.384492: step 71480, loss = 1.63 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 05:59:39.627804: step 71490, loss = 1.53 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 05:59:50.166837: step 71500, loss = 1.49 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 06:00:00.425767: step 71510, loss = 1.66 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:00:10.740226: step 71520, loss = 1.43 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 06:00:20.999345: step 71530, loss = 1.51 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:00:31.379705: step 71540, loss = 1.62 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 06:00:41.627764: step 71550, loss = 1.57 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:00:51.877034: step 71560, loss = 1.58 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:01:02.110396: step 71570, loss = 1.43 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:01:12.368980: step 71580, loss = 1.58 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:01:22.571045: step 71590, loss = 1.51 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 06:01:33.070131: step 71600, loss = 1.58 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 06:01:43.290343: step 71610, loss = 1.69 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:01:53.486053: step 71620, loss = 1.55 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 06:02:03.752281: step 71630, loss = 1.52 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:02:14.009642: step 71640, loss = 1.39 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:02:24.234068: step 71650, loss = 1.30 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:02:34.453882: step 71660, loss = 1.22 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:02:44.682904: step 71670, loss = 1.39 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:02:54.897285: step 71680, loss = 1.46 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 06:03:05.148585: step 71690, loss = 1.40 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:03:15.706212: step 71700, loss = 1.72 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 06:03:25.940744: step 71710, loss = 1.49 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:03:36.186085: step 71720, loss = 1.36 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:03:46.426535: step 71730, loss = 1.49 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:03:56.679981: step 71740, loss = 1.53 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 06:04:06.969196: step 71750, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:04:17.312989: step 71760, loss = 1.40 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 06:04:27.543521: step 71770, loss = 1.42 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:04:37.807842: step 71780, loss = 1.42 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:04:48.057555: step 71790, loss = 1.57 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:04:58.587821: step 71800, loss = 1.45 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 06:05:08.881212: step 71810, loss = 1.55 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:05:19.167537: step 71820, loss = 1.44 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:05:29.402374: step 71830, loss = 1.58 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:05:39.660106: step 71840, loss = 1.25 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:05:49.924146: step 71850, loss = 1.66 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:06:00.190138: step 71860, loss = 1.54 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:06:10.530643: step 71870, loss = 1.60 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:06:20.803650: step 71880, loss = 1.47 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:06:31.051952: step 71890, loss = 1.60 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:06:41.766979: step 71900, loss = 1.38 (119.5 examples/sec; 1.072 sec/batch)
2018-04-10 06:06:52.066813: step 71910, loss = 1.38 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:07:02.335533: step 71920, loss = 1.53 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:07:12.664193: step 71930, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:07:22.905557: step 71940, loss = 1.44 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:07:33.173622: step 71950, loss = 1.64 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:07:43.530387: step 71960, loss = 1.62 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 06:07:53.769188: step 71970, loss = 1.33 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:08:04.068267: step 71980, loss = 1.68 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:08:14.397071: step 71990, loss = 1.36 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:08:24.945093: step 72000, loss = 1.54 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 06:08:35.219896: step 72010, loss = 1.37 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:08:45.456603: step 72020, loss = 1.47 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:08:55.734181: step 72030, loss = 1.39 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:09:06.085673: step 72040, loss = 1.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:09:16.412189: step 72050, loss = 1.53 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 06:09:26.667000: step 72060, loss = 1.61 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 06:09:36.869261: step 72070, loss = 1.36 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 06:09:47.147616: step 72080, loss = 1.63 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:09:57.358798: step 72090, loss = 1.39 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:10:07.902554: step 72100, loss = 1.59 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 06:10:18.162428: step 72110, loss = 1.53 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:10:28.413144: step 72120, loss = 1.34 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:10:38.754349: step 72130, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:10:48.996942: step 72140, loss = 1.43 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:10:59.277559: step 72150, loss = 1.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:11:09.595179: step 72160, loss = 1.59 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 06:11:19.849702: step 72170, loss = 1.69 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 06:11:30.052914: step 72180, loss = 1.55 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 06:11:40.274529: step 72190, loss = 1.49 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:11:50.761188: step 72200, loss = 1.58 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 06:12:00.966057: step 72210, loss = 1.48 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 06:12:11.244595: step 72220, loss = 1.10 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:12:21.514916: step 72230, loss = 1.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:12:31.743056: step 72240, loss = 1.51 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:12:41.971877: step 72250, loss = 1.50 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:12:52.196640: step 72260, loss = 1.41 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:13:02.498422: step 72270, loss = 1.47 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:13:12.745909: step 72280, loss = 1.31 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:13:22.950769: step 72290, loss = 1.60 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 06:13:33.415505: step 72300, loss = 1.59 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 06:13:43.670696: step 72310, loss = 1.43 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:13:53.860993: step 72320, loss = 1.48 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:14:04.085656: step 72330, loss = 1.63 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:14:14.319451: step 72340, loss = 1.50 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:14:24.614311: step 72350, loss = 1.52 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 06:14:34.823325: step 72360, loss = 1.60 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:14:44.998625: step 72370, loss = 1.39 (125.8 examples/sec; 1.018 sec/batch)
2018-04-10 06:14:55.197133: step 72380, loss = 1.51 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 06:15:05.463557: step 72390, loss = 1.69 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:15:16.026787: step 72400, loss = 1.68 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 06:15:26.238668: step 72410, loss = 1.72 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 06:15:36.430624: step 72420, loss = 1.56 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:15:46.773954: step 72430, loss = 1.42 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:15:56.983135: step 72440, loss = 1.49 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:16:07.231011: step 72450, loss = 1.47 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:16:17.467806: step 72460, loss = 1.70 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:16:27.723278: step 72470, loss = 1.25 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:16:37.954653: step 72480, loss = 1.65 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:16:48.142298: step 72490, loss = 1.54 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:16:58.615141: step 72500, loss = 1.51 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 06:17:08.844697: step 72510, loss = 1.73 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:17:19.052614: step 72520, loss = 1.37 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:17:29.226398: step 72530, loss = 1.37 (125.8 examples/sec; 1.017 sec/batch)
2018-04-10 06:17:39.401882: step 72540, loss = 1.68 (125.8 examples/sec; 1.018 sec/batch)
2018-04-10 06:17:49.625043: step 72550, loss = 1.59 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:17:59.834232: step 72560, loss = 1.23 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:18:10.077948: step 72570, loss = 1.41 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:18:20.340836: step 72580, loss = 1.48 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:18:30.528068: step 72590, loss = 1.50 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:18:40.973270: step 72600, loss = 1.33 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 06:18:51.137614: step 72610, loss = 1.60 (125.9 examples/sec; 1.016 sec/batch)
2018-04-10 06:19:01.324775: step 72620, loss = 1.41 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:19:11.553107: step 72630, loss = 1.52 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:19:21.824364: step 72640, loss = 1.54 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:19:32.018793: step 72650, loss = 1.64 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:19:42.234272: step 72660, loss = 1.47 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 06:19:52.421458: step 72670, loss = 1.54 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:20:02.635519: step 72680, loss = 1.39 (125.3 examples/sec; 1.021 sec/batch)
2018-04-10 06:20:12.861227: step 72690, loss = 1.47 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 06:20:23.326603: step 72700, loss = 1.57 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 06:20:33.603548: step 72710, loss = 1.36 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 06:20:43.839070: step 72720, loss = 1.47 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 06:20:54.073944: step 72730, loss = 1.52 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:21:04.387442: step 72740, loss = 1.36 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 06:21:14.667325: step 72750, loss = 1.38 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:21:24.896259: step 72760, loss = 1.70 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:21:35.090659: step 72770, loss = 1.38 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:21:45.291554: step 72780, loss = 1.26 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 06:21:55.495492: step 72790, loss = 1.77 (125.4 examples/sec; 1.020 sec/batch)
2018-04-10 06:22:06.049155: step 72800, loss = 1.49 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 06:22:16.281744: step 72810, loss = 1.45 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:22:26.476421: step 72820, loss = 1.43 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:22:36.649121: step 72830, loss = 1.63 (125.8 examples/sec; 1.017 sec/batch)
2018-04-10 06:22:46.835228: step 72840, loss = 1.46 (125.7 examples/sec; 1.019 sec/batch)
2018-04-10 06:22:57.028147: step 72850, loss = 1.49 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:23:07.319877: step 72860, loss = 1.59 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:23:17.549511: step 72870, loss = 1.66 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:23:27.770226: step 72880, loss = 1.69 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:23:37.989873: step 72890, loss = 1.66 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:23:48.484142: step 72900, loss = 1.61 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 06:23:58.689209: step 72910, loss = 1.42 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:24:08.936121: step 72920, loss = 1.78 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:24:19.202468: step 72930, loss = 1.39 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:24:29.432263: step 72940, loss = 1.53 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:24:39.621214: step 72950, loss = 1.57 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 06:24:49.827285: step 72960, loss = 1.64 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:24:59.999190: step 72970, loss = 1.42 (125.8 examples/sec; 1.017 sec/batch)
2018-04-10 06:25:10.453032: step 72980, loss = 1.61 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 06:25:20.686156: step 72990, loss = 1.68 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:25:31.203040: step 73000, loss = 1.38 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 06:25:41.445703: step 73010, loss = 1.75 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:25:51.711778: step 73020, loss = 1.52 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:26:02.002206: step 73030, loss = 1.53 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:26:12.284384: step 73040, loss = 1.59 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:26:22.562569: step 73050, loss = 1.48 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:26:32.840758: step 73060, loss = 1.74 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:26:43.311041: step 73070, loss = 1.53 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 06:26:53.740892: step 73080, loss = 1.58 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 06:27:04.079199: step 73090, loss = 1.58 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:27:14.643771: step 73100, loss = 1.54 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 06:27:24.892482: step 73110, loss = 1.46 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:27:35.147507: step 73120, loss = 1.44 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:27:45.394209: step 73130, loss = 1.37 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:27:55.640614: step 73140, loss = 1.66 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:28:05.891656: step 73150, loss = 1.36 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:28:16.219293: step 73160, loss = 1.32 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:28:26.487108: step 73170, loss = 1.36 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:28:36.756989: step 73180, loss = 1.72 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:28:47.001661: step 73190, loss = 1.46 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 06:28:57.528286: step 73200, loss = 1.59 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 06:29:07.818007: step 73210, loss = 1.53 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:29:18.114142: step 73220, loss = 1.44 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:29:28.386092: step 73230, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:29:38.673151: step 73240, loss = 1.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:29:48.942859: step 73250, loss = 1.49 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:29:59.195059: step 73260, loss = 1.40 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:30:09.523079: step 73270, loss = 1.42 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:30:19.817206: step 73280, loss = 1.38 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 06:30:30.079048: step 73290, loss = 1.43 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:30:40.714099: step 73300, loss = 1.73 (120.4 examples/sec; 1.064 sec/batch)
2018-04-10 06:30:50.957538: step 73310, loss = 1.34 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:31:01.282408: step 73320, loss = 1.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:31:11.587169: step 73330, loss = 1.51 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 06:31:21.888675: step 73340, loss = 1.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:31:32.168211: step 73350, loss = 1.40 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:31:42.430165: step 73360, loss = 1.63 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:31:52.678770: step 73370, loss = 1.54 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:32:02.980572: step 73380, loss = 1.60 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:32:13.290853: step 73390, loss = 1.71 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 06:32:23.857363: step 73400, loss = 1.50 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 06:32:34.114988: step 73410, loss = 1.31 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:32:44.406519: step 73420, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:32:54.665387: step 73430, loss = 1.47 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:33:04.985315: step 73440, loss = 1.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:33:15.331522: step 73450, loss = 1.57 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:33:25.653294: step 73460, loss = 1.55 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:33:35.941124: step 73470, loss = 1.58 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:33:46.241715: step 73480, loss = 1.70 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:33:56.488238: step 73490, loss = 1.50 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:34:07.078068: step 73500, loss = 1.44 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 06:34:17.410671: step 73510, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:34:27.678777: step 73520, loss = 1.48 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:34:38.012969: step 73530, loss = 1.37 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:34:48.317249: step 73540, loss = 1.50 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 06:34:58.602273: step 73550, loss = 1.66 (124.5 examples/sec; 1.029 sec/batch)
2018-04-10 06:35:08.926937: step 73560, loss = 1.42 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:35:19.188563: step 73570, loss = 1.48 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:35:29.437240: step 73580, loss = 1.54 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:35:39.690411: step 73590, loss = 1.39 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 06:35:50.226756: step 73600, loss = 1.69 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 06:36:00.464895: step 73610, loss = 1.73 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:36:10.864236: step 73620, loss = 1.49 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 06:36:21.132169: step 73630, loss = 1.51 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:36:31.459997: step 73640, loss = 1.46 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:36:41.833336: step 73650, loss = 1.35 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 06:36:52.154576: step 73660, loss = 1.40 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:37:02.455743: step 73670, loss = 1.44 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:37:12.779634: step 73680, loss = 1.36 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:37:23.100647: step 73690, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:37:33.731670: step 73700, loss = 1.64 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 06:37:43.986084: step 73710, loss = 1.43 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 06:37:54.249764: step 73720, loss = 1.41 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:38:04.640328: step 73730, loss = 1.80 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 06:38:14.912741: step 73740, loss = 1.66 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:38:25.148531: step 73750, loss = 1.36 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 06:38:35.388143: step 73760, loss = 1.52 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:38:45.629059: step 73770, loss = 1.57 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:38:55.875157: step 73780, loss = 1.17 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:39:06.190240: step 73790, loss = 1.56 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 06:39:16.738486: step 73800, loss = 1.71 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 06:39:26.972184: step 73810, loss = 1.55 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:39:37.233711: step 73820, loss = 1.48 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:39:47.503952: step 73830, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:39:57.733948: step 73840, loss = 1.33 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:40:08.017173: step 73850, loss = 1.43 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:40:18.322241: step 73860, loss = 1.65 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 06:40:28.564292: step 73870, loss = 1.43 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:40:38.914569: step 73880, loss = 1.51 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:40:49.157781: step 73890, loss = 1.57 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:40:59.692921: step 73900, loss = 1.64 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 06:41:09.965693: step 73910, loss = 1.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:41:20.221591: step 73920, loss = 1.46 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:41:30.456964: step 73930, loss = 1.39 (125.1 examples/sec; 1.024 sec/batch)
2018-04-10 06:41:40.704029: step 73940, loss = 1.62 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:41:50.984338: step 73950, loss = 1.66 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:42:01.313834: step 73960, loss = 1.60 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:42:11.626715: step 73970, loss = 1.39 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 06:42:21.958949: step 73980, loss = 1.68 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:42:32.346044: step 73990, loss = 1.52 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 06:42:42.940155: step 74000, loss = 1.42 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 06:42:53.213433: step 74010, loss = 1.53 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:43:03.541380: step 74020, loss = 1.53 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:43:13.875743: step 74030, loss = 1.57 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:43:24.147352: step 74040, loss = 1.64 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:43:34.439285: step 74050, loss = 1.35 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:43:44.766160: step 74060, loss = 1.34 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:43:55.060345: step 74070, loss = 1.51 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 06:44:05.418358: step 74080, loss = 1.47 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 06:44:15.765289: step 74090, loss = 1.69 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:44:26.337894: step 74100, loss = 1.31 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 06:44:36.612227: step 74110, loss = 1.40 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:44:46.942113: step 74120, loss = 1.31 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:44:57.209579: step 74130, loss = 1.38 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 06:45:07.540392: step 74140, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 06:45:17.843978: step 74150, loss = 1.59 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 06:45:28.158990: step 74160, loss = 1.30 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 06:45:38.402126: step 74170, loss = 1.69 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:45:48.703594: step 74180, loss = 1.65 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:45:59.089848: step 74190, loss = 1.42 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 06:46:09.764114: step 74200, loss = 1.62 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 06:46:20.063429: step 74210, loss = 1.40 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:46:30.361910: step 74220, loss = 1.56 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:46:40.599972: step 74230, loss = 1.53 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:46:50.857608: step 74240, loss = 1.52 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:47:01.119660: step 74250, loss = 1.52 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:47:11.436545: step 74260, loss = 1.71 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 06:47:21.705567: step 74270, loss = 1.33 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:47:32.017976: step 74280, loss = 1.72 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 06:47:42.257718: step 74290, loss = 1.53 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:47:52.783687: step 74300, loss = 1.61 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 06:48:03.003433: step 74310, loss = 1.24 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 06:48:13.259550: step 74320, loss = 1.38 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:48:23.509652: step 74330, loss = 1.36 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:48:33.740782: step 74340, loss = 1.61 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:48:43.986767: step 74350, loss = 1.45 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:48:54.244502: step 74360, loss = 1.76 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:49:04.536641: step 74370, loss = 1.35 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:49:15.151325: step 74380, loss = 1.30 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 06:49:25.443980: step 74390, loss = 1.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:49:35.954640: step 74400, loss = 1.66 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 06:49:46.170168: step 74410, loss = 1.36 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 06:49:56.577996: step 74420, loss = 1.35 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 06:50:06.917607: step 74430, loss = 1.34 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:50:17.209731: step 74440, loss = 1.55 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:50:27.455106: step 74450, loss = 1.55 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:50:37.826870: step 74460, loss = 1.48 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 06:50:48.084979: step 74470, loss = 1.25 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:50:58.362290: step 74480, loss = 1.69 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:51:08.685359: step 74490, loss = 1.69 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:51:19.247291: step 74500, loss = 1.52 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 06:51:29.484999: step 74510, loss = 1.50 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:51:39.748667: step 74520, loss = 1.59 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:51:49.997472: step 74530, loss = 1.44 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:52:00.267224: step 74540, loss = 1.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:52:10.583682: step 74550, loss = 1.47 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 06:52:20.882940: step 74560, loss = 1.47 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:52:31.146924: step 74570, loss = 1.41 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:52:41.392936: step 74580, loss = 1.63 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:52:51.654081: step 74590, loss = 1.38 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:53:02.264061: step 74600, loss = 1.46 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 06:53:12.571870: step 74610, loss = 1.52 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 06:53:22.870182: step 74620, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:53:33.147995: step 74630, loss = 1.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:53:43.496492: step 74640, loss = 1.59 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:53:53.818050: step 74650, loss = 1.52 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:54:04.134731: step 74660, loss = 1.45 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 06:54:14.474783: step 74670, loss = 1.62 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:54:24.764502: step 74680, loss = 1.61 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:54:35.008725: step 74690, loss = 1.52 (124.9 examples/sec; 1.024 sec/batch)
2018-04-10 06:54:45.533402: step 74700, loss = 1.48 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 06:54:55.774654: step 74710, loss = 1.77 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:55:06.065761: step 74720, loss = 1.31 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:55:16.349877: step 74730, loss = 1.40 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:55:26.641691: step 74740, loss = 1.74 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:55:36.892389: step 74750, loss = 1.45 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:55:47.176303: step 74760, loss = 1.70 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:55:57.426111: step 74770, loss = 1.63 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 06:56:07.774536: step 74780, loss = 1.69 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:56:18.065483: step 74790, loss = 1.55 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:56:28.639022: step 74800, loss = 1.46 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 06:56:38.975733: step 74810, loss = 1.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 06:56:49.240205: step 74820, loss = 1.65 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:56:59.495712: step 74830, loss = 1.53 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 06:57:09.841311: step 74840, loss = 1.47 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 06:57:20.154915: step 74850, loss = 1.59 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 06:57:30.416326: step 74860, loss = 1.67 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 06:57:40.650093: step 74870, loss = 1.59 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 06:57:50.887492: step 74880, loss = 1.50 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 06:58:01.098720: step 74890, loss = 1.61 (125.4 examples/sec; 1.021 sec/batch)
2018-04-10 06:58:11.725976: step 74900, loss = 1.45 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 06:58:22.022303: step 74910, loss = 1.60 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 06:58:32.326138: step 74920, loss = 1.50 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 06:58:42.607903: step 74930, loss = 1.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 06:58:52.895056: step 74940, loss = 1.33 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 06:59:03.213975: step 74950, loss = 1.59 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 06:59:13.603568: step 74960, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 06:59:23.872949: step 74970, loss = 1.42 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:59:37.575867: step 74980, loss = 1.54 (93.4 examples/sec; 1.370 sec/batch)
2018-04-10 06:59:47.850348: step 74990, loss = 1.48 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 06:59:58.425771: step 75000, loss = 1.39 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 07:00:08.790649: step 75010, loss = 1.51 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:00:19.118509: step 75020, loss = 1.54 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:00:29.406684: step 75030, loss = 1.39 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:00:39.815721: step 75040, loss = 1.57 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 07:00:50.180312: step 75050, loss = 1.64 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:01:00.595350: step 75060, loss = 1.40 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 07:01:11.059453: step 75070, loss = 1.66 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 07:01:21.435442: step 75080, loss = 1.58 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 07:01:31.724126: step 75090, loss = 1.68 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:01:42.317609: step 75100, loss = 1.53 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 07:01:52.604038: step 75110, loss = 1.67 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:02:02.984430: step 75120, loss = 1.52 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:02:13.327853: step 75130, loss = 1.41 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:02:23.629434: step 75140, loss = 1.26 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:02:33.933225: step 75150, loss = 1.72 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:02:44.290895: step 75160, loss = 1.41 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:02:54.601064: step 75170, loss = 1.51 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:03:04.937782: step 75180, loss = 1.42 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:03:15.340045: step 75190, loss = 1.39 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 07:03:25.942219: step 75200, loss = 1.47 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 07:03:36.232078: step 75210, loss = 1.63 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:03:46.532916: step 75220, loss = 1.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:03:56.832275: step 75230, loss = 1.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:04:07.182709: step 75240, loss = 1.51 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:04:17.557454: step 75250, loss = 1.52 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:04:27.914448: step 75260, loss = 1.60 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:04:38.195601: step 75270, loss = 1.41 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:04:48.528987: step 75280, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:04:58.823390: step 75290, loss = 1.53 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 07:05:09.462100: step 75300, loss = 1.78 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 07:05:19.838977: step 75310, loss = 1.30 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 07:05:30.242729: step 75320, loss = 1.36 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 07:05:40.557086: step 75330, loss = 1.39 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:05:50.890910: step 75340, loss = 1.41 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:06:01.255425: step 75350, loss = 1.62 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:06:11.631924: step 75360, loss = 1.44 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 07:06:21.935278: step 75370, loss = 1.44 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:06:32.203009: step 75380, loss = 1.46 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 07:06:42.482124: step 75390, loss = 1.63 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:06:53.057144: step 75400, loss = 1.57 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 07:07:03.371125: step 75410, loss = 1.55 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:07:13.782134: step 75420, loss = 1.69 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 07:07:24.089445: step 75430, loss = 1.67 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:07:34.386199: step 75440, loss = 1.44 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:07:44.700412: step 75450, loss = 1.55 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:07:55.014178: step 75460, loss = 1.42 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:08:05.362276: step 75470, loss = 1.39 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:08:15.733267: step 75480, loss = 1.67 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:08:26.080250: step 75490, loss = 1.53 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:08:36.667742: step 75500, loss = 1.54 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 07:08:46.963038: step 75510, loss = 1.46 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:08:57.326263: step 75520, loss = 1.35 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:09:07.705985: step 75530, loss = 1.31 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:09:18.041197: step 75540, loss = 1.67 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:09:28.338508: step 75550, loss = 1.61 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:09:38.639090: step 75560, loss = 1.41 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:09:48.942450: step 75570, loss = 1.69 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:09:59.255063: step 75580, loss = 1.43 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:10:09.608829: step 75590, loss = 1.33 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:10:20.224423: step 75600, loss = 1.62 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 07:10:30.502576: step 75610, loss = 1.51 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:10:40.895884: step 75620, loss = 1.40 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:10:51.178873: step 75630, loss = 1.44 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:11:01.524764: step 75640, loss = 1.35 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:11:11.858372: step 75650, loss = 1.38 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:11:22.183854: step 75660, loss = 1.59 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 07:11:32.445267: step 75670, loss = 1.36 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 07:11:42.725139: step 75680, loss = 1.52 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:11:53.000448: step 75690, loss = 1.35 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 07:12:03.576081: step 75700, loss = 1.65 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 07:12:13.849034: step 75710, loss = 1.36 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:12:24.197187: step 75720, loss = 1.42 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:12:34.492104: step 75730, loss = 1.68 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 07:12:44.769834: step 75740, loss = 1.34 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:12:55.042482: step 75750, loss = 1.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:13:05.356729: step 75760, loss = 1.45 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:13:15.660924: step 75770, loss = 1.44 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:13:25.969865: step 75780, loss = 1.51 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:13:36.244214: step 75790, loss = 1.67 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:13:46.809817: step 75800, loss = 1.66 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 07:13:57.059954: step 75810, loss = 1.36 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 07:14:07.393929: step 75820, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:14:17.721724: step 75830, loss = 1.46 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:14:28.000946: step 75840, loss = 1.52 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:14:38.246778: step 75850, loss = 1.49 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 07:14:48.541774: step 75860, loss = 1.50 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 07:14:58.854444: step 75870, loss = 1.48 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:15:09.225887: step 75880, loss = 1.38 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:15:19.589125: step 75890, loss = 1.56 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:15:30.195115: step 75900, loss = 1.42 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 07:15:40.588572: step 75910, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:15:50.934723: step 75920, loss = 1.53 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:16:01.271036: step 75930, loss = 1.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:16:11.665301: step 75940, loss = 1.42 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 07:16:22.059506: step 75950, loss = 1.63 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 07:16:32.549374: step 75960, loss = 1.44 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 07:16:42.870867: step 75970, loss = 1.64 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:16:53.133816: step 75980, loss = 1.82 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 07:17:03.473939: step 75990, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:17:14.090369: step 76000, loss = 1.48 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 07:17:24.381535: step 76010, loss = 1.37 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:17:34.681464: step 76020, loss = 1.72 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:17:44.999995: step 76030, loss = 1.33 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:17:55.327696: step 76040, loss = 1.64 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:18:05.703652: step 76050, loss = 1.61 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 07:18:16.073882: step 76060, loss = 1.41 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:18:26.345897: step 76070, loss = 1.45 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:18:36.725953: step 76080, loss = 1.51 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:18:46.965077: step 76090, loss = 1.43 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 07:18:57.627771: step 76100, loss = 1.62 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 07:19:07.942080: step 76110, loss = 1.45 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:19:18.250559: step 76120, loss = 1.29 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:19:28.577324: step 76130, loss = 1.38 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:19:38.870744: step 76140, loss = 1.44 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:19:49.209506: step 76150, loss = 1.31 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:19:59.550290: step 76160, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:20:09.887464: step 76170, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:20:20.279931: step 76180, loss = 1.79 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:20:30.665142: step 76190, loss = 1.41 (123.3 examples/sec; 1.039 sec/batch)
2018-04-10 07:20:41.338395: step 76200, loss = 1.43 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 07:20:51.644561: step 76210, loss = 1.52 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:21:02.024491: step 76220, loss = 1.41 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:21:12.379379: step 76230, loss = 1.67 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:21:22.731818: step 76240, loss = 1.38 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:21:33.052141: step 76250, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:21:43.371503: step 76260, loss = 1.54 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:21:53.696718: step 76270, loss = 1.62 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 07:22:04.097736: step 76280, loss = 1.41 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 07:22:14.415928: step 76290, loss = 1.50 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:22:25.049953: step 76300, loss = 1.57 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 07:22:35.327185: step 76310, loss = 1.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:22:45.634799: step 76320, loss = 1.55 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:22:55.926458: step 76330, loss = 1.59 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:23:06.277720: step 76340, loss = 1.49 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:23:16.631511: step 76350, loss = 1.53 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:23:27.032088: step 76360, loss = 1.59 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 07:23:37.334372: step 76370, loss = 1.67 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:23:47.653921: step 76380, loss = 1.47 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:23:57.947813: step 76390, loss = 1.51 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 07:24:08.560467: step 76400, loss = 1.50 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 07:24:18.872246: step 76410, loss = 1.43 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:24:29.192907: step 76420, loss = 1.47 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:24:39.527166: step 76430, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:24:49.845707: step 76440, loss = 1.35 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:25:00.138762: step 76450, loss = 1.64 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:25:10.510377: step 76460, loss = 1.60 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:25:20.864323: step 76470, loss = 1.21 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:25:31.225027: step 76480, loss = 1.63 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:25:41.522141: step 76490, loss = 1.42 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:25:52.089354: step 76500, loss = 1.40 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 07:26:02.429529: step 76510, loss = 1.45 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:26:12.866045: step 76520, loss = 1.56 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 07:26:23.166989: step 76530, loss = 1.46 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:26:33.481492: step 76540, loss = 1.59 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:26:43.851659: step 76550, loss = 1.42 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:26:54.175379: step 76560, loss = 1.54 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:27:04.520459: step 76570, loss = 1.39 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:27:14.853174: step 76580, loss = 1.56 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:27:25.187901: step 76590, loss = 1.44 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:27:35.782141: step 76600, loss = 1.68 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 07:27:46.034827: step 76610, loss = 1.63 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 07:27:56.327687: step 76620, loss = 1.40 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:28:06.666201: step 76630, loss = 1.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:28:16.995451: step 76640, loss = 1.48 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:28:27.306224: step 76650, loss = 1.31 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:28:37.627248: step 76660, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:28:47.966535: step 76670, loss = 1.57 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:28:58.341400: step 76680, loss = 1.39 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:29:08.734610: step 76690, loss = 1.72 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:29:19.327652: step 76700, loss = 1.66 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 07:29:29.600724: step 76710, loss = 1.41 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:29:39.843563: step 76720, loss = 1.35 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 07:29:50.107178: step 76730, loss = 1.57 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 07:30:00.383372: step 76740, loss = 1.58 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 07:30:10.743598: step 76750, loss = 1.52 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:30:21.062781: step 76760, loss = 1.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:30:31.368917: step 76770, loss = 1.53 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:30:41.731081: step 76780, loss = 1.52 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:30:52.018326: step 76790, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:31:02.630464: step 76800, loss = 1.63 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 07:31:12.961148: step 76810, loss = 1.44 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:31:23.267337: step 76820, loss = 1.64 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:31:33.585443: step 76830, loss = 1.58 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:31:43.907295: step 76840, loss = 1.80 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:31:54.197116: step 76850, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:32:04.557896: step 76860, loss = 1.67 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:32:14.851339: step 76870, loss = 1.36 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:32:25.193782: step 76880, loss = 1.56 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:32:35.501649: step 76890, loss = 1.52 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:32:46.069155: step 76900, loss = 1.64 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 07:32:56.356421: step 76910, loss = 1.44 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:33:06.691746: step 76920, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:33:17.013310: step 76930, loss = 1.65 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:33:27.310710: step 76940, loss = 1.33 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:33:37.635640: step 76950, loss = 1.77 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:33:47.992576: step 76960, loss = 1.20 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:33:58.294655: step 76970, loss = 1.33 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:34:08.688017: step 76980, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:34:19.023895: step 76990, loss = 1.35 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:34:29.711798: step 77000, loss = 1.43 (119.8 examples/sec; 1.069 sec/batch)
2018-04-10 07:34:39.996099: step 77010, loss = 1.46 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:34:50.313132: step 77020, loss = 1.54 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:35:00.631493: step 77030, loss = 1.41 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:35:10.995815: step 77040, loss = 1.71 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:35:21.360186: step 77050, loss = 1.54 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:35:31.682035: step 77060, loss = 1.46 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:35:42.033542: step 77070, loss = 1.53 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:35:52.461986: step 77080, loss = 1.67 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 07:36:02.809755: step 77090, loss = 1.37 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:36:13.410661: step 77100, loss = 1.35 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 07:36:23.714821: step 77110, loss = 1.69 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:36:34.079520: step 77120, loss = 1.58 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:36:44.416092: step 77130, loss = 1.48 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:36:54.738765: step 77140, loss = 1.51 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:37:05.109510: step 77150, loss = 1.44 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:37:15.478447: step 77160, loss = 1.43 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:37:25.809584: step 77170, loss = 1.67 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:37:36.179264: step 77180, loss = 1.48 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:37:46.481994: step 77190, loss = 1.42 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:37:57.083233: step 77200, loss = 1.40 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 07:38:07.437045: step 77210, loss = 1.37 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:38:17.898439: step 77220, loss = 1.38 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 07:38:28.276370: step 77230, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:38:38.590609: step 77240, loss = 1.32 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:38:48.904525: step 77250, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:38:59.226090: step 77260, loss = 1.62 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:39:09.602878: step 77270, loss = 1.53 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 07:39:19.909264: step 77280, loss = 1.37 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:39:30.206915: step 77290, loss = 1.60 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:39:40.817327: step 77300, loss = 1.54 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 07:39:51.103444: step 77310, loss = 1.54 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:40:01.421155: step 77320, loss = 1.39 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:40:11.786101: step 77330, loss = 1.57 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:40:22.147642: step 77340, loss = 1.47 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:40:32.451594: step 77350, loss = 1.24 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:40:42.910593: step 77360, loss = 1.51 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 07:40:53.207442: step 77370, loss = 1.62 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:41:03.627854: step 77380, loss = 1.48 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 07:41:13.950209: step 77390, loss = 1.41 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:41:24.550002: step 77400, loss = 1.41 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 07:41:34.831163: step 77410, loss = 1.26 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:41:45.153446: step 77420, loss = 1.62 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:41:55.419824: step 77430, loss = 1.37 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 07:42:05.761570: step 77440, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:42:16.067165: step 77450, loss = 1.60 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:42:26.385004: step 77460, loss = 1.22 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:42:36.656950: step 77470, loss = 1.50 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:42:46.980359: step 77480, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:42:57.271033: step 77490, loss = 1.48 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:43:07.847459: step 77500, loss = 1.50 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 07:43:18.145400: step 77510, loss = 1.62 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:43:28.450555: step 77520, loss = 1.50 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:43:38.730213: step 77530, loss = 1.44 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:43:49.055186: step 77540, loss = 1.40 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:43:59.400358: step 77550, loss = 1.65 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:44:09.799226: step 77560, loss = 1.39 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 07:44:20.094955: step 77570, loss = 1.68 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:44:30.386198: step 77580, loss = 1.40 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:44:40.672120: step 77590, loss = 1.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:44:51.240643: step 77600, loss = 1.53 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 07:45:01.513964: step 77610, loss = 1.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 07:45:11.854273: step 77620, loss = 1.53 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:45:22.176951: step 77630, loss = 1.54 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:45:32.494808: step 77640, loss = 1.63 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:45:42.900122: step 77650, loss = 1.24 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 07:45:53.257595: step 77660, loss = 1.52 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:46:03.601943: step 77670, loss = 1.39 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 07:46:13.935827: step 77680, loss = 1.44 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:46:24.225881: step 77690, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:46:34.795690: step 77700, loss = 1.47 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 07:46:45.073743: step 77710, loss = 1.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:46:55.377083: step 77720, loss = 1.56 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:47:05.733882: step 77730, loss = 1.27 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:47:16.081402: step 77740, loss = 1.32 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:47:26.424475: step 77750, loss = 1.25 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:47:36.778679: step 77760, loss = 1.52 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:47:47.027073: step 77770, loss = 1.54 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 07:47:57.304795: step 77780, loss = 1.37 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:48:07.602791: step 77790, loss = 1.62 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:48:18.183907: step 77800, loss = 1.45 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 07:48:28.478299: step 77810, loss = 1.61 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 07:48:38.756044: step 77820, loss = 1.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:48:49.037388: step 77830, loss = 1.74 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:48:59.348177: step 77840, loss = 1.44 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:49:09.675218: step 77850, loss = 1.45 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:49:20.001377: step 77860, loss = 1.52 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 07:49:30.281761: step 77870, loss = 1.56 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 07:49:40.570447: step 77880, loss = 1.65 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:49:50.811943: step 77890, loss = 1.61 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 07:50:01.385284: step 77900, loss = 1.46 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 07:50:11.713324: step 77910, loss = 1.40 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:50:22.045687: step 77920, loss = 1.48 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:50:32.338385: step 77930, loss = 1.55 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:50:42.742970: step 77940, loss = 1.35 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 07:50:53.124852: step 77950, loss = 1.59 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:51:03.522994: step 77960, loss = 1.54 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 07:51:13.862584: step 77970, loss = 1.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:51:24.187176: step 77980, loss = 1.61 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:51:34.497969: step 77990, loss = 1.57 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:51:45.113634: step 78000, loss = 1.57 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 07:51:55.416604: step 78010, loss = 1.53 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 07:52:05.795541: step 78020, loss = 1.34 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:52:16.137043: step 78030, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:52:26.474913: step 78040, loss = 1.71 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:52:36.768201: step 78050, loss = 1.48 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:52:47.088239: step 78060, loss = 1.40 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:52:57.399487: step 78070, loss = 1.54 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 07:53:07.782463: step 78080, loss = 1.33 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 07:53:18.114987: step 78090, loss = 1.51 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:53:28.689453: step 78100, loss = 1.27 (121.0 examples/sec; 1.057 sec/batch)
2018-04-10 07:53:38.941193: step 78110, loss = 1.61 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 07:53:49.232128: step 78120, loss = 1.18 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:53:59.485274: step 78130, loss = 1.22 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 07:54:09.834649: step 78140, loss = 1.54 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:54:20.130590: step 78150, loss = 1.35 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 07:54:30.421119: step 78160, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:54:40.684574: step 78170, loss = 1.67 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 07:54:50.977479: step 78180, loss = 1.49 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 07:55:01.237622: step 78190, loss = 1.51 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 07:55:11.860309: step 78200, loss = 1.53 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 07:55:22.181440: step 78210, loss = 1.37 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:55:32.508574: step 78220, loss = 1.47 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 07:55:42.834305: step 78230, loss = 1.54 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 07:55:53.230919: step 78240, loss = 1.46 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 07:56:03.637959: step 78250, loss = 1.42 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 07:56:14.010575: step 78260, loss = 1.56 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 07:56:24.317653: step 78270, loss = 1.44 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 07:56:34.654071: step 78280, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:56:44.969099: step 78290, loss = 1.40 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 07:56:55.601024: step 78300, loss = 1.57 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 07:57:05.959411: step 78310, loss = 1.57 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:57:16.317946: step 78320, loss = 1.56 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 07:57:26.657337: step 78330, loss = 1.34 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:57:36.996613: step 78340, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:57:47.334193: step 78350, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 07:57:57.686198: step 78360, loss = 1.76 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:58:08.079410: step 78370, loss = 1.51 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:58:18.441316: step 78380, loss = 1.40 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 07:58:28.788131: step 78390, loss = 1.88 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 07:58:39.386679: step 78400, loss = 1.57 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 07:58:49.706577: step 78410, loss = 1.43 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 07:59:00.060279: step 78420, loss = 1.41 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 07:59:10.496329: step 78430, loss = 1.57 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 07:59:20.862615: step 78440, loss = 1.42 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 07:59:31.250716: step 78450, loss = 1.77 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 07:59:41.627505: step 78460, loss = 1.61 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 07:59:51.952327: step 78470, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:00:02.351239: step 78480, loss = 1.37 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:00:12.737549: step 78490, loss = 1.20 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:00:23.363384: step 78500, loss = 1.39 (120.5 examples/sec; 1.063 sec/batch)
2018-04-10 08:00:33.677351: step 78510, loss = 1.57 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:00:44.114574: step 78520, loss = 1.46 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 08:00:54.496254: step 78530, loss = 1.51 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:01:04.947019: step 78540, loss = 1.72 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:01:15.344254: step 78550, loss = 1.53 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:01:25.692021: step 78560, loss = 1.38 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:01:35.992894: step 78570, loss = 1.53 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 08:01:46.298692: step 78580, loss = 1.54 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:01:56.604483: step 78590, loss = 1.38 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:02:07.247648: step 78600, loss = 1.44 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 08:02:17.589849: step 78610, loss = 1.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:02:27.925419: step 78620, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:02:38.241276: step 78630, loss = 1.58 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:02:48.568682: step 78640, loss = 1.62 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:02:58.911625: step 78650, loss = 1.53 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:03:09.276417: step 78660, loss = 1.62 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:03:19.618163: step 78670, loss = 1.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:03:29.936988: step 78680, loss = 1.59 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:03:40.236119: step 78690, loss = 1.79 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 08:03:50.945129: step 78700, loss = 1.44 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 08:04:01.312693: step 78710, loss = 1.51 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:04:11.682304: step 78720, loss = 1.52 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:04:21.988934: step 78730, loss = 1.61 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:04:32.306058: step 78740, loss = 1.61 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:04:42.594612: step 78750, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:04:52.880757: step 78760, loss = 1.50 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:05:03.194961: step 78770, loss = 1.56 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:05:13.547886: step 78780, loss = 1.59 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 08:05:23.829969: step 78790, loss = 1.57 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:05:34.363318: step 78800, loss = 1.51 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 08:05:44.619702: step 78810, loss = 1.35 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 08:05:54.936490: step 78820, loss = 1.43 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:06:05.296011: step 78830, loss = 1.72 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:06:15.648074: step 78840, loss = 1.56 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 08:06:25.959822: step 78850, loss = 1.36 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:06:36.259447: step 78860, loss = 1.59 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 08:06:46.548090: step 78870, loss = 1.51 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:06:56.862383: step 78880, loss = 1.40 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:07:07.208865: step 78890, loss = 1.51 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:07:17.872817: step 78900, loss = 1.50 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 08:07:28.141887: step 78910, loss = 1.49 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 08:07:38.436278: step 78920, loss = 1.72 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 08:07:48.721117: step 78930, loss = 1.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:07:59.042309: step 78940, loss = 1.35 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:08:09.407176: step 78950, loss = 1.49 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:08:19.761586: step 78960, loss = 1.38 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 08:08:30.039639: step 78970, loss = 1.34 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:08:40.312691: step 78980, loss = 1.52 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 08:08:50.575444: step 78990, loss = 1.40 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 08:09:01.195822: step 79000, loss = 1.43 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 08:09:11.579908: step 79010, loss = 1.57 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:09:21.919631: step 79020, loss = 1.43 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:09:32.249436: step 79030, loss = 1.30 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:09:42.665320: step 79040, loss = 1.55 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 08:09:53.065057: step 79050, loss = 1.42 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:10:03.475745: step 79060, loss = 1.37 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 08:10:13.897986: step 79070, loss = 1.39 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 08:10:24.253764: step 79080, loss = 1.41 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:10:34.560348: step 79090, loss = 1.37 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:10:45.263612: step 79100, loss = 1.49 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 08:10:55.592773: step 79110, loss = 1.53 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:11:06.096278: step 79120, loss = 1.33 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 08:11:16.545295: step 79130, loss = 1.59 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:11:26.972194: step 79140, loss = 1.38 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 08:11:37.371334: step 79150, loss = 1.55 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:11:47.756453: step 79160, loss = 1.56 (123.3 examples/sec; 1.039 sec/batch)
2018-04-10 08:11:58.077696: step 79170, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:12:08.452366: step 79180, loss = 1.47 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:12:18.773938: step 79190, loss = 1.40 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:12:29.350277: step 79200, loss = 1.51 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 08:12:39.622525: step 79210, loss = 1.48 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 08:12:49.916129: step 79220, loss = 1.57 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 08:13:00.201430: step 79230, loss = 1.50 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:13:10.560367: step 79240, loss = 1.60 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:13:20.879240: step 79250, loss = 1.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:13:31.188913: step 79260, loss = 1.57 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:13:41.450501: step 79270, loss = 1.33 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 08:13:51.736015: step 79280, loss = 1.35 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:14:02.110628: step 79290, loss = 1.46 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:14:12.712470: step 79300, loss = 1.61 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 08:14:22.998247: step 79310, loss = 1.44 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:14:33.275259: step 79320, loss = 1.59 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:14:43.595212: step 79330, loss = 1.53 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:14:53.927629: step 79340, loss = 1.41 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:15:04.265439: step 79350, loss = 1.32 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:15:14.599114: step 79360, loss = 1.65 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:15:24.859236: step 79370, loss = 1.42 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 08:15:35.127130: step 79380, loss = 1.51 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 08:15:45.381923: step 79390, loss = 1.64 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 08:15:55.944136: step 79400, loss = 1.34 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 08:16:06.255008: step 79410, loss = 1.48 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:16:16.688463: step 79420, loss = 1.63 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:16:27.014078: step 79430, loss = 1.61 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 08:16:37.326600: step 79440, loss = 1.46 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:16:47.666680: step 79450, loss = 1.57 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:16:58.117737: step 79460, loss = 1.40 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:17:08.508571: step 79470, loss = 1.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:17:18.886805: step 79480, loss = 1.43 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:17:29.216244: step 79490, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:17:39.815046: step 79500, loss = 1.40 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 08:17:50.136781: step 79510, loss = 1.50 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:18:00.483566: step 79520, loss = 1.55 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:18:10.852569: step 79530, loss = 1.30 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:18:21.208480: step 79540, loss = 1.48 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:18:31.578654: step 79550, loss = 1.42 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:18:41.929958: step 79560, loss = 1.37 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:18:52.296723: step 79570, loss = 1.52 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:19:02.645289: step 79580, loss = 1.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:19:13.018902: step 79590, loss = 1.35 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:19:23.644055: step 79600, loss = 1.48 (120.5 examples/sec; 1.063 sec/batch)
2018-04-10 08:19:33.916083: step 79610, loss = 1.43 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 08:19:44.224106: step 79620, loss = 1.55 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:19:54.547894: step 79630, loss = 1.54 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:20:04.936075: step 79640, loss = 1.54 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:20:15.313902: step 79650, loss = 1.45 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:20:25.689798: step 79660, loss = 1.56 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:20:36.001418: step 79670, loss = 1.59 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:20:46.428143: step 79680, loss = 1.69 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 08:20:56.825424: step 79690, loss = 1.56 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:21:07.478684: step 79700, loss = 1.59 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 08:21:17.938303: step 79710, loss = 1.63 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 08:21:28.340216: step 79720, loss = 1.48 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:21:38.690151: step 79730, loss = 1.53 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:21:49.094017: step 79740, loss = 1.53 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:21:59.471664: step 79750, loss = 1.52 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:22:09.932990: step 79760, loss = 1.47 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 08:22:20.368228: step 79770, loss = 1.48 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 08:22:30.779921: step 79780, loss = 1.40 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 08:22:41.136210: step 79790, loss = 1.52 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:22:51.786621: step 79800, loss = 1.52 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 08:23:02.182679: step 79810, loss = 1.69 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:23:12.613190: step 79820, loss = 1.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:23:23.012885: step 79830, loss = 1.46 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:23:33.375255: step 79840, loss = 1.33 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:23:43.733124: step 79850, loss = 1.60 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:23:54.127426: step 79860, loss = 1.70 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 08:24:04.554564: step 79870, loss = 1.40 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 08:24:14.974836: step 79880, loss = 1.36 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 08:24:25.406422: step 79890, loss = 1.57 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:24:36.114881: step 79900, loss = 1.57 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 08:24:46.474773: step 79910, loss = 1.40 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:24:56.904171: step 79920, loss = 1.49 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:25:07.370881: step 79930, loss = 1.54 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 08:25:17.834694: step 79940, loss = 1.26 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 08:25:28.271383: step 79950, loss = 1.53 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 08:25:38.681553: step 79960, loss = 1.50 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 08:25:49.087131: step 79970, loss = 1.44 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 08:25:59.516540: step 79980, loss = 1.40 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:26:09.976774: step 79990, loss = 1.55 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 08:26:20.677710: step 80000, loss = 1.50 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 08:26:31.067625: step 80010, loss = 1.65 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:26:41.461319: step 80020, loss = 1.38 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:26:51.834416: step 80030, loss = 1.48 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:27:02.224299: step 80040, loss = 1.40 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:27:12.648051: step 80050, loss = 1.41 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 08:27:23.041429: step 80060, loss = 1.51 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:27:33.434585: step 80070, loss = 1.45 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:27:43.822480: step 80080, loss = 1.42 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:27:54.218691: step 80090, loss = 1.33 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:28:04.889424: step 80100, loss = 1.44 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 08:28:15.283166: step 80110, loss = 1.56 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:28:25.726636: step 80120, loss = 1.37 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 08:28:36.113756: step 80130, loss = 1.31 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:28:46.488106: step 80140, loss = 1.37 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:28:56.936259: step 80150, loss = 1.68 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:29:07.436397: step 80160, loss = 1.41 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 08:29:17.886096: step 80170, loss = 1.52 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:29:28.247411: step 80180, loss = 1.27 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:29:38.622934: step 80190, loss = 1.41 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:29:49.308640: step 80200, loss = 1.58 (119.8 examples/sec; 1.069 sec/batch)
2018-04-10 08:29:59.690416: step 80210, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:30:10.121650: step 80220, loss = 1.46 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:30:20.465790: step 80230, loss = 1.58 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 08:30:30.851765: step 80240, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:30:41.284275: step 80250, loss = 1.65 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:30:51.675962: step 80260, loss = 1.64 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:31:02.094926: step 80270, loss = 1.56 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 08:31:12.554783: step 80280, loss = 1.48 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 08:31:22.923686: step 80290, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:31:33.521968: step 80300, loss = 1.44 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 08:31:43.849051: step 80310, loss = 1.32 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:31:54.246428: step 80320, loss = 1.47 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:32:04.678439: step 80330, loss = 1.49 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:32:15.065232: step 80340, loss = 1.35 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:32:25.431903: step 80350, loss = 1.47 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:32:35.865907: step 80360, loss = 1.58 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 08:32:46.230340: step 80370, loss = 1.39 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:32:56.594089: step 80380, loss = 1.39 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:33:06.983900: step 80390, loss = 1.60 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:33:17.613518: step 80400, loss = 1.52 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 08:33:27.929700: step 80410, loss = 1.57 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:33:38.273821: step 80420, loss = 1.56 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 08:33:48.593082: step 80430, loss = 1.57 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:33:58.983383: step 80440, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:34:09.387205: step 80450, loss = 1.53 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:34:19.738469: step 80460, loss = 1.49 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:34:30.058490: step 80470, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:34:40.424887: step 80480, loss = 1.53 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:34:50.771965: step 80490, loss = 1.58 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:35:01.389329: step 80500, loss = 1.51 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 08:35:11.771808: step 80510, loss = 1.34 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:35:22.163174: step 80520, loss = 1.57 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:35:32.521733: step 80530, loss = 1.39 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:35:42.896589: step 80540, loss = 1.34 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:35:53.259361: step 80550, loss = 1.48 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:36:03.650656: step 80560, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:36:14.000619: step 80570, loss = 1.30 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 08:36:24.344782: step 80580, loss = 1.55 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 08:36:34.687073: step 80590, loss = 1.56 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:36:45.319913: step 80600, loss = 1.34 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 08:36:55.651785: step 80610, loss = 1.29 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:37:06.079052: step 80620, loss = 1.41 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 08:37:16.452145: step 80630, loss = 1.40 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:37:26.825439: step 80640, loss = 1.63 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:37:37.238170: step 80650, loss = 1.41 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 08:37:47.651470: step 80660, loss = 1.44 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 08:37:58.053822: step 80670, loss = 1.47 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:38:08.514434: step 80680, loss = 1.61 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 08:38:18.911548: step 80690, loss = 1.30 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:38:29.614342: step 80700, loss = 1.72 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 08:38:40.000148: step 80710, loss = 1.53 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:38:50.386808: step 80720, loss = 1.39 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:39:00.789990: step 80730, loss = 1.58 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:39:11.272610: step 80740, loss = 1.28 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 08:39:21.737332: step 80750, loss = 1.62 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 08:39:32.276941: step 80760, loss = 1.50 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 08:39:42.776181: step 80770, loss = 1.43 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 08:39:53.180192: step 80780, loss = 1.56 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:40:03.633088: step 80790, loss = 1.47 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:40:14.339751: step 80800, loss = 1.42 (119.6 examples/sec; 1.071 sec/batch)
2018-04-10 08:40:24.742418: step 80810, loss = 1.49 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:40:35.118370: step 80820, loss = 1.59 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:40:45.570301: step 80830, loss = 1.39 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:40:55.955191: step 80840, loss = 1.22 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:41:06.445471: step 80850, loss = 1.68 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 08:41:16.883058: step 80860, loss = 1.50 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 08:41:27.266256: step 80870, loss = 1.47 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:41:37.762055: step 80880, loss = 1.32 (122.0 examples/sec; 1.050 sec/batch)
2018-04-10 08:41:48.138675: step 80890, loss = 1.66 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:41:58.793313: step 80900, loss = 1.36 (120.1 examples/sec; 1.065 sec/batch)
2018-04-10 08:42:09.184632: step 80910, loss = 1.67 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:42:19.577810: step 80920, loss = 1.72 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:42:29.953346: step 80930, loss = 1.62 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:42:40.329027: step 80940, loss = 1.25 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:42:50.702876: step 80950, loss = 1.62 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:43:01.094901: step 80960, loss = 1.61 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:43:11.521364: step 80970, loss = 1.39 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 08:43:21.915535: step 80980, loss = 1.57 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 08:43:32.297871: step 80990, loss = 1.38 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:43:42.964662: step 81000, loss = 1.45 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 08:43:53.334871: step 81010, loss = 1.61 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:44:03.782603: step 81020, loss = 1.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:44:14.180687: step 81030, loss = 1.49 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:44:24.579256: step 81040, loss = 1.42 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:44:34.948255: step 81050, loss = 1.57 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:44:45.343358: step 81060, loss = 1.54 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:44:55.717390: step 81070, loss = 1.71 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:45:06.142027: step 81080, loss = 1.55 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 08:45:16.546906: step 81090, loss = 1.44 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 08:45:27.211844: step 81100, loss = 1.44 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 08:45:37.550495: step 81110, loss = 1.45 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:45:47.935913: step 81120, loss = 1.64 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:45:58.343765: step 81130, loss = 1.29 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 08:46:08.782194: step 81140, loss = 1.71 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 08:46:19.193030: step 81150, loss = 1.58 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 08:46:29.728474: step 81160, loss = 1.32 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 08:46:40.125051: step 81170, loss = 1.56 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:46:50.526132: step 81180, loss = 1.59 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:47:00.973200: step 81190, loss = 1.35 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 08:47:11.695572: step 81200, loss = 1.74 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 08:47:22.035786: step 81210, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:47:32.397309: step 81220, loss = 1.37 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:47:42.795438: step 81230, loss = 1.64 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:47:53.262617: step 81240, loss = 1.49 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 08:48:03.662051: step 81250, loss = 1.31 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:48:14.069879: step 81260, loss = 1.63 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 08:48:24.429916: step 81270, loss = 1.68 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:48:34.798435: step 81280, loss = 1.34 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:48:45.139926: step 81290, loss = 1.47 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:48:55.751822: step 81300, loss = 1.58 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 08:49:06.138008: step 81310, loss = 1.39 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:49:16.535262: step 81320, loss = 1.42 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:49:26.877121: step 81330, loss = 1.75 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:49:37.209414: step 81340, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:49:47.542454: step 81350, loss = 1.38 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:49:57.912206: step 81360, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:50:08.337911: step 81370, loss = 1.58 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 08:50:18.714210: step 81380, loss = 1.31 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 08:50:29.072787: step 81390, loss = 1.66 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:50:39.787441: step 81400, loss = 1.44 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 08:50:50.115189: step 81410, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 08:51:00.479223: step 81420, loss = 1.79 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:51:10.897919: step 81430, loss = 1.45 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 08:51:21.263565: step 81440, loss = 1.45 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:51:31.633008: step 81450, loss = 1.56 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 08:51:41.997054: step 81460, loss = 1.64 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:51:52.360674: step 81470, loss = 1.42 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:52:02.762087: step 81480, loss = 1.42 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 08:52:13.145348: step 81490, loss = 1.46 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:52:23.785219: step 81500, loss = 1.31 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 08:52:34.082653: step 81510, loss = 1.52 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 08:52:44.353375: step 81520, loss = 1.58 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 08:52:54.619195: step 81530, loss = 1.44 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 08:53:04.964167: step 81540, loss = 1.54 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 08:53:15.271589: step 81550, loss = 1.44 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:53:25.566690: step 81560, loss = 1.36 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 08:53:35.840688: step 81570, loss = 1.51 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 08:53:46.122148: step 81580, loss = 1.48 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:53:56.384714: step 81590, loss = 1.45 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 08:54:06.987838: step 81600, loss = 1.49 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 08:54:17.327636: step 81610, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:54:27.610189: step 81620, loss = 1.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:54:37.873273: step 81630, loss = 1.54 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 08:54:48.133109: step 81640, loss = 1.78 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 08:54:58.424350: step 81650, loss = 1.48 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:55:08.781042: step 81660, loss = 1.32 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 08:55:19.085947: step 81670, loss = 1.43 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 08:55:29.402802: step 81680, loss = 1.64 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:55:39.713898: step 81690, loss = 1.42 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:55:50.418350: step 81700, loss = 1.57 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 08:56:00.754742: step 81710, loss = 1.57 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 08:56:11.132526: step 81720, loss = 1.43 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 08:56:21.439319: step 81730, loss = 1.62 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:56:31.745110: step 81740, loss = 1.51 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:56:42.039786: step 81750, loss = 1.29 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 08:56:52.357427: step 81760, loss = 1.67 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:57:02.676412: step 81770, loss = 1.55 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:57:13.036983: step 81780, loss = 1.43 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 08:57:23.345377: step 81790, loss = 1.51 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:57:33.936095: step 81800, loss = 1.54 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 08:57:44.328926: step 81810, loss = 1.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 08:57:54.647891: step 81820, loss = 1.49 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 08:58:04.974313: step 81830, loss = 1.36 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 08:58:15.339923: step 81840, loss = 1.53 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:58:25.656933: step 81850, loss = 1.57 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 08:58:35.968851: step 81860, loss = 1.50 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 08:58:46.262726: step 81870, loss = 1.47 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 08:58:56.571989: step 81880, loss = 1.64 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 08:59:06.937899: step 81890, loss = 1.71 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 08:59:17.549151: step 81900, loss = 1.59 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 08:59:27.841158: step 81910, loss = 1.29 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 08:59:38.120332: step 81920, loss = 1.57 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 08:59:48.417360: step 81930, loss = 1.40 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 08:59:58.722823: step 81940, loss = 1.37 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:00:09.063932: step 81950, loss = 1.67 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:00:19.347248: step 81960, loss = 1.43 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:00:29.632168: step 81970, loss = 1.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:00:39.964269: step 81980, loss = 1.69 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:00:50.164344: step 81990, loss = 1.41 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 09:01:00.701382: step 82000, loss = 1.57 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 09:01:11.012035: step 82010, loss = 1.64 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:01:21.309225: step 82020, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:01:31.613442: step 82030, loss = 1.49 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 09:01:41.879703: step 82040, loss = 1.37 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:01:52.270534: step 82050, loss = 1.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:02:02.595789: step 82060, loss = 1.47 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 09:02:12.916221: step 82070, loss = 1.63 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:02:23.184859: step 82080, loss = 1.61 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:02:33.430258: step 82090, loss = 1.58 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 09:02:43.963431: step 82100, loss = 1.62 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 09:02:54.198281: step 82110, loss = 1.57 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 09:03:04.492872: step 82120, loss = 1.31 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 09:03:14.820844: step 82130, loss = 1.55 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:03:25.088420: step 82140, loss = 1.46 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:03:35.369289: step 82150, loss = 1.41 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:03:45.640996: step 82160, loss = 1.44 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:03:55.914409: step 82170, loss = 1.45 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:04:06.248253: step 82180, loss = 1.55 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:04:16.558894: step 82190, loss = 1.54 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:04:27.122057: step 82200, loss = 1.60 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 09:04:37.385561: step 82210, loss = 1.45 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 09:04:47.666028: step 82220, loss = 1.30 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:04:57.939891: step 82230, loss = 1.57 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:05:08.267998: step 82240, loss = 1.64 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:05:18.579270: step 82250, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:05:28.857694: step 82260, loss = 1.46 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:05:39.094934: step 82270, loss = 1.40 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:05:49.366149: step 82280, loss = 1.51 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:05:59.646932: step 82290, loss = 1.55 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:06:10.224456: step 82300, loss = 1.42 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 09:06:20.502999: step 82310, loss = 1.28 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:06:30.780892: step 82320, loss = 1.48 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:06:41.030319: step 82330, loss = 1.72 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 09:06:51.247373: step 82340, loss = 1.66 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 09:07:01.571263: step 82350, loss = 1.50 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:07:11.875932: step 82360, loss = 1.61 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 09:07:22.160546: step 82370, loss = 1.60 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:07:32.426672: step 82380, loss = 1.42 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:07:42.693920: step 82390, loss = 1.47 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:07:53.238536: step 82400, loss = 1.53 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 09:08:03.536934: step 82410, loss = 1.43 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:08:13.811617: step 82420, loss = 1.50 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:08:24.044657: step 82430, loss = 1.59 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 09:08:34.281504: step 82440, loss = 1.53 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:08:44.523318: step 82450, loss = 1.52 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:08:54.811987: step 82460, loss = 1.44 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:09:05.123494: step 82470, loss = 1.37 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:09:15.412827: step 82480, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:09:25.677272: step 82490, loss = 1.53 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 09:09:36.200720: step 82500, loss = 1.52 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 09:09:46.432135: step 82510, loss = 1.46 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 09:09:56.702626: step 82520, loss = 1.41 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:10:07.002234: step 82530, loss = 1.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:10:17.300605: step 82540, loss = 1.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:10:27.592355: step 82550, loss = 1.48 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:10:38.009133: step 82560, loss = 1.46 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 09:10:48.210526: step 82570, loss = 1.54 (125.5 examples/sec; 1.020 sec/batch)
2018-04-10 09:10:58.538751: step 82580, loss = 1.46 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:11:08.883667: step 82590, loss = 1.42 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 09:11:19.511447: step 82600, loss = 1.50 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 09:11:29.788951: step 82610, loss = 1.64 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:11:40.072234: step 82620, loss = 1.63 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:11:50.376099: step 82630, loss = 1.46 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 09:12:00.658110: step 82640, loss = 1.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:12:11.026964: step 82650, loss = 1.54 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 09:12:21.355136: step 82660, loss = 1.42 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:12:31.640704: step 82670, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:12:41.920422: step 82680, loss = 1.50 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:12:52.204028: step 82690, loss = 1.35 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:13:02.774084: step 82700, loss = 1.49 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 09:13:13.095576: step 82710, loss = 1.39 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:13:23.443759: step 82720, loss = 1.72 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:13:33.744905: step 82730, loss = 1.41 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:13:44.062182: step 82740, loss = 1.73 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 09:13:54.357970: step 82750, loss = 1.50 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:14:04.765313: step 82760, loss = 1.51 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 09:14:15.117684: step 82770, loss = 1.65 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 09:14:25.476064: step 82780, loss = 1.39 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:14:35.719548: step 82790, loss = 1.38 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:14:46.247676: step 82800, loss = 1.54 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 09:14:56.488473: step 82810, loss = 1.38 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:15:06.792503: step 82820, loss = 1.41 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 09:15:17.125862: step 82830, loss = 1.64 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:15:27.416772: step 82840, loss = 1.57 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:15:37.714263: step 82850, loss = 1.68 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:15:47.996121: step 82860, loss = 1.43 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:15:58.288961: step 82870, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:16:08.652420: step 82880, loss = 1.67 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:16:19.012533: step 82890, loss = 1.40 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:16:29.681886: step 82900, loss = 1.54 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 09:16:39.990233: step 82910, loss = 1.44 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:16:50.304931: step 82920, loss = 1.56 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:17:00.618500: step 82930, loss = 1.71 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:17:10.977855: step 82940, loss = 1.56 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:17:21.270194: step 82950, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:17:31.576181: step 82960, loss = 1.56 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:17:41.840307: step 82970, loss = 1.40 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 09:17:52.182387: step 82980, loss = 1.57 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:18:02.513010: step 82990, loss = 1.57 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:18:13.114361: step 83000, loss = 1.66 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 09:18:23.400485: step 83010, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:18:33.729933: step 83020, loss = 1.45 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:18:44.042402: step 83030, loss = 1.41 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:18:54.328912: step 83040, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:19:04.666097: step 83050, loss = 1.35 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:19:15.030005: step 83060, loss = 1.35 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:19:25.322519: step 83070, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:19:35.610233: step 83080, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:19:45.919470: step 83090, loss = 1.60 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:19:56.538898: step 83100, loss = 1.30 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 09:20:07.038671: step 83110, loss = 1.53 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 09:20:17.417100: step 83120, loss = 1.81 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 09:20:27.740860: step 83130, loss = 1.45 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:20:38.168387: step 83140, loss = 1.55 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 09:20:48.359800: step 83150, loss = 1.45 (125.6 examples/sec; 1.019 sec/batch)
2018-04-10 09:20:58.645121: step 83160, loss = 1.50 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:21:08.986597: step 83170, loss = 1.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:21:19.302845: step 83180, loss = 1.33 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 09:21:29.606276: step 83190, loss = 1.62 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 09:21:40.152300: step 83200, loss = 1.57 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 09:21:50.422604: step 83210, loss = 1.38 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:22:00.688122: step 83220, loss = 1.65 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:22:11.000111: step 83230, loss = 1.77 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:22:21.323832: step 83240, loss = 1.68 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:22:31.636495: step 83250, loss = 1.40 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:22:42.102000: step 83260, loss = 1.33 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 09:22:52.552446: step 83270, loss = 1.52 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 09:23:03.200358: step 83280, loss = 1.36 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 09:23:13.710636: step 83290, loss = 1.39 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 09:23:24.457874: step 83300, loss = 1.60 (119.1 examples/sec; 1.075 sec/batch)
2018-04-10 09:23:34.983266: step 83310, loss = 1.69 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 09:23:45.340402: step 83320, loss = 1.52 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:23:55.636356: step 83330, loss = 1.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:24:05.957599: step 83340, loss = 1.41 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:24:16.231576: step 83350, loss = 1.39 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:24:26.510960: step 83360, loss = 1.50 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:24:36.727012: step 83370, loss = 1.30 (125.3 examples/sec; 1.022 sec/batch)
2018-04-10 09:24:46.951916: step 83380, loss = 1.34 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 09:24:57.181666: step 83390, loss = 1.46 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 09:25:07.750074: step 83400, loss = 1.62 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 09:25:18.035339: step 83410, loss = 1.63 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:25:28.322085: step 83420, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:25:38.573835: step 83430, loss = 1.56 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 09:25:48.838293: step 83440, loss = 1.61 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 09:25:59.143802: step 83450, loss = 1.65 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:26:09.506809: step 83460, loss = 1.51 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:26:19.767256: step 83470, loss = 1.60 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 09:26:30.038494: step 83480, loss = 1.41 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:26:40.287388: step 83490, loss = 1.61 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 09:26:50.841874: step 83500, loss = 1.20 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 09:27:01.082514: step 83510, loss = 1.48 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:27:11.399867: step 83520, loss = 1.43 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 09:27:21.736938: step 83530, loss = 1.75 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:27:32.068611: step 83540, loss = 1.54 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:27:42.374091: step 83550, loss = 1.67 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:27:52.697756: step 83560, loss = 1.39 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:28:03.053936: step 83570, loss = 1.58 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:28:13.486352: step 83580, loss = 1.55 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 09:28:23.838833: step 83590, loss = 1.53 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 09:28:34.434968: step 83600, loss = 1.82 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 09:28:44.727073: step 83610, loss = 1.35 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:28:55.038563: step 83620, loss = 1.40 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:29:05.398578: step 83630, loss = 1.42 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:29:15.736107: step 83640, loss = 1.81 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:29:26.043519: step 83650, loss = 1.48 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:29:36.340967: step 83660, loss = 1.62 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:29:46.606720: step 83670, loss = 1.57 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:29:56.946251: step 83680, loss = 1.59 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:30:07.282217: step 83690, loss = 1.34 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:30:17.894354: step 83700, loss = 1.63 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 09:30:28.153250: step 83710, loss = 1.44 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 09:30:38.547174: step 83720, loss = 1.44 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 09:30:48.711239: step 83730, loss = 1.55 (125.9 examples/sec; 1.016 sec/batch)
2018-04-10 09:30:58.979714: step 83740, loss = 1.44 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:31:09.316047: step 83750, loss = 1.39 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:31:19.596481: step 83760, loss = 1.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:31:29.875508: step 83770, loss = 1.56 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:31:40.158220: step 83780, loss = 1.34 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:31:50.385108: step 83790, loss = 1.41 (125.2 examples/sec; 1.023 sec/batch)
2018-04-10 09:32:00.979414: step 83800, loss = 1.56 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 09:32:11.338375: step 83810, loss = 1.54 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:32:21.649311: step 83820, loss = 1.28 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:32:31.956442: step 83830, loss = 1.46 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:32:42.293632: step 83840, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:32:52.573419: step 83850, loss = 1.56 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:33:02.896910: step 83860, loss = 1.61 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:33:13.232179: step 83870, loss = 1.60 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:33:23.491707: step 83880, loss = 1.59 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 09:33:33.758119: step 83890, loss = 1.67 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:33:44.281650: step 83900, loss = 1.38 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 09:33:54.559868: step 83910, loss = 1.41 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:34:04.901572: step 83920, loss = 1.70 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:34:15.194561: step 83930, loss = 1.68 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:34:25.477395: step 83940, loss = 1.42 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:34:35.756203: step 83950, loss = 1.47 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:34:46.036640: step 83960, loss = 1.57 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:34:56.302638: step 83970, loss = 1.38 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 09:35:06.658000: step 83980, loss = 1.43 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:35:16.976738: step 83990, loss = 1.63 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:35:27.572903: step 84000, loss = 1.32 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 09:35:37.909762: step 84010, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:35:48.275531: step 84020, loss = 1.28 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 09:35:58.615207: step 84030, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:36:08.950723: step 84040, loss = 1.59 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:36:19.256389: step 84050, loss = 1.39 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:36:29.585830: step 84060, loss = 1.38 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:36:39.945378: step 84070, loss = 1.32 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:36:50.297817: step 84080, loss = 1.63 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 09:37:00.577395: step 84090, loss = 1.47 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:37:11.323866: step 84100, loss = 1.60 (119.1 examples/sec; 1.075 sec/batch)
2018-04-10 09:37:21.636016: step 84110, loss = 1.67 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:37:31.971204: step 84120, loss = 1.45 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:37:42.270730: step 84130, loss = 1.35 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:37:52.559395: step 84140, loss = 1.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:38:02.880424: step 84150, loss = 1.42 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:38:13.249272: step 84160, loss = 1.38 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 09:38:23.531341: step 84170, loss = 1.44 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:38:33.802770: step 84180, loss = 1.35 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:38:44.079303: step 84190, loss = 1.50 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 09:38:54.661491: step 84200, loss = 1.69 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 09:39:05.053918: step 84210, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:39:15.425647: step 84220, loss = 1.51 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 09:39:25.727141: step 84230, loss = 1.75 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:39:36.118606: step 84240, loss = 1.37 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:39:46.529774: step 84250, loss = 1.57 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 09:39:56.966766: step 84260, loss = 1.61 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 09:40:07.359007: step 84270, loss = 1.44 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:40:17.747871: step 84280, loss = 1.54 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:40:28.084814: step 84290, loss = 1.29 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:40:38.810414: step 84300, loss = 1.51 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 09:40:49.053606: step 84310, loss = 1.65 (125.0 examples/sec; 1.024 sec/batch)
2018-04-10 09:40:59.446828: step 84320, loss = 1.47 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:41:09.850659: step 84330, loss = 1.68 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 09:41:20.189593: step 84340, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:41:30.524851: step 84350, loss = 1.41 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:41:40.880505: step 84360, loss = 1.56 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:41:51.187916: step 84370, loss = 1.17 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:42:01.538401: step 84380, loss = 1.45 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:42:11.861332: step 84390, loss = 1.71 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:42:22.457520: step 84400, loss = 1.44 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 09:42:32.781816: step 84410, loss = 1.37 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:42:43.093335: step 84420, loss = 1.45 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:42:53.387863: step 84430, loss = 1.43 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 09:43:03.709775: step 84440, loss = 1.52 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:43:14.113678: step 84450, loss = 1.49 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 09:43:24.441316: step 84460, loss = 1.47 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:43:34.802165: step 84470, loss = 1.69 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:43:45.128696: step 84480, loss = 1.50 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 09:43:55.478236: step 84490, loss = 1.65 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:44:06.062531: step 84500, loss = 1.30 (120.9 examples/sec; 1.058 sec/batch)
2018-04-10 09:44:16.398187: step 84510, loss = 1.67 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:44:26.702368: step 84520, loss = 1.61 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 09:44:36.963207: step 84530, loss = 1.30 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 09:44:47.269296: step 84540, loss = 1.35 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:44:57.556454: step 84550, loss = 1.66 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:45:07.907105: step 84560, loss = 1.59 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:45:18.223007: step 84570, loss = 1.73 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 09:45:28.537514: step 84580, loss = 1.51 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:45:38.843368: step 84590, loss = 1.48 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:45:49.486790: step 84600, loss = 1.43 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 09:45:59.863891: step 84610, loss = 1.64 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 09:46:10.338243: step 84620, loss = 1.28 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 09:46:20.727811: step 84630, loss = 1.45 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 09:46:31.029494: step 84640, loss = 1.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:46:41.300673: step 84650, loss = 1.44 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:46:51.572174: step 84660, loss = 1.35 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:47:01.869191: step 84670, loss = 1.57 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:47:12.193780: step 84680, loss = 1.42 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:47:22.531640: step 84690, loss = 1.45 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:47:33.200413: step 84700, loss = 1.71 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 09:47:43.560984: step 84710, loss = 1.46 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:47:53.892493: step 84720, loss = 1.41 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:48:04.265968: step 84730, loss = 1.54 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 09:48:14.630905: step 84740, loss = 1.51 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:48:24.955361: step 84750, loss = 1.52 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:48:35.277333: step 84760, loss = 1.60 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:48:45.538123: step 84770, loss = 1.43 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 09:48:55.834449: step 84780, loss = 1.41 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:49:06.173439: step 84790, loss = 1.63 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:49:16.766586: step 84800, loss = 1.60 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 09:49:27.023132: step 84810, loss = 1.40 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 09:49:37.314722: step 84820, loss = 1.65 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:49:47.588066: step 84830, loss = 1.29 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 09:49:57.881254: step 84840, loss = 1.37 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:50:08.261339: step 84850, loss = 1.36 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 09:50:18.599672: step 84860, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:50:28.898664: step 84870, loss = 1.55 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 09:50:39.338053: step 84880, loss = 1.47 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 09:50:49.559807: step 84890, loss = 1.55 (125.2 examples/sec; 1.022 sec/batch)
2018-04-10 09:51:00.169830: step 84900, loss = 1.31 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 09:51:10.523301: step 84910, loss = 1.40 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 09:51:20.858706: step 84920, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:51:31.145810: step 84930, loss = 1.51 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 09:51:41.455709: step 84940, loss = 1.44 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:51:51.779760: step 84950, loss = 1.68 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:52:02.218256: step 84960, loss = 1.44 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 09:52:12.592249: step 84970, loss = 1.33 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 09:52:22.942926: step 84980, loss = 1.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:52:33.260483: step 84990, loss = 1.50 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 09:52:43.915436: step 85000, loss = 1.43 (120.1 examples/sec; 1.065 sec/batch)
2018-04-10 09:52:54.225489: step 85010, loss = 1.67 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:53:04.651100: step 85020, loss = 1.70 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 09:53:15.009833: step 85030, loss = 1.41 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:53:25.350567: step 85040, loss = 1.52 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:53:35.733001: step 85050, loss = 1.41 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 09:53:46.045835: step 85060, loss = 1.52 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 09:53:56.369620: step 85070, loss = 1.45 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:54:07.638873: step 85080, loss = 1.64 (113.6 examples/sec; 1.127 sec/batch)
2018-04-10 09:54:18.012330: step 85090, loss = 1.40 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 09:54:28.620106: step 85100, loss = 1.56 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 09:54:38.953729: step 85110, loss = 1.72 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:54:49.419741: step 85120, loss = 1.34 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 09:54:59.750408: step 85130, loss = 1.39 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:55:10.187468: step 85140, loss = 1.65 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 09:55:20.554078: step 85150, loss = 1.42 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 09:55:30.920826: step 85160, loss = 1.50 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 09:55:41.264244: step 85170, loss = 1.61 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:55:51.622190: step 85180, loss = 1.40 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 09:56:02.059267: step 85190, loss = 1.38 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 09:56:12.731007: step 85200, loss = 1.75 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 09:56:23.014384: step 85210, loss = 1.73 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 09:56:33.354150: step 85220, loss = 1.47 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:56:43.680923: step 85230, loss = 1.53 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 09:56:54.021089: step 85240, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:57:04.431189: step 85250, loss = 1.55 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 09:57:14.834022: step 85260, loss = 1.50 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 09:57:25.172859: step 85270, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 09:57:35.525620: step 85280, loss = 1.44 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 09:57:45.907527: step 85290, loss = 1.54 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 09:57:56.530926: step 85300, loss = 1.50 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 09:58:06.936486: step 85310, loss = 1.51 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 09:58:17.301108: step 85320, loss = 1.30 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 09:58:27.620946: step 85330, loss = 1.31 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:58:37.942139: step 85340, loss = 1.43 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:58:48.249645: step 85350, loss = 1.46 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 09:58:58.595788: step 85360, loss = 1.32 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:59:08.976337: step 85370, loss = 1.61 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 09:59:19.322397: step 85380, loss = 1.48 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 09:59:29.642761: step 85390, loss = 1.71 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 09:59:40.261541: step 85400, loss = 1.26 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 09:59:50.560440: step 85410, loss = 1.52 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:00:00.890134: step 85420, loss = 1.69 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:00:11.278444: step 85430, loss = 1.56 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:00:21.596370: step 85440, loss = 1.58 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:00:31.927994: step 85450, loss = 1.62 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:00:42.401144: step 85460, loss = 1.44 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 10:00:52.698844: step 85470, loss = 1.39 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:01:03.092587: step 85480, loss = 1.39 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:01:13.523892: step 85490, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 10:01:24.163039: step 85500, loss = 1.37 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 10:01:34.473685: step 85510, loss = 1.59 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:01:44.827495: step 85520, loss = 1.47 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:01:55.127802: step 85530, loss = 1.56 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:02:05.538459: step 85540, loss = 1.39 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 10:02:15.959678: step 85550, loss = 1.56 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 10:02:26.327972: step 85560, loss = 1.38 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:02:36.658502: step 85570, loss = 1.68 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:02:47.004238: step 85580, loss = 1.61 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:02:57.303009: step 85590, loss = 1.33 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:03:07.992954: step 85600, loss = 1.61 (119.7 examples/sec; 1.069 sec/batch)
2018-04-10 10:03:18.354201: step 85610, loss = 1.64 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:03:28.656331: step 85620, loss = 1.54 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 10:03:38.948480: step 85630, loss = 1.75 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:03:49.276681: step 85640, loss = 1.43 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:03:59.568653: step 85650, loss = 1.40 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:04:09.928551: step 85660, loss = 1.60 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 10:04:20.239249: step 85670, loss = 1.79 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:04:30.563544: step 85680, loss = 1.56 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:04:40.863659: step 85690, loss = 1.50 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:04:51.421007: step 85700, loss = 1.55 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 10:05:01.738771: step 85710, loss = 1.47 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:05:12.150719: step 85720, loss = 1.59 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 10:05:22.442678: step 85730, loss = 1.52 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:05:32.744018: step 85740, loss = 1.56 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:05:43.029429: step 85750, loss = 1.45 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:05:53.316320: step 85760, loss = 1.56 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:06:03.680665: step 85770, loss = 1.56 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:06:14.047641: step 85780, loss = 1.38 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:06:24.339678: step 85790, loss = 1.72 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:06:34.904196: step 85800, loss = 1.65 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 10:06:45.183691: step 85810, loss = 1.48 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 10:06:55.476818: step 85820, loss = 1.35 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:07:05.847107: step 85830, loss = 1.45 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:07:16.186504: step 85840, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:07:26.455623: step 85850, loss = 1.43 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:07:36.750646: step 85860, loss = 1.35 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:07:47.019295: step 85870, loss = 1.34 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 10:07:57.337389: step 85880, loss = 1.30 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:08:07.683893: step 85890, loss = 1.64 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:08:18.398540: step 85900, loss = 1.53 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 10:08:28.675694: step 85910, loss = 1.49 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 10:08:38.991742: step 85920, loss = 1.59 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:08:49.293531: step 85930, loss = 1.56 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:08:59.613890: step 85940, loss = 1.49 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:09:10.078174: step 85950, loss = 1.50 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 10:09:20.447964: step 85960, loss = 1.47 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:09:30.817611: step 85970, loss = 1.57 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:09:41.225473: step 85980, loss = 1.47 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 10:09:51.547595: step 85990, loss = 1.46 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:10:02.231360: step 86000, loss = 1.55 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 10:10:12.858567: step 86010, loss = 1.75 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 10:10:23.232351: step 86020, loss = 1.42 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:10:33.603996: step 86030, loss = 1.42 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:10:44.039662: step 86040, loss = 1.58 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 10:10:54.409554: step 86050, loss = 1.48 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:11:04.850115: step 86060, loss = 1.41 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 10:11:15.212221: step 86070, loss = 1.46 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:11:25.572702: step 86080, loss = 1.60 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:11:35.911516: step 86090, loss = 1.40 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:11:46.554801: step 86100, loss = 1.50 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 10:11:56.920753: step 86110, loss = 1.49 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:12:07.307615: step 86120, loss = 1.45 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:12:17.692536: step 86130, loss = 1.56 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:12:28.057381: step 86140, loss = 1.61 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:12:38.410314: step 86150, loss = 1.49 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:12:48.779438: step 86160, loss = 1.38 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:12:59.204599: step 86170, loss = 1.35 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 10:13:09.632568: step 86180, loss = 1.53 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 10:13:20.081989: step 86190, loss = 1.32 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 10:13:30.718607: step 86200, loss = 1.64 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 10:13:41.048549: step 86210, loss = 1.38 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:13:51.413033: step 86220, loss = 1.46 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:14:01.806495: step 86230, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:14:12.252890: step 86240, loss = 1.47 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 10:14:22.631629: step 86250, loss = 1.39 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:14:33.007123: step 86260, loss = 1.66 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 10:14:43.339450: step 86270, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:14:53.677987: step 86280, loss = 1.41 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:15:04.057613: step 86290, loss = 1.55 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:15:14.715465: step 86300, loss = 1.53 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 10:15:25.071113: step 86310, loss = 1.40 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 10:15:35.443103: step 86320, loss = 1.42 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:15:45.798887: step 86330, loss = 1.59 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 10:15:56.154001: step 86340, loss = 1.56 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 10:16:06.559607: step 86350, loss = 1.58 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 10:16:16.963533: step 86360, loss = 1.48 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 10:16:27.324186: step 86370, loss = 1.48 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:16:37.752624: step 86380, loss = 1.55 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 10:16:48.120385: step 86390, loss = 1.58 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:16:58.734950: step 86400, loss = 1.40 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 10:17:09.125571: step 86410, loss = 1.68 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:17:19.548408: step 86420, loss = 1.63 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 10:17:29.901729: step 86430, loss = 1.57 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:17:40.267685: step 86440, loss = 1.52 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:17:50.712470: step 86450, loss = 1.47 (122.5 examples/sec; 1.044 sec/batch)
2018-04-10 10:18:01.091864: step 86460, loss = 1.62 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:18:11.550457: step 86470, loss = 1.64 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 10:18:21.921108: step 86480, loss = 1.51 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:18:32.346113: step 86490, loss = 1.20 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 10:18:42.954342: step 86500, loss = 1.57 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 10:18:53.280098: step 86510, loss = 1.34 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 10:19:03.670185: step 86520, loss = 1.60 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:19:14.034638: step 86530, loss = 1.33 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:19:24.450134: step 86540, loss = 1.42 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 10:19:34.793990: step 86550, loss = 1.58 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 10:19:45.183942: step 86560, loss = 1.52 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:19:55.561372: step 86570, loss = 1.48 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:20:05.977328: step 86580, loss = 1.60 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 10:20:16.408330: step 86590, loss = 1.62 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 10:20:27.127627: step 86600, loss = 1.41 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 10:20:37.534584: step 86610, loss = 1.54 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 10:20:48.077902: step 86620, loss = 1.26 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 10:20:58.482803: step 86630, loss = 1.55 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 10:21:08.934347: step 86640, loss = 1.68 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 10:21:19.272166: step 86650, loss = 1.45 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:21:29.573086: step 86660, loss = 1.44 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:21:39.864363: step 86670, loss = 1.50 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:21:50.125174: step 86680, loss = 1.58 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 10:22:00.412119: step 86690, loss = 1.37 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:22:11.022332: step 86700, loss = 1.60 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 10:22:21.297701: step 86710, loss = 1.56 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 10:22:31.623198: step 86720, loss = 1.53 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 10:22:42.046067: step 86730, loss = 1.47 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 10:22:52.391426: step 86740, loss = 1.46 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:23:02.823539: step 86750, loss = 1.45 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 10:23:13.228365: step 86760, loss = 1.47 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 10:23:23.531534: step 86770, loss = 1.63 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 10:23:33.880572: step 86780, loss = 1.32 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:23:44.184597: step 86790, loss = 1.45 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 10:23:54.760239: step 86800, loss = 1.53 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 10:24:05.074907: step 86810, loss = 1.47 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:24:15.412442: step 86820, loss = 1.42 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:24:25.746894: step 86830, loss = 1.67 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:24:36.110507: step 86840, loss = 1.42 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:24:46.442789: step 86850, loss = 1.46 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:24:56.770649: step 86860, loss = 1.51 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:25:07.107109: step 86870, loss = 1.50 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:25:17.436828: step 86880, loss = 1.40 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:25:27.760630: step 86890, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:25:38.325150: step 86900, loss = 1.48 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 10:25:48.599094: step 86910, loss = 1.34 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:25:58.934774: step 86920, loss = 1.59 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:26:09.297558: step 86930, loss = 1.45 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:26:19.608173: step 86940, loss = 1.48 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:26:29.915570: step 86950, loss = 1.68 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:26:40.216656: step 86960, loss = 1.48 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:26:50.484560: step 86970, loss = 1.56 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 10:27:00.782153: step 86980, loss = 1.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:27:11.126290: step 86990, loss = 1.37 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 10:27:21.708691: step 87000, loss = 1.40 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 10:27:32.013847: step 87010, loss = 1.45 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:27:42.318347: step 87020, loss = 1.56 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 10:27:52.605733: step 87030, loss = 1.50 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:28:02.971984: step 87040, loss = 1.60 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:28:13.300170: step 87050, loss = 1.51 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:28:23.571035: step 87060, loss = 1.47 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:28:33.857584: step 87070, loss = 1.44 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:28:44.131079: step 87080, loss = 1.74 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:28:54.379482: step 87090, loss = 1.58 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 10:29:04.978580: step 87100, loss = 1.45 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 10:29:15.253144: step 87110, loss = 1.60 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:29:25.517832: step 87120, loss = 1.37 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 10:29:35.829872: step 87130, loss = 1.26 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:29:46.099991: step 87140, loss = 1.46 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:29:56.369028: step 87150, loss = 1.69 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:30:06.727760: step 87160, loss = 1.37 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 10:30:17.030543: step 87170, loss = 1.58 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 10:30:27.304238: step 87180, loss = 1.56 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:30:37.589754: step 87190, loss = 1.25 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:30:48.207114: step 87200, loss = 1.56 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 10:30:58.538422: step 87210, loss = 1.50 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:31:08.898370: step 87220, loss = 1.50 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 10:31:19.207178: step 87230, loss = 1.48 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:31:29.506540: step 87240, loss = 1.51 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:31:39.791832: step 87250, loss = 1.43 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:31:50.041943: step 87260, loss = 1.49 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 10:32:00.295607: step 87270, loss = 1.50 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 10:32:10.606409: step 87280, loss = 1.25 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:32:20.874860: step 87290, loss = 1.48 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 10:32:31.394620: step 87300, loss = 1.44 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 10:32:41.652453: step 87310, loss = 1.58 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 10:32:51.912151: step 87320, loss = 1.61 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 10:33:02.185076: step 87330, loss = 1.43 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:33:12.514462: step 87340, loss = 1.54 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:33:22.808697: step 87350, loss = 1.57 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 10:33:33.080636: step 87360, loss = 1.68 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:33:43.367777: step 87370, loss = 1.43 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:33:53.661571: step 87380, loss = 1.46 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 10:34:03.951841: step 87390, loss = 1.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:34:14.579189: step 87400, loss = 1.54 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 10:34:24.872538: step 87410, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:34:35.153066: step 87420, loss = 1.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 10:34:45.447539: step 87430, loss = 1.53 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 10:34:55.716099: step 87440, loss = 1.29 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 10:35:06.105956: step 87450, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:35:16.455953: step 87460, loss = 1.43 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:35:26.790529: step 87470, loss = 1.38 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:35:37.133386: step 87480, loss = 1.67 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:35:47.469283: step 87490, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:35:58.104237: step 87500, loss = 1.52 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 10:36:08.502197: step 87510, loss = 1.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 10:36:18.892191: step 87520, loss = 1.44 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:36:29.227125: step 87530, loss = 1.31 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:36:39.599453: step 87540, loss = 1.50 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:36:49.932549: step 87550, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:37:00.274877: step 87560, loss = 1.57 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:37:10.664826: step 87570, loss = 1.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:37:21.007504: step 87580, loss = 1.56 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:37:31.338101: step 87590, loss = 1.51 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:37:41.955363: step 87600, loss = 1.42 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 10:37:52.269690: step 87610, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:38:02.690189: step 87620, loss = 1.63 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 10:38:13.060432: step 87630, loss = 1.54 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:38:23.398221: step 87640, loss = 1.48 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:38:33.740820: step 87650, loss = 1.66 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:38:44.120586: step 87660, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:38:54.443813: step 87670, loss = 1.37 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:39:04.877044: step 87680, loss = 1.35 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 10:39:15.249812: step 87690, loss = 1.39 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:39:25.870020: step 87700, loss = 1.60 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 10:39:36.205023: step 87710, loss = 1.46 (123.9 examples/sec; 1.034 sec/batch)
2018-04-10 10:39:46.558430: step 87720, loss = 1.21 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:39:56.891737: step 87730, loss = 1.48 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:40:07.330785: step 87740, loss = 1.51 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 10:40:17.720023: step 87750, loss = 1.19 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:40:28.098923: step 87760, loss = 1.27 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:40:38.442332: step 87770, loss = 1.43 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:40:48.936714: step 87780, loss = 1.45 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 10:40:59.354760: step 87790, loss = 1.71 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 10:41:10.065957: step 87800, loss = 1.43 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 10:41:20.418865: step 87810, loss = 1.33 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:41:30.805865: step 87820, loss = 1.40 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:41:41.178845: step 87830, loss = 1.74 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:41:51.541254: step 87840, loss = 1.74 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:42:01.921200: step 87850, loss = 1.37 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:42:12.320531: step 87860, loss = 1.60 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 10:42:22.614623: step 87870, loss = 1.53 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 10:42:32.932200: step 87880, loss = 1.30 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:42:43.223363: step 87890, loss = 1.41 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:42:53.770436: step 87900, loss = 1.32 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 10:43:04.083459: step 87910, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:43:14.402062: step 87920, loss = 1.56 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:43:24.712500: step 87930, loss = 1.56 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:43:35.019122: step 87940, loss = 1.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:43:45.309952: step 87950, loss = 1.55 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:43:55.627222: step 87960, loss = 1.62 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:44:05.993863: step 87970, loss = 1.47 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:44:16.362260: step 87980, loss = 1.58 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:44:26.714620: step 87990, loss = 1.41 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:44:37.326873: step 88000, loss = 1.59 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 10:44:47.663900: step 88010, loss = 1.53 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:44:57.983852: step 88020, loss = 1.55 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:45:08.344410: step 88030, loss = 1.37 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:45:18.687702: step 88040, loss = 1.32 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:45:29.011013: step 88050, loss = 1.62 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:45:39.382789: step 88060, loss = 1.44 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:45:49.792455: step 88070, loss = 1.29 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 10:46:00.090429: step 88080, loss = 1.42 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:46:10.458678: step 88090, loss = 1.50 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:46:21.051625: step 88100, loss = 1.49 (120.8 examples/sec; 1.059 sec/batch)
2018-04-10 10:46:31.388377: step 88110, loss = 1.43 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:46:41.756275: step 88120, loss = 1.46 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 10:46:52.081740: step 88130, loss = 1.60 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 10:47:02.443542: step 88140, loss = 1.64 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:47:12.784935: step 88150, loss = 1.69 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:47:23.086399: step 88160, loss = 1.57 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:47:33.406636: step 88170, loss = 1.55 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:47:43.705111: step 88180, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:47:53.993021: step 88190, loss = 1.53 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:48:04.601191: step 88200, loss = 1.56 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 10:48:14.914234: step 88210, loss = 1.58 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:48:25.262110: step 88220, loss = 1.52 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:48:35.561623: step 88230, loss = 1.63 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:48:45.874157: step 88240, loss = 1.43 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:48:56.162033: step 88250, loss = 1.49 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:49:06.546196: step 88260, loss = 1.58 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:49:16.910749: step 88270, loss = 1.52 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 10:49:27.233407: step 88280, loss = 1.43 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:49:37.533736: step 88290, loss = 1.60 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:49:48.109941: step 88300, loss = 1.56 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 10:49:58.415187: step 88310, loss = 1.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:50:08.820114: step 88320, loss = 1.46 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 10:50:19.174922: step 88330, loss = 1.45 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:50:29.489309: step 88340, loss = 1.66 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:50:39.789238: step 88350, loss = 1.54 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:50:50.203253: step 88360, loss = 1.51 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 10:51:00.520358: step 88370, loss = 1.40 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 10:51:10.975217: step 88380, loss = 1.45 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 10:51:21.361226: step 88390, loss = 1.66 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 10:51:31.960713: step 88400, loss = 1.58 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 10:51:42.329345: step 88410, loss = 1.30 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 10:51:52.652138: step 88420, loss = 1.71 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:52:02.974716: step 88430, loss = 1.58 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:52:13.323554: step 88440, loss = 1.42 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 10:52:23.621371: step 88450, loss = 1.58 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:52:33.932819: step 88460, loss = 1.64 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:52:44.185043: step 88470, loss = 1.42 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 10:52:54.470846: step 88480, loss = 1.56 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:53:04.799202: step 88490, loss = 1.26 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:53:15.390678: step 88500, loss = 1.37 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 10:53:25.677420: step 88510, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:53:36.055581: step 88520, loss = 1.58 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:53:46.309529: step 88530, loss = 1.49 (124.8 examples/sec; 1.025 sec/batch)
2018-04-10 10:53:56.601291: step 88540, loss = 1.47 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:54:06.956122: step 88550, loss = 1.58 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 10:54:17.299387: step 88560, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:54:27.585430: step 88570, loss = 1.46 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:54:37.876778: step 88580, loss = 1.61 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:54:48.160932: step 88590, loss = 1.45 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 10:54:58.701520: step 88600, loss = 1.31 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 10:55:09.022576: step 88610, loss = 1.44 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:55:19.321635: step 88620, loss = 1.36 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:55:29.592946: step 88630, loss = 1.54 (124.6 examples/sec; 1.027 sec/batch)
2018-04-10 10:55:39.913565: step 88640, loss = 1.51 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 10:55:50.214274: step 88650, loss = 1.33 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:56:00.557730: step 88660, loss = 1.43 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 10:56:10.893354: step 88670, loss = 1.55 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:56:21.170264: step 88680, loss = 1.73 (124.6 examples/sec; 1.028 sec/batch)
2018-04-10 10:56:31.421357: step 88690, loss = 1.57 (124.9 examples/sec; 1.025 sec/batch)
2018-04-10 10:56:41.940861: step 88700, loss = 1.56 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 10:56:52.171573: step 88710, loss = 1.54 (125.1 examples/sec; 1.023 sec/batch)
2018-04-10 10:57:02.478527: step 88720, loss = 1.75 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:57:12.860144: step 88730, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 10:57:23.164954: step 88740, loss = 1.40 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 10:57:33.478288: step 88750, loss = 1.42 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:57:43.760113: step 88760, loss = 1.62 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 10:57:54.096630: step 88770, loss = 1.42 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 10:58:04.406597: step 88780, loss = 1.47 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 10:58:14.731652: step 88790, loss = 1.55 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 10:58:25.268620: step 88800, loss = 1.59 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 10:58:35.537300: step 88810, loss = 1.57 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 10:58:45.826901: step 88820, loss = 1.62 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 10:58:56.086358: step 88830, loss = 1.50 (124.8 examples/sec; 1.026 sec/batch)
2018-04-10 10:59:06.419833: step 88840, loss = 1.52 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 10:59:16.796500: step 88850, loss = 1.48 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 10:59:27.108225: step 88860, loss = 1.60 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 10:59:37.403222: step 88870, loss = 1.43 (124.3 examples/sec; 1.029 sec/batch)
2018-04-10 10:59:47.702514: step 88880, loss = 1.53 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 10:59:57.989179: step 88890, loss = 1.48 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 11:00:08.688892: step 88900, loss = 1.40 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 11:00:19.062386: step 88910, loss = 1.74 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:00:29.366226: step 88920, loss = 1.71 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 11:00:39.652063: step 88930, loss = 1.37 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 11:00:50.034936: step 88940, loss = 1.36 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:01:00.383365: step 88950, loss = 1.47 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:01:10.788682: step 88960, loss = 1.31 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:01:21.165062: step 88970, loss = 1.47 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 11:01:31.514134: step 88980, loss = 1.75 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:01:41.842244: step 88990, loss = 1.48 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:01:52.431085: step 89000, loss = 1.38 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 11:02:02.806990: step 89010, loss = 1.67 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 11:02:13.193992: step 89020, loss = 1.63 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:02:23.507394: step 89030, loss = 1.44 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 11:02:33.825327: step 89040, loss = 1.44 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 11:02:44.159684: step 89050, loss = 1.61 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:02:54.495907: step 89060, loss = 1.37 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:03:04.854759: step 89070, loss = 1.67 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:03:15.286323: step 89080, loss = 1.45 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:03:25.691185: step 89090, loss = 1.23 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 11:03:36.278638: step 89100, loss = 1.35 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 11:03:46.628013: step 89110, loss = 1.44 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:03:56.942406: step 89120, loss = 1.53 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 11:04:07.293496: step 89130, loss = 1.66 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:04:17.672380: step 89140, loss = 1.48 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:04:28.003015: step 89150, loss = 1.42 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:04:38.351746: step 89160, loss = 1.66 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:04:48.671081: step 89170, loss = 1.74 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 11:04:59.014192: step 89180, loss = 1.48 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:05:09.430121: step 89190, loss = 1.51 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:05:20.062352: step 89200, loss = 1.54 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 11:05:30.372720: step 89210, loss = 1.42 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 11:05:40.698187: step 89220, loss = 1.55 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 11:05:50.988310: step 89230, loss = 1.37 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 11:06:01.348519: step 89240, loss = 1.60 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:06:11.740724: step 89250, loss = 1.68 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:06:22.108412: step 89260, loss = 1.56 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:06:32.430022: step 89270, loss = 1.47 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 11:06:42.750806: step 89280, loss = 1.71 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 11:06:53.043762: step 89290, loss = 1.48 (124.4 examples/sec; 1.029 sec/batch)
2018-04-10 11:07:03.649303: step 89300, loss = 1.51 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 11:07:13.978132: step 89310, loss = 1.44 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:07:24.318204: step 89320, loss = 1.61 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:07:34.680650: step 89330, loss = 1.42 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:07:45.139236: step 89340, loss = 1.60 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 11:07:55.540108: step 89350, loss = 1.31 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:08:06.044720: step 89360, loss = 1.61 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 11:08:16.496742: step 89370, loss = 1.71 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:08:26.928339: step 89380, loss = 1.32 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:08:37.352243: step 89390, loss = 1.43 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:08:48.068915: step 89400, loss = 1.44 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 11:08:58.476260: step 89410, loss = 1.38 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:09:08.991216: step 89420, loss = 1.38 (121.7 examples/sec; 1.051 sec/batch)
2018-04-10 11:09:19.462717: step 89430, loss = 1.54 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 11:09:29.889330: step 89440, loss = 1.28 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 11:09:40.307427: step 89450, loss = 1.40 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:09:50.766964: step 89460, loss = 1.50 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 11:10:01.152432: step 89470, loss = 1.52 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:10:11.603225: step 89480, loss = 1.32 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:10:22.053515: step 89490, loss = 1.45 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:10:32.679978: step 89500, loss = 1.53 (120.5 examples/sec; 1.063 sec/batch)
2018-04-10 11:10:43.019597: step 89510, loss = 1.47 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:10:53.465794: step 89520, loss = 1.47 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:11:03.985886: step 89530, loss = 1.63 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 11:11:14.414356: step 89540, loss = 1.75 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:11:24.819875: step 89550, loss = 1.60 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:11:35.240369: step 89560, loss = 1.36 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:11:45.628600: step 89570, loss = 1.42 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:11:56.074604: step 89580, loss = 1.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:12:06.507843: step 89590, loss = 1.38 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:12:17.206888: step 89600, loss = 1.41 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 11:12:27.630063: step 89610, loss = 1.63 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:12:38.052843: step 89620, loss = 1.56 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:12:48.470374: step 89630, loss = 1.33 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:12:58.910980: step 89640, loss = 1.58 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:13:09.355466: step 89650, loss = 1.47 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:13:19.775185: step 89660, loss = 1.34 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:13:30.158774: step 89670, loss = 1.42 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:13:40.587723: step 89680, loss = 1.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:13:51.045081: step 89690, loss = 1.58 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 11:14:01.810479: step 89700, loss = 1.41 (118.9 examples/sec; 1.077 sec/batch)
2018-04-10 11:14:12.259647: step 89710, loss = 1.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:14:22.745711: step 89720, loss = 1.61 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 11:14:33.170105: step 89730, loss = 1.67 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:14:43.598534: step 89740, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:14:53.986589: step 89750, loss = 1.30 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:15:04.376289: step 89760, loss = 1.57 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:15:14.777661: step 89770, loss = 1.38 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:15:25.174224: step 89780, loss = 1.46 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:15:35.542378: step 89790, loss = 1.55 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:15:46.217242: step 89800, loss = 1.45 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 11:15:56.594402: step 89810, loss = 1.54 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:16:07.062969: step 89820, loss = 1.61 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 11:16:17.476270: step 89830, loss = 1.55 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 11:16:27.866065: step 89840, loss = 1.56 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:16:38.265550: step 89850, loss = 1.48 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:16:48.683773: step 89860, loss = 1.49 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:16:59.107815: step 89870, loss = 1.34 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:17:09.577825: step 89880, loss = 1.61 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 11:17:20.026255: step 89890, loss = 1.64 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:17:30.710300: step 89900, loss = 1.48 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 11:17:41.078745: step 89910, loss = 1.30 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:17:51.569091: step 89920, loss = 1.66 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 11:18:02.131859: step 89930, loss = 1.24 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 11:18:12.718363: step 89940, loss = 1.36 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 11:18:23.275720: step 89950, loss = 1.51 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 11:18:33.804843: step 89960, loss = 1.47 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 11:18:44.329018: step 89970, loss = 1.52 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 11:18:54.898350: step 89980, loss = 1.58 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 11:19:05.516385: step 89990, loss = 1.58 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 11:19:16.355346: step 90000, loss = 1.50 (118.1 examples/sec; 1.084 sec/batch)
2018-04-10 11:19:26.867152: step 90010, loss = 1.44 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 11:19:37.203161: step 90020, loss = 1.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:19:47.577434: step 90030, loss = 1.38 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:19:57.926947: step 90040, loss = 1.45 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:20:08.324536: step 90050, loss = 1.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:20:18.690975: step 90060, loss = 1.55 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:20:29.020530: step 90070, loss = 1.49 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:20:39.361184: step 90080, loss = 1.65 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:20:49.780837: step 90090, loss = 1.46 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:21:00.434357: step 90100, loss = 1.50 (120.1 examples/sec; 1.065 sec/batch)
2018-04-10 11:21:10.828595: step 90110, loss = 1.66 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 11:21:21.211623: step 90120, loss = 1.27 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:21:31.573281: step 90130, loss = 1.62 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:21:41.924687: step 90140, loss = 1.57 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:21:52.271088: step 90150, loss = 1.38 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:22:02.682352: step 90160, loss = 1.34 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 11:22:13.052575: step 90170, loss = 1.52 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:22:23.394700: step 90180, loss = 1.42 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:22:33.723291: step 90190, loss = 1.31 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:22:44.345984: step 90200, loss = 1.64 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 11:22:54.672182: step 90210, loss = 1.43 (124.0 examples/sec; 1.033 sec/batch)
2018-04-10 11:23:05.050483: step 90220, loss = 1.48 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:23:15.354562: step 90230, loss = 1.47 (124.2 examples/sec; 1.030 sec/batch)
2018-04-10 11:23:25.677832: step 90240, loss = 1.61 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 11:23:36.009398: step 90250, loss = 1.59 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:23:46.377760: step 90260, loss = 1.56 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:23:56.775361: step 90270, loss = 1.64 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:24:07.272191: step 90280, loss = 1.48 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 11:24:17.684040: step 90290, loss = 1.42 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 11:24:28.352950: step 90300, loss = 1.71 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 11:24:38.717851: step 90310, loss = 1.36 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:24:49.084847: step 90320, loss = 1.38 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:24:59.463893: step 90330, loss = 1.62 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:25:09.953832: step 90340, loss = 1.41 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 11:25:20.342706: step 90350, loss = 1.42 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:25:30.711339: step 90360, loss = 1.57 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:25:41.059495: step 90370, loss = 1.44 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:25:51.417456: step 90380, loss = 1.76 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:26:01.881151: step 90390, loss = 1.41 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 11:26:12.613348: step 90400, loss = 1.56 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 11:26:22.990271: step 90410, loss = 1.44 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 11:26:33.424452: step 90420, loss = 1.56 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:26:43.811661: step 90430, loss = 1.66 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:26:54.192185: step 90440, loss = 1.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:27:04.581060: step 90450, loss = 1.62 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:27:15.014426: step 90460, loss = 1.57 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:27:25.344249: step 90470, loss = 1.39 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:27:35.703852: step 90480, loss = 1.51 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:27:46.165081: step 90490, loss = 1.63 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 11:27:57.110976: step 90500, loss = 1.49 (116.9 examples/sec; 1.095 sec/batch)
2018-04-10 11:28:07.827037: step 90510, loss = 1.67 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 11:28:18.329208: step 90520, loss = 1.60 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 11:28:28.714062: step 90530, loss = 1.21 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:28:39.097044: step 90540, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:28:49.483889: step 90550, loss = 1.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:28:59.888826: step 90560, loss = 1.59 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 11:29:10.327177: step 90570, loss = 1.70 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:29:20.751226: step 90580, loss = 1.47 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:29:31.149139: step 90590, loss = 1.40 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:29:41.884831: step 90600, loss = 1.48 (119.2 examples/sec; 1.074 sec/batch)
2018-04-10 11:29:52.299722: step 90610, loss = 1.53 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 11:30:02.814975: step 90620, loss = 1.39 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 11:30:13.297247: step 90630, loss = 1.59 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 11:30:23.763005: step 90640, loss = 1.42 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 11:30:34.212649: step 90650, loss = 1.55 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:30:44.691032: step 90660, loss = 1.69 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 11:30:55.227828: step 90670, loss = 1.45 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 11:31:05.768645: step 90680, loss = 1.53 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 11:31:16.246652: step 90690, loss = 1.49 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 11:31:26.964721: step 90700, loss = 1.57 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 11:31:37.382038: step 90710, loss = 1.59 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:31:47.841411: step 90720, loss = 1.52 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 11:31:58.276347: step 90730, loss = 1.66 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:32:08.782022: step 90740, loss = 1.64 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 11:32:19.246398: step 90750, loss = 1.45 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 11:32:29.688555: step 90760, loss = 1.58 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:32:40.094352: step 90770, loss = 1.60 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:32:50.532745: step 90780, loss = 1.28 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:33:00.970921: step 90790, loss = 1.67 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:33:11.715041: step 90800, loss = 1.68 (119.1 examples/sec; 1.074 sec/batch)
2018-04-10 11:33:22.159064: step 90810, loss = 1.55 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:33:32.621661: step 90820, loss = 1.48 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 11:33:43.057473: step 90830, loss = 1.57 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 11:33:53.529014: step 90840, loss = 1.27 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 11:34:03.956548: step 90850, loss = 1.48 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 11:34:14.394174: step 90860, loss = 1.66 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:34:24.770655: step 90870, loss = 1.46 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 11:34:35.105855: step 90880, loss = 1.32 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:34:45.417634: step 90890, loss = 1.51 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 11:34:56.038597: step 90900, loss = 1.60 (120.5 examples/sec; 1.062 sec/batch)
2018-04-10 11:35:06.420560: step 90910, loss = 1.41 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:35:16.810241: step 90920, loss = 1.72 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:35:27.167357: step 90930, loss = 1.52 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:35:37.514000: step 90940, loss = 1.51 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:35:47.836914: step 90950, loss = 1.52 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 11:35:58.148966: step 90960, loss = 1.47 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 11:36:08.542611: step 90970, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:36:18.841548: step 90980, loss = 1.42 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 11:36:29.109915: step 90990, loss = 1.63 (124.7 examples/sec; 1.027 sec/batch)
2018-04-10 11:36:39.722781: step 91000, loss = 1.40 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 11:36:50.032236: step 91010, loss = 1.43 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 11:37:00.390499: step 91020, loss = 1.49 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:37:10.789053: step 91030, loss = 1.27 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:37:21.138746: step 91040, loss = 1.41 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:37:31.480156: step 91050, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:37:41.841593: step 91060, loss = 1.31 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:37:52.189195: step 91070, loss = 1.42 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:38:02.602383: step 91080, loss = 1.42 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 11:38:12.991559: step 91090, loss = 1.35 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:38:23.648215: step 91100, loss = 1.45 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 11:38:34.004630: step 91110, loss = 1.47 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:38:44.387920: step 91120, loss = 1.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:38:54.755706: step 91130, loss = 1.51 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 11:39:05.270999: step 91140, loss = 1.45 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 11:39:15.896159: step 91150, loss = 1.55 (120.5 examples/sec; 1.063 sec/batch)
2018-04-10 11:39:26.286724: step 91160, loss = 1.75 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:39:36.695449: step 91170, loss = 1.61 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:39:47.100033: step 91180, loss = 1.48 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 11:39:57.509414: step 91190, loss = 1.43 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:40:08.286460: step 91200, loss = 1.53 (118.8 examples/sec; 1.078 sec/batch)
2018-04-10 11:40:18.673600: step 91210, loss = 1.30 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:40:29.044673: step 91220, loss = 1.41 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:40:39.394934: step 91230, loss = 1.80 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:40:49.859756: step 91240, loss = 1.45 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 11:41:00.240197: step 91250, loss = 1.31 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:41:10.684488: step 91260, loss = 1.49 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:41:21.054928: step 91270, loss = 1.56 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:41:31.407110: step 91280, loss = 1.52 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:41:41.747243: step 91290, loss = 1.34 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:41:52.346987: step 91300, loss = 1.37 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 11:42:02.699743: step 91310, loss = 1.57 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:42:13.121216: step 91320, loss = 1.26 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:42:23.475644: step 91330, loss = 1.50 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:42:33.847856: step 91340, loss = 1.49 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:42:44.204045: step 91350, loss = 1.39 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:42:54.559917: step 91360, loss = 1.85 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:43:04.934645: step 91370, loss = 1.36 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:43:15.363370: step 91380, loss = 1.51 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:43:25.716542: step 91390, loss = 1.56 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:43:36.328300: step 91400, loss = 1.53 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 11:43:46.691115: step 91410, loss = 1.63 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:43:57.041453: step 91420, loss = 1.49 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:44:07.433543: step 91430, loss = 1.62 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:44:17.827341: step 91440, loss = 1.31 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:44:28.296547: step 91450, loss = 1.43 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 11:44:38.654041: step 91460, loss = 1.32 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:44:49.006011: step 91470, loss = 1.41 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:44:59.370941: step 91480, loss = 1.36 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:45:09.786769: step 91490, loss = 1.39 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:45:20.422780: step 91500, loss = 1.43 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 11:45:30.746826: step 91510, loss = 1.38 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 11:45:41.117139: step 91520, loss = 1.46 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:45:51.461305: step 91530, loss = 1.31 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 11:46:01.853221: step 91540, loss = 1.56 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:46:12.270313: step 91550, loss = 1.73 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:46:22.706040: step 91560, loss = 1.35 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 11:46:33.089150: step 91570, loss = 1.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:46:43.432826: step 91580, loss = 1.49 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 11:46:53.763442: step 91590, loss = 1.40 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:47:04.410930: step 91600, loss = 1.41 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 11:47:14.796806: step 91610, loss = 1.63 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:47:25.155844: step 91620, loss = 1.60 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:47:35.527276: step 91630, loss = 1.33 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:47:45.913067: step 91640, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:47:56.272177: step 91650, loss = 1.27 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:48:06.634558: step 91660, loss = 1.50 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:48:16.950238: step 91670, loss = 1.46 (124.1 examples/sec; 1.032 sec/batch)
2018-04-10 11:48:27.227242: step 91680, loss = 1.54 (124.5 examples/sec; 1.028 sec/batch)
2018-04-10 11:48:37.490300: step 91690, loss = 1.65 (124.7 examples/sec; 1.026 sec/batch)
2018-04-10 11:48:48.103559: step 91700, loss = 1.50 (120.6 examples/sec; 1.061 sec/batch)
2018-04-10 11:48:58.440286: step 91710, loss = 1.44 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:49:08.841433: step 91720, loss = 1.35 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:49:19.236286: step 91730, loss = 1.48 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 11:49:29.636455: step 91740, loss = 1.64 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:49:39.990242: step 91750, loss = 1.27 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:49:50.353678: step 91760, loss = 1.18 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:50:00.785819: step 91770, loss = 1.55 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:50:11.359137: step 91780, loss = 1.70 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 11:50:21.781446: step 91790, loss = 1.56 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 11:50:32.382573: step 91800, loss = 1.42 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 11:50:42.727720: step 91810, loss = 1.33 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:50:53.200755: step 91820, loss = 1.57 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 11:51:03.618838: step 91830, loss = 1.70 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 11:51:14.032160: step 91840, loss = 1.39 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 11:51:24.390500: step 91850, loss = 1.62 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 11:51:34.750858: step 91860, loss = 1.61 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:51:45.092508: step 91870, loss = 1.63 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:51:55.440550: step 91880, loss = 1.52 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:52:05.817911: step 91890, loss = 1.60 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:52:16.478200: step 91900, loss = 1.51 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 11:52:26.815320: step 91910, loss = 1.51 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:52:37.143174: step 91920, loss = 1.49 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:52:47.480672: step 91930, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:52:57.833253: step 91940, loss = 1.36 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:53:08.231788: step 91950, loss = 1.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 11:53:18.621013: step 91960, loss = 1.38 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:53:28.933910: step 91970, loss = 1.51 (124.1 examples/sec; 1.031 sec/batch)
2018-04-10 11:53:39.262125: step 91980, loss = 1.49 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:53:49.593473: step 91990, loss = 1.48 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:54:00.202373: step 92000, loss = 1.35 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 11:54:10.650554: step 92010, loss = 1.70 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:54:21.079159: step 92020, loss = 1.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:54:31.462975: step 92030, loss = 1.47 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:54:41.857315: step 92040, loss = 1.62 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 11:54:52.226062: step 92050, loss = 1.67 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:55:02.697948: step 92060, loss = 1.44 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 11:55:13.131807: step 92070, loss = 1.39 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 11:55:23.539484: step 92080, loss = 1.30 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:55:33.903922: step 92090, loss = 1.23 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 11:55:44.506173: step 92100, loss = 1.52 (120.7 examples/sec; 1.060 sec/batch)
2018-04-10 11:55:54.815116: step 92110, loss = 1.51 (124.2 examples/sec; 1.031 sec/batch)
2018-04-10 11:56:05.253846: step 92120, loss = 1.30 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:56:15.640445: step 92130, loss = 1.43 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:56:26.032358: step 92140, loss = 1.44 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 11:56:36.383699: step 92150, loss = 1.58 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 11:56:46.752556: step 92160, loss = 1.51 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:56:57.082017: step 92170, loss = 1.16 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 11:57:07.450971: step 92180, loss = 1.44 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 11:57:17.833877: step 92190, loss = 1.36 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:57:28.472598: step 92200, loss = 1.77 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 11:57:38.807839: step 92210, loss = 1.48 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:57:49.147744: step 92220, loss = 1.67 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 11:57:59.500045: step 92230, loss = 1.54 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 11:58:09.908472: step 92240, loss = 1.49 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:58:20.289141: step 92250, loss = 1.38 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:58:30.672964: step 92260, loss = 1.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:58:41.052214: step 92270, loss = 1.63 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 11:58:51.477950: step 92280, loss = 1.57 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 11:59:01.924134: step 92290, loss = 1.30 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 11:59:12.657149: step 92300, loss = 1.51 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 11:59:23.062196: step 92310, loss = 1.39 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 11:59:33.506871: step 92320, loss = 1.41 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 11:59:43.934191: step 92330, loss = 1.53 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 11:59:54.339744: step 92340, loss = 1.42 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:00:04.792662: step 92350, loss = 1.42 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:00:15.290771: step 92360, loss = 1.41 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 12:00:25.735476: step 92370, loss = 1.35 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:00:36.153965: step 92380, loss = 1.46 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:00:46.567566: step 92390, loss = 1.46 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:00:57.337467: step 92400, loss = 1.61 (118.8 examples/sec; 1.077 sec/batch)
2018-04-10 12:01:07.812821: step 92410, loss = 1.41 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 12:01:18.278216: step 92420, loss = 1.38 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:01:28.726092: step 92430, loss = 1.57 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:01:39.150666: step 92440, loss = 1.30 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:01:49.591042: step 92450, loss = 1.68 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:02:00.004293: step 92460, loss = 1.77 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:02:10.407219: step 92470, loss = 1.53 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:02:20.812510: step 92480, loss = 1.61 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:02:31.201956: step 92490, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:02:41.857290: step 92500, loss = 1.23 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 12:02:52.200641: step 92510, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 12:03:02.593587: step 92520, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:03:12.998495: step 92530, loss = 1.52 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:03:23.375299: step 92540, loss = 1.46 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 12:03:33.774256: step 92550, loss = 1.46 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:03:44.173647: step 92560, loss = 1.44 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:03:54.544912: step 92570, loss = 1.39 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:04:04.957774: step 92580, loss = 1.43 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:04:15.383003: step 92590, loss = 1.56 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:04:26.030771: step 92600, loss = 1.52 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 12:04:36.360961: step 92610, loss = 1.74 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 12:04:46.734928: step 92620, loss = 1.59 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:04:57.095985: step 92630, loss = 1.34 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 12:05:07.526695: step 92640, loss = 1.50 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:05:17.983549: step 92650, loss = 1.70 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:05:28.429713: step 92660, loss = 1.40 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:05:38.924361: step 92670, loss = 1.49 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:05:49.336953: step 92680, loss = 1.55 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:05:59.853124: step 92690, loss = 1.59 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 12:06:10.849240: step 92700, loss = 1.31 (116.4 examples/sec; 1.100 sec/batch)
2018-04-10 12:06:21.240524: step 92710, loss = 1.51 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:06:31.684240: step 92720, loss = 1.41 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:06:42.100642: step 92730, loss = 1.44 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:06:52.519375: step 92740, loss = 1.62 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:07:02.939931: step 92750, loss = 1.39 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:07:13.375151: step 92760, loss = 1.72 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 12:07:23.718187: step 92770, loss = 1.57 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 12:07:34.080733: step 92780, loss = 1.40 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 12:07:44.547369: step 92790, loss = 1.35 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:07:55.231920: step 92800, loss = 1.40 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 12:08:05.702623: step 92810, loss = 1.46 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 12:08:16.189150: step 92820, loss = 1.52 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 12:08:26.616822: step 92830, loss = 1.69 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:08:37.063586: step 92840, loss = 1.55 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:08:47.503115: step 92850, loss = 1.39 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:08:57.949858: step 92860, loss = 1.48 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:09:08.485302: step 92870, loss = 1.35 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 12:09:18.960636: step 92880, loss = 1.43 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 12:09:29.414565: step 92890, loss = 1.54 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 12:09:40.126226: step 92900, loss = 1.44 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 12:09:50.555957: step 92910, loss = 1.33 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:10:01.000755: step 92920, loss = 1.76 (122.5 examples/sec; 1.044 sec/batch)
2018-04-10 12:10:11.516687: step 92930, loss = 1.67 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 12:10:21.970042: step 92940, loss = 1.55 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 12:10:32.418067: step 92950, loss = 1.37 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:10:42.845246: step 92960, loss = 1.70 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:10:53.390821: step 92970, loss = 1.38 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 12:11:03.941246: step 92980, loss = 1.63 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 12:11:14.429688: step 92990, loss = 1.55 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:11:25.111300: step 93000, loss = 1.57 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 12:11:35.481125: step 93010, loss = 1.49 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:11:45.868036: step 93020, loss = 1.42 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:11:56.223836: step 93030, loss = 1.46 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 12:12:06.673848: step 93040, loss = 1.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:12:17.116823: step 93050, loss = 1.39 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:12:27.558261: step 93060, loss = 1.37 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:12:37.961689: step 93070, loss = 1.81 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:12:48.336423: step 93080, loss = 1.50 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:12:58.696778: step 93090, loss = 1.58 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 12:13:09.383980: step 93100, loss = 1.50 (119.8 examples/sec; 1.069 sec/batch)
2018-04-10 12:13:19.806169: step 93110, loss = 1.53 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:13:30.212273: step 93120, loss = 1.52 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:13:40.626488: step 93130, loss = 1.46 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:13:51.042083: step 93140, loss = 1.57 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:14:01.609726: step 93150, loss = 1.49 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 12:14:12.283409: step 93160, loss = 1.62 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 12:14:22.723653: step 93170, loss = 1.72 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:14:33.193005: step 93180, loss = 1.61 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:14:43.647847: step 93190, loss = 1.30 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 12:14:54.359230: step 93200, loss = 1.54 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 12:15:04.799372: step 93210, loss = 1.44 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:15:15.278497: step 93220, loss = 1.35 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:15:25.685959: step 93230, loss = 1.52 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:15:36.143171: step 93240, loss = 1.38 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:15:46.602636: step 93250, loss = 1.34 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:15:57.073544: step 93260, loss = 1.51 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 12:16:07.553510: step 93270, loss = 1.53 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:16:18.022470: step 93280, loss = 1.64 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:16:28.437162: step 93290, loss = 1.51 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:16:39.178813: step 93300, loss = 1.43 (119.2 examples/sec; 1.074 sec/batch)
2018-04-10 12:16:49.612595: step 93310, loss = 1.55 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:17:00.039671: step 93320, loss = 1.47 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:17:10.547142: step 93330, loss = 1.67 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 12:17:21.043008: step 93340, loss = 1.59 (122.0 examples/sec; 1.050 sec/batch)
2018-04-10 12:17:31.541466: step 93350, loss = 1.42 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 12:17:42.034884: step 93360, loss = 1.43 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:17:52.468531: step 93370, loss = 1.35 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:18:03.005895: step 93380, loss = 1.57 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 12:18:13.583260: step 93390, loss = 1.77 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 12:18:24.302725: step 93400, loss = 1.42 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 12:18:34.773025: step 93410, loss = 1.34 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:18:45.238795: step 93420, loss = 1.47 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:18:55.659436: step 93430, loss = 1.49 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:19:06.151429: step 93440, loss = 1.41 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:19:16.634763: step 93450, loss = 1.71 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:19:27.094463: step 93460, loss = 1.60 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:19:37.511909: step 93470, loss = 1.39 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:19:47.880639: step 93480, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:19:58.267449: step 93490, loss = 1.63 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:20:08.967815: step 93500, loss = 1.42 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 12:20:19.379478: step 93510, loss = 1.57 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:20:29.781572: step 93520, loss = 1.59 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:20:40.172691: step 93530, loss = 1.46 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:20:50.681414: step 93540, loss = 1.52 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 12:21:01.062003: step 93550, loss = 1.63 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:21:11.541668: step 93560, loss = 1.62 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:21:21.934277: step 93570, loss = 1.54 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:21:32.308694: step 93580, loss = 1.52 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:21:42.692592: step 93590, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:21:53.327606: step 93600, loss = 1.33 (120.4 examples/sec; 1.064 sec/batch)
2018-04-10 12:22:03.721418: step 93610, loss = 1.62 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:22:14.147208: step 93620, loss = 1.30 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:22:24.515409: step 93630, loss = 1.53 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 12:22:34.898434: step 93640, loss = 1.51 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:22:45.284288: step 93650, loss = 1.54 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:22:55.685137: step 93660, loss = 1.30 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:23:06.140616: step 93670, loss = 1.60 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:23:16.613962: step 93680, loss = 1.47 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 12:23:27.048864: step 93690, loss = 1.47 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:23:37.715538: step 93700, loss = 1.53 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 12:23:48.153107: step 93710, loss = 1.62 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:23:58.584260: step 93720, loss = 1.41 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:24:09.072719: step 93730, loss = 1.59 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:24:19.526618: step 93740, loss = 1.38 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 12:24:29.937978: step 93750, loss = 1.29 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:24:40.342512: step 93760, loss = 1.43 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:24:50.721060: step 93770, loss = 1.40 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:25:01.136302: step 93780, loss = 1.50 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:25:11.599932: step 93790, loss = 1.50 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 12:25:22.281461: step 93800, loss = 1.56 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 12:25:32.677193: step 93810, loss = 1.66 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:25:43.191283: step 93820, loss = 1.65 (121.7 examples/sec; 1.051 sec/batch)
2018-04-10 12:25:53.638115: step 93830, loss = 1.56 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:26:04.161633: step 93840, loss = 1.54 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 12:26:14.673624: step 93850, loss = 1.40 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 12:26:25.094090: step 93860, loss = 1.55 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:26:35.519587: step 93870, loss = 1.40 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:26:45.961255: step 93880, loss = 1.61 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:26:56.379875: step 93890, loss = 1.64 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:27:07.108989: step 93900, loss = 1.52 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 12:27:17.561944: step 93910, loss = 1.62 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:27:28.005543: step 93920, loss = 1.51 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:27:38.404331: step 93930, loss = 1.56 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:27:48.850972: step 93940, loss = 1.52 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:27:59.276954: step 93950, loss = 1.47 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:28:09.743959: step 93960, loss = 1.36 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:28:20.240839: step 93970, loss = 1.58 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 12:28:30.683730: step 93980, loss = 1.63 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:28:41.095367: step 93990, loss = 1.62 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:28:51.766464: step 94000, loss = 1.24 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 12:29:02.216626: step 94010, loss = 1.56 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:29:12.658098: step 94020, loss = 1.46 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:29:23.036452: step 94030, loss = 1.47 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:29:33.430845: step 94040, loss = 1.68 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 12:29:43.821127: step 94050, loss = 1.61 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:29:54.222311: step 94060, loss = 1.29 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:30:04.691806: step 94070, loss = 1.41 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:30:15.138406: step 94080, loss = 1.63 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:30:25.528832: step 94090, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:30:36.202254: step 94100, loss = 1.63 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 12:30:46.645235: step 94110, loss = 1.59 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:30:57.167899: step 94120, loss = 1.60 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 12:31:07.646418: step 94130, loss = 1.51 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 12:31:18.136218: step 94140, loss = 1.60 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:31:28.603912: step 94150, loss = 1.56 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:31:39.024907: step 94160, loss = 1.43 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:31:49.454716: step 94170, loss = 1.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:31:59.911090: step 94180, loss = 1.68 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:32:10.403660: step 94190, loss = 1.46 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:32:21.195700: step 94200, loss = 1.55 (118.6 examples/sec; 1.079 sec/batch)
2018-04-10 12:32:31.626052: step 94210, loss = 1.41 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:32:42.091682: step 94220, loss = 1.48 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:32:52.517633: step 94230, loss = 1.65 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:33:03.005566: step 94240, loss = 1.62 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 12:33:13.509899: step 94250, loss = 1.23 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 12:33:24.017118: step 94260, loss = 1.40 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 12:33:34.492884: step 94270, loss = 1.39 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 12:33:44.941626: step 94280, loss = 1.50 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:33:55.404451: step 94290, loss = 1.41 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 12:34:06.200950: step 94300, loss = 1.60 (118.6 examples/sec; 1.080 sec/batch)
2018-04-10 12:34:16.696874: step 94310, loss = 1.30 (122.0 examples/sec; 1.050 sec/batch)
2018-04-10 12:34:27.144986: step 94320, loss = 1.55 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:34:37.549692: step 94330, loss = 1.45 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:34:47.978556: step 94340, loss = 1.29 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:34:58.421651: step 94350, loss = 1.34 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:35:08.940230: step 94360, loss = 1.78 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 12:35:19.448434: step 94370, loss = 1.48 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 12:35:29.908497: step 94380, loss = 1.48 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:35:40.347747: step 94390, loss = 1.46 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:35:51.055784: step 94400, loss = 1.45 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 12:36:01.490013: step 94410, loss = 1.44 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:36:12.010846: step 94420, loss = 1.63 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 12:36:22.451519: step 94430, loss = 1.39 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:36:32.890462: step 94440, loss = 1.46 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:36:43.310280: step 94450, loss = 1.74 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:36:53.736782: step 94460, loss = 1.25 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:37:04.201194: step 94470, loss = 1.36 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 12:37:14.683927: step 94480, loss = 1.42 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:37:25.118596: step 94490, loss = 1.66 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:37:35.853489: step 94500, loss = 1.62 (119.2 examples/sec; 1.073 sec/batch)
2018-04-10 12:37:46.282328: step 94510, loss = 1.60 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:37:56.733261: step 94520, loss = 1.65 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:38:07.217930: step 94530, loss = 1.40 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:38:17.704435: step 94540, loss = 1.63 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 12:38:28.182399: step 94550, loss = 1.62 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 12:38:38.669442: step 94560, loss = 1.52 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 12:38:49.093634: step 94570, loss = 1.65 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:38:59.526183: step 94580, loss = 1.55 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:39:10.013417: step 94590, loss = 1.41 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 12:39:20.864755: step 94600, loss = 1.40 (118.0 examples/sec; 1.085 sec/batch)
2018-04-10 12:39:31.380930: step 94610, loss = 1.57 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 12:39:41.929148: step 94620, loss = 1.64 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 12:39:52.453665: step 94630, loss = 1.64 (121.6 examples/sec; 1.052 sec/batch)
2018-04-10 12:40:03.020686: step 94640, loss = 1.43 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 12:40:13.602077: step 94650, loss = 1.53 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 12:40:24.035232: step 94660, loss = 1.39 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:40:34.469964: step 94670, loss = 1.57 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:40:44.928815: step 94680, loss = 1.36 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:40:55.414226: step 94690, loss = 1.59 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 12:41:06.152631: step 94700, loss = 1.61 (119.2 examples/sec; 1.074 sec/batch)
2018-04-10 12:41:16.573579: step 94710, loss = 1.43 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:41:27.055331: step 94720, loss = 1.35 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:41:37.482002: step 94730, loss = 1.62 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:41:47.911562: step 94740, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:41:58.321307: step 94750, loss = 1.56 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:42:08.793532: step 94760, loss = 1.43 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 12:42:19.253163: step 94770, loss = 1.59 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:42:29.674259: step 94780, loss = 1.51 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:42:40.060766: step 94790, loss = 1.53 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:42:50.760271: step 94800, loss = 1.71 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 12:43:01.110843: step 94810, loss = 1.57 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 12:43:11.540042: step 94820, loss = 1.26 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:43:21.925237: step 94830, loss = 1.43 (123.3 examples/sec; 1.039 sec/batch)
2018-04-10 12:43:32.316113: step 94840, loss = 1.29 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:43:42.682905: step 94850, loss = 1.45 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 12:43:53.055991: step 94860, loss = 1.51 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:44:03.468979: step 94870, loss = 1.43 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:44:13.873274: step 94880, loss = 1.44 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:44:24.272367: step 94890, loss = 1.41 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:44:34.942738: step 94900, loss = 1.66 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 12:44:45.299710: step 94910, loss = 1.31 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 12:44:55.698817: step 94920, loss = 1.53 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:45:06.081369: step 94930, loss = 1.36 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:45:16.515581: step 94940, loss = 1.49 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:45:26.943864: step 94950, loss = 1.41 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:45:37.321958: step 94960, loss = 1.54 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:45:47.824606: step 94970, loss = 1.46 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 12:45:58.271708: step 94980, loss = 1.57 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 12:46:08.694734: step 94990, loss = 1.54 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:46:19.359759: step 95000, loss = 1.61 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 12:46:29.721124: step 95010, loss = 1.69 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 12:46:40.102678: step 95020, loss = 1.64 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:46:50.467089: step 95030, loss = 1.39 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 12:47:00.857175: step 95040, loss = 1.59 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:47:11.323542: step 95050, loss = 1.47 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:47:21.807249: step 95060, loss = 1.56 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 12:47:32.275537: step 95070, loss = 1.62 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 12:47:42.846147: step 95080, loss = 1.35 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 12:47:53.366094: step 95090, loss = 1.46 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 12:48:04.173298: step 95100, loss = 1.49 (118.4 examples/sec; 1.081 sec/batch)
2018-04-10 12:48:14.760456: step 95110, loss = 1.57 (120.9 examples/sec; 1.059 sec/batch)
2018-04-10 12:48:25.307532: step 95120, loss = 1.39 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 12:48:35.884190: step 95130, loss = 1.59 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 12:48:46.460071: step 95140, loss = 1.59 (121.0 examples/sec; 1.058 sec/batch)
2018-04-10 12:48:56.990994: step 95150, loss = 1.44 (121.5 examples/sec; 1.053 sec/batch)
2018-04-10 12:49:07.558665: step 95160, loss = 1.48 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 12:49:18.123842: step 95170, loss = 1.48 (121.2 examples/sec; 1.057 sec/batch)
2018-04-10 12:49:28.662966: step 95180, loss = 1.47 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 12:49:39.219242: step 95190, loss = 1.62 (121.3 examples/sec; 1.056 sec/batch)
2018-04-10 12:49:50.035213: step 95200, loss = 1.50 (118.3 examples/sec; 1.082 sec/batch)
2018-04-10 12:50:00.576511: step 95210, loss = 1.61 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 12:50:11.176018: step 95220, loss = 1.60 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 12:50:21.608484: step 95230, loss = 1.56 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:50:32.072242: step 95240, loss = 1.43 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 12:50:42.527856: step 95250, loss = 1.41 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:50:53.072346: step 95260, loss = 1.57 (121.4 examples/sec; 1.054 sec/batch)
2018-04-10 12:51:03.799559: step 95270, loss = 1.50 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 12:51:14.525066: step 95280, loss = 1.47 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 12:51:24.951340: step 95290, loss = 1.58 (122.8 examples/sec; 1.043 sec/batch)
2018-04-10 12:51:35.646980: step 95300, loss = 1.44 (119.7 examples/sec; 1.070 sec/batch)
2018-04-10 12:51:46.090675: step 95310, loss = 1.40 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:51:56.474698: step 95320, loss = 1.25 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:52:06.898868: step 95330, loss = 1.46 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:52:17.297916: step 95340, loss = 1.53 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:52:27.720880: step 95350, loss = 1.57 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:52:38.098746: step 95360, loss = 1.54 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:52:48.510023: step 95370, loss = 1.55 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:52:58.887928: step 95380, loss = 1.50 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:53:09.294439: step 95390, loss = 1.43 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:53:20.019605: step 95400, loss = 1.55 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 12:53:30.439286: step 95410, loss = 1.51 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:53:40.848777: step 95420, loss = 1.79 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:53:51.262243: step 95430, loss = 1.56 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:54:01.685371: step 95440, loss = 1.48 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 12:54:12.127693: step 95450, loss = 1.57 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 12:54:22.536646: step 95460, loss = 1.44 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:54:32.990496: step 95470, loss = 1.55 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 12:54:43.399275: step 95480, loss = 1.40 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:54:53.804188: step 95490, loss = 1.58 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 12:55:04.480285: step 95500, loss = 1.66 (119.9 examples/sec; 1.068 sec/batch)
2018-04-10 12:55:14.874192: step 95510, loss = 1.51 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 12:55:25.257441: step 95520, loss = 1.55 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:55:35.673911: step 95530, loss = 1.32 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:55:46.067624: step 95540, loss = 1.36 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:55:56.582014: step 95550, loss = 1.53 (121.7 examples/sec; 1.051 sec/batch)
2018-04-10 12:56:07.042694: step 95560, loss = 1.73 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 12:56:17.461891: step 95570, loss = 1.46 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:56:27.845369: step 95580, loss = 1.39 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:56:38.208519: step 95590, loss = 1.54 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 12:56:48.882137: step 95600, loss = 1.44 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 12:56:59.278202: step 95610, loss = 1.61 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:57:09.709562: step 95620, loss = 1.50 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:57:20.117778: step 95630, loss = 1.44 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 12:57:30.514440: step 95640, loss = 1.69 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:57:40.886614: step 95650, loss = 1.40 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:57:51.297622: step 95660, loss = 1.47 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:58:01.729751: step 95670, loss = 1.59 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 12:58:12.144516: step 95680, loss = 1.60 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 12:58:22.523418: step 95690, loss = 1.47 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:58:33.808519: step 95700, loss = 1.59 (113.4 examples/sec; 1.129 sec/batch)
2018-04-10 12:58:44.184697: step 95710, loss = 1.39 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 12:58:54.574879: step 95720, loss = 1.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 12:59:04.975641: step 95730, loss = 1.60 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 12:59:15.392923: step 95740, loss = 1.59 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 12:59:25.766492: step 95750, loss = 1.35 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 12:59:36.144031: step 95760, loss = 1.69 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:59:46.528373: step 95770, loss = 1.39 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 12:59:56.904630: step 95780, loss = 1.32 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 13:00:07.301518: step 95790, loss = 1.44 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:00:17.996895: step 95800, loss = 1.53 (119.7 examples/sec; 1.070 sec/batch)
2018-04-10 13:00:28.481839: step 95810, loss = 1.52 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 13:00:38.926160: step 95820, loss = 1.42 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:00:49.307042: step 95830, loss = 1.42 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:00:59.797668: step 95840, loss = 1.49 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:01:10.208216: step 95850, loss = 1.56 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:01:20.624464: step 95860, loss = 1.51 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:01:30.990293: step 95870, loss = 1.31 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:01:41.343297: step 95880, loss = 1.42 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 13:01:51.703889: step 95890, loss = 1.50 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:02:02.387930: step 95900, loss = 1.52 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 13:02:12.795553: step 95910, loss = 1.44 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:02:23.153186: step 95920, loss = 1.57 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:02:33.529327: step 95930, loss = 1.50 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 13:02:43.917360: step 95940, loss = 1.41 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:02:54.301042: step 95950, loss = 1.38 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:03:04.705977: step 95960, loss = 1.45 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 13:03:15.105774: step 95970, loss = 1.55 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:03:25.464625: step 95980, loss = 1.77 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:03:35.825590: step 95990, loss = 1.72 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:03:46.462265: step 96000, loss = 1.54 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 13:03:56.848189: step 96010, loss = 1.41 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:04:07.262042: step 96020, loss = 1.59 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:04:17.684134: step 96030, loss = 1.34 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:04:28.062493: step 96040, loss = 1.38 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:04:38.436881: step 96050, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:04:48.890290: step 96060, loss = 1.37 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 13:04:59.311665: step 96070, loss = 1.46 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:05:09.740869: step 96080, loss = 1.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:05:20.141703: step 96090, loss = 1.45 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:05:30.801031: step 96100, loss = 1.84 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 13:05:41.240136: step 96110, loss = 1.39 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:05:51.657315: step 96120, loss = 1.58 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:06:02.096600: step 96130, loss = 1.71 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:06:12.539062: step 96140, loss = 1.64 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:06:22.942899: step 96150, loss = 1.32 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 13:06:33.356976: step 96160, loss = 1.45 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:06:43.735058: step 96170, loss = 1.60 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:06:54.098198: step 96180, loss = 1.42 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:07:04.506781: step 96190, loss = 1.56 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:07:15.205057: step 96200, loss = 1.31 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 13:07:25.567798: step 96210, loss = 1.58 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:07:35.936325: step 96220, loss = 1.36 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:07:46.320569: step 96230, loss = 1.47 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:07:56.696296: step 96240, loss = 1.52 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 13:08:07.146190: step 96250, loss = 1.38 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:08:17.562563: step 96260, loss = 1.56 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:08:27.943138: step 96270, loss = 1.45 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:08:38.307851: step 96280, loss = 1.44 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:08:48.679894: step 96290, loss = 1.70 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:08:59.341065: step 96300, loss = 1.56 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 13:09:09.791861: step 96310, loss = 1.51 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:09:20.188496: step 96320, loss = 1.27 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:09:30.589737: step 96330, loss = 1.40 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:09:40.977200: step 96340, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:09:51.354029: step 96350, loss = 1.49 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 13:10:01.784929: step 96360, loss = 1.54 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:10:12.188074: step 96370, loss = 1.55 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 13:10:22.581629: step 96380, loss = 1.39 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:10:32.959669: step 96390, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:10:43.616162: step 96400, loss = 1.64 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 13:10:54.130901: step 96410, loss = 1.62 (121.7 examples/sec; 1.051 sec/batch)
2018-04-10 13:11:04.608798: step 96420, loss = 1.49 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 13:11:15.010587: step 96430, loss = 1.43 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:11:25.382769: step 96440, loss = 1.65 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:11:35.770808: step 96450, loss = 1.52 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:11:46.154979: step 96460, loss = 1.32 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:11:56.509544: step 96470, loss = 1.24 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 13:12:06.926849: step 96480, loss = 1.58 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:12:17.340361: step 96490, loss = 1.51 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:12:27.969242: step 96500, loss = 1.58 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 13:12:38.326045: step 96510, loss = 1.56 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:12:48.689408: step 96520, loss = 1.41 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:12:59.061785: step 96530, loss = 1.46 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:13:09.476206: step 96540, loss = 1.59 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:13:19.875790: step 96550, loss = 1.59 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:13:30.224973: step 96560, loss = 1.77 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:13:40.575241: step 96570, loss = 1.50 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:13:50.940555: step 96580, loss = 1.50 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:14:01.305791: step 96590, loss = 1.55 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:14:11.993020: step 96600, loss = 1.56 (119.8 examples/sec; 1.069 sec/batch)
2018-04-10 13:14:22.383613: step 96610, loss = 1.59 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:14:32.752103: step 96620, loss = 1.25 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:14:43.153202: step 96630, loss = 1.60 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:14:53.572403: step 96640, loss = 1.55 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:15:04.019041: step 96650, loss = 1.51 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:15:14.462080: step 96660, loss = 1.24 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:15:24.868989: step 96670, loss = 1.77 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:15:35.308553: step 96680, loss = 1.64 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:15:45.741490: step 96690, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:15:56.400565: step 96700, loss = 1.54 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 13:16:06.855082: step 96710, loss = 1.51 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 13:16:17.298523: step 96720, loss = 1.36 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:16:27.664490: step 96730, loss = 1.62 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:16:38.016719: step 96740, loss = 1.62 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 13:16:48.380441: step 96750, loss = 1.60 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:16:58.759053: step 96760, loss = 1.41 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:17:09.191971: step 96770, loss = 1.39 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:17:19.597843: step 96780, loss = 1.46 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:17:29.976435: step 96790, loss = 1.57 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:17:40.806039: step 96800, loss = 1.61 (118.2 examples/sec; 1.083 sec/batch)
2018-04-10 13:17:51.227882: step 96810, loss = 1.45 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:18:01.657316: step 96820, loss = 1.44 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:18:12.111372: step 96830, loss = 1.60 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 13:18:22.601088: step 96840, loss = 1.50 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:18:33.007750: step 96850, loss = 1.43 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:18:43.495975: step 96860, loss = 1.54 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:18:54.026402: step 96870, loss = 1.43 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 13:19:04.582765: step 96880, loss = 1.25 (121.3 examples/sec; 1.056 sec/batch)
2018-04-10 13:19:15.128193: step 96890, loss = 1.38 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 13:19:25.942045: step 96900, loss = 1.40 (118.4 examples/sec; 1.081 sec/batch)
2018-04-10 13:19:36.471803: step 96910, loss = 1.37 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 13:19:46.972202: step 96920, loss = 1.80 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 13:19:57.458536: step 96930, loss = 1.46 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 13:20:07.936193: step 96940, loss = 1.53 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 13:20:18.355781: step 96950, loss = 1.66 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:20:28.802265: step 96960, loss = 1.54 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:20:39.241963: step 96970, loss = 1.47 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:20:49.629376: step 96980, loss = 1.56 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:21:00.225115: step 96990, loss = 1.51 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 13:21:10.991222: step 97000, loss = 1.61 (118.9 examples/sec; 1.077 sec/batch)
2018-04-10 13:21:21.421970: step 97010, loss = 1.48 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:21:31.862515: step 97020, loss = 1.48 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:21:42.295677: step 97030, loss = 1.39 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:21:52.725327: step 97040, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:22:03.178110: step 97050, loss = 1.65 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:22:13.665663: step 97060, loss = 1.40 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:22:24.123256: step 97070, loss = 1.51 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:22:34.568620: step 97080, loss = 1.54 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:22:44.987982: step 97090, loss = 1.38 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:22:55.652385: step 97100, loss = 1.25 (120.0 examples/sec; 1.066 sec/batch)
2018-04-10 13:23:06.091409: step 97110, loss = 1.53 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:23:16.534262: step 97120, loss = 1.54 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:23:26.939096: step 97130, loss = 1.36 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 13:23:37.392783: step 97140, loss = 1.54 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 13:23:47.826438: step 97150, loss = 1.44 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:23:58.260564: step 97160, loss = 1.31 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:24:08.723664: step 97170, loss = 1.47 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 13:24:19.219682: step 97180, loss = 1.56 (122.0 examples/sec; 1.050 sec/batch)
2018-04-10 13:24:29.608106: step 97190, loss = 1.49 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:24:40.333408: step 97200, loss = 1.65 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 13:24:50.741201: step 97210, loss = 1.53 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:25:01.173219: step 97220, loss = 1.59 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:25:11.592403: step 97230, loss = 1.41 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:25:22.002390: step 97240, loss = 1.66 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:25:32.524147: step 97250, loss = 1.55 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 13:25:43.077609: step 97260, loss = 1.29 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 13:25:53.637158: step 97270, loss = 1.52 (121.2 examples/sec; 1.056 sec/batch)
2018-04-10 13:26:04.120455: step 97280, loss = 1.70 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 13:26:14.550680: step 97290, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:26:25.211051: step 97300, loss = 1.46 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 13:26:35.592228: step 97310, loss = 1.37 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:26:46.022592: step 97320, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:26:56.479476: step 97330, loss = 1.46 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:27:06.923412: step 97340, loss = 1.32 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:27:17.353234: step 97350, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:27:27.754655: step 97360, loss = 1.62 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:27:38.158306: step 97370, loss = 1.43 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 13:27:48.606288: step 97380, loss = 1.45 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:27:59.019557: step 97390, loss = 1.54 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:28:09.765515: step 97400, loss = 1.49 (119.1 examples/sec; 1.075 sec/batch)
2018-04-10 13:28:20.200168: step 97410, loss = 1.42 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:28:30.610674: step 97420, loss = 1.47 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:28:41.019435: step 97430, loss = 1.60 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:28:51.481540: step 97440, loss = 1.63 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 13:29:01.959553: step 97450, loss = 1.51 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 13:29:12.447792: step 97460, loss = 1.41 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:29:22.858763: step 97470, loss = 1.53 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:29:33.257887: step 97480, loss = 1.59 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:29:43.653055: step 97490, loss = 1.51 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:29:54.331735: step 97500, loss = 1.36 (119.9 examples/sec; 1.068 sec/batch)
2018-04-10 13:30:04.765478: step 97510, loss = 1.48 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:30:15.198987: step 97520, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:30:25.559596: step 97530, loss = 1.39 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:30:35.930679: step 97540, loss = 1.56 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:30:46.323976: step 97550, loss = 1.40 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:30:56.803407: step 97560, loss = 1.66 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 13:31:07.217891: step 97570, loss = 1.27 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:31:17.641192: step 97580, loss = 1.60 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:31:28.071361: step 97590, loss = 1.61 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:31:38.679013: step 97600, loss = 1.38 (120.7 examples/sec; 1.061 sec/batch)
2018-04-10 13:31:49.035496: step 97610, loss = 1.53 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:31:59.403759: step 97620, loss = 1.47 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:32:09.776348: step 97630, loss = 1.39 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:32:20.147690: step 97640, loss = 1.63 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:32:30.524390: step 97650, loss = 1.37 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 13:32:40.900895: step 97660, loss = 1.53 (123.4 examples/sec; 1.038 sec/batch)
2018-04-10 13:32:51.259020: step 97670, loss = 1.64 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:33:01.621518: step 97680, loss = 1.64 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:33:12.009239: step 97690, loss = 1.28 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:33:22.657772: step 97700, loss = 1.41 (120.2 examples/sec; 1.065 sec/batch)
2018-04-10 13:33:33.007450: step 97710, loss = 1.55 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:33:43.419855: step 97720, loss = 1.47 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:33:53.811028: step 97730, loss = 1.66 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:34:04.183961: step 97740, loss = 1.66 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:34:14.575278: step 97750, loss = 1.52 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:34:24.944060: step 97760, loss = 1.60 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:34:35.313554: step 97770, loss = 1.61 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:34:45.726649: step 97780, loss = 1.44 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:34:56.077263: step 97790, loss = 1.56 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:35:06.750810: step 97800, loss = 1.61 (119.9 examples/sec; 1.067 sec/batch)
2018-04-10 13:35:17.179921: step 97810, loss = 1.50 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:35:27.520176: step 97820, loss = 1.37 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:35:37.866247: step 97830, loss = 1.61 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:35:48.253229: step 97840, loss = 1.48 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:35:58.626882: step 97850, loss = 1.50 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:36:09.177814: step 97860, loss = 1.64 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 13:36:19.775548: step 97870, loss = 1.46 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 13:36:30.269219: step 97880, loss = 1.64 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:36:40.737840: step 97890, loss = 1.66 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 13:36:51.493059: step 97900, loss = 1.33 (119.0 examples/sec; 1.076 sec/batch)
2018-04-10 13:37:01.978126: step 97910, loss = 1.49 (122.1 examples/sec; 1.049 sec/batch)
2018-04-10 13:37:12.506256: step 97920, loss = 1.41 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 13:37:22.986003: step 97930, loss = 1.47 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 13:37:33.473681: step 97940, loss = 1.35 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:37:43.953378: step 97950, loss = 1.51 (122.1 examples/sec; 1.048 sec/batch)
2018-04-10 13:37:54.456767: step 97960, loss = 1.68 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 13:38:04.975798: step 97970, loss = 1.54 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 13:38:15.494709: step 97980, loss = 1.45 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 13:38:25.959320: step 97990, loss = 1.48 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 13:38:36.764518: step 98000, loss = 1.38 (118.5 examples/sec; 1.081 sec/batch)
2018-04-10 13:38:47.230442: step 98010, loss = 1.67 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 13:38:57.705675: step 98020, loss = 1.51 (122.2 examples/sec; 1.048 sec/batch)
2018-04-10 13:39:08.252935: step 98030, loss = 1.42 (121.4 examples/sec; 1.055 sec/batch)
2018-04-10 13:39:18.758381: step 98040, loss = 1.50 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 13:39:29.138294: step 98050, loss = 1.63 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:39:39.516021: step 98060, loss = 1.68 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:39:49.887514: step 98070, loss = 1.45 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:40:00.226606: step 98080, loss = 1.53 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:40:10.615173: step 98090, loss = 1.40 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:40:21.329465: step 98100, loss = 1.46 (119.5 examples/sec; 1.071 sec/batch)
2018-04-10 13:40:31.712796: step 98110, loss = 1.65 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:40:42.084993: step 98120, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:40:52.442497: step 98130, loss = 1.55 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:41:02.956933: step 98140, loss = 1.66 (121.7 examples/sec; 1.051 sec/batch)
2018-04-10 13:41:13.413415: step 98150, loss = 1.59 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:41:23.786435: step 98160, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:41:34.124591: step 98170, loss = 1.46 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:41:44.487402: step 98180, loss = 1.46 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:41:54.834671: step 98190, loss = 1.66 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:42:05.473963: step 98200, loss = 1.52 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 13:42:15.826766: step 98210, loss = 1.52 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 13:42:26.168279: step 98220, loss = 1.21 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:42:36.510780: step 98230, loss = 1.72 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:42:46.861004: step 98240, loss = 1.75 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:42:57.238056: step 98250, loss = 1.17 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:43:07.703494: step 98260, loss = 1.51 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 13:43:18.114343: step 98270, loss = 1.48 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:43:28.486883: step 98280, loss = 1.33 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:43:38.867166: step 98290, loss = 1.40 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:43:49.482310: step 98300, loss = 1.47 (120.6 examples/sec; 1.062 sec/batch)
2018-04-10 13:43:59.829974: step 98310, loss = 1.32 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:44:10.197144: step 98320, loss = 1.54 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:44:20.606121: step 98330, loss = 1.52 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:44:30.953291: step 98340, loss = 1.49 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 13:44:41.345105: step 98350, loss = 1.53 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:44:51.736702: step 98360, loss = 1.61 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:45:02.104384: step 98370, loss = 1.50 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:45:12.483540: step 98380, loss = 1.44 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:45:22.826166: step 98390, loss = 1.52 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:45:33.463747: step 98400, loss = 1.24 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 13:45:43.831356: step 98410, loss = 1.60 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 13:45:54.225690: step 98420, loss = 1.69 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 13:46:04.639451: step 98430, loss = 1.39 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:46:15.091009: step 98440, loss = 1.34 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:46:25.491716: step 98450, loss = 1.58 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:46:35.925366: step 98460, loss = 1.44 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:46:46.313634: step 98470, loss = 1.39 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:46:56.683490: step 98480, loss = 1.53 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 13:47:07.094414: step 98490, loss = 1.45 (122.9 examples/sec; 1.041 sec/batch)
2018-04-10 13:47:17.756456: step 98500, loss = 1.40 (120.1 examples/sec; 1.066 sec/batch)
2018-04-10 13:47:28.108441: step 98510, loss = 1.46 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 13:47:38.495884: step 98520, loss = 1.47 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:47:48.884882: step 98530, loss = 1.25 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:47:59.243932: step 98540, loss = 1.28 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:48:09.692276: step 98550, loss = 1.59 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:48:20.102330: step 98560, loss = 1.57 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:48:30.495296: step 98570, loss = 1.50 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:48:40.872406: step 98580, loss = 1.40 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:48:51.272050: step 98590, loss = 1.69 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:49:02.053715: step 98600, loss = 1.55 (118.7 examples/sec; 1.078 sec/batch)
2018-04-10 13:49:12.468848: step 98610, loss = 1.65 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:49:22.826830: step 98620, loss = 1.48 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:49:33.163476: step 98630, loss = 1.54 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 13:49:43.555567: step 98640, loss = 1.51 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:49:53.908079: step 98650, loss = 1.47 (123.6 examples/sec; 1.035 sec/batch)
2018-04-10 13:50:04.314327: step 98660, loss = 1.53 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 13:50:14.746795: step 98670, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:50:25.212921: step 98680, loss = 1.33 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 13:50:35.670783: step 98690, loss = 1.56 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:50:46.404318: step 98700, loss = 1.75 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 13:50:56.955672: step 98710, loss = 1.48 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 13:51:07.512004: step 98720, loss = 1.36 (121.3 examples/sec; 1.056 sec/batch)
2018-04-10 13:51:18.033897: step 98730, loss = 1.64 (121.7 examples/sec; 1.052 sec/batch)
2018-04-10 13:51:28.538601: step 98740, loss = 1.48 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 13:51:38.999135: step 98750, loss = 1.36 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:51:49.490025: step 98760, loss = 1.64 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:51:59.874207: step 98770, loss = 1.59 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 13:52:10.296417: step 98780, loss = 1.34 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 13:52:20.654580: step 98790, loss = 1.40 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:52:31.252915: step 98800, loss = 1.77 (120.8 examples/sec; 1.060 sec/batch)
2018-04-10 13:52:41.586413: step 98810, loss = 1.57 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 13:52:51.947496: step 98820, loss = 1.51 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 13:53:02.305863: step 98830, loss = 1.61 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 13:53:12.696140: step 98840, loss = 1.56 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:53:23.093893: step 98850, loss = 1.48 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:53:33.509565: step 98860, loss = 1.54 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:53:43.901701: step 98870, loss = 1.28 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:53:54.318007: step 98880, loss = 1.55 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 13:54:04.790754: step 98890, loss = 1.49 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 13:54:15.517324: step 98900, loss = 1.81 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 13:54:25.909916: step 98910, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:54:36.313918: step 98920, loss = 1.50 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 13:54:46.705920: step 98930, loss = 1.37 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 13:54:57.102332: step 98940, loss = 1.39 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 13:55:07.532398: step 98950, loss = 1.58 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:55:18.001115: step 98960, loss = 1.61 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 13:55:28.465485: step 98970, loss = 1.41 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 13:55:38.924684: step 98980, loss = 1.47 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:55:49.374731: step 98990, loss = 1.42 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 13:56:00.105087: step 99000, loss = 1.40 (119.3 examples/sec; 1.073 sec/batch)
2018-04-10 13:56:10.653279: step 99010, loss = 1.34 (121.3 examples/sec; 1.055 sec/batch)
2018-04-10 13:56:21.146813: step 99020, loss = 1.42 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:56:31.647896: step 99030, loss = 1.44 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 13:56:42.077040: step 99040, loss = 1.60 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:56:52.518123: step 99050, loss = 1.50 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 13:57:03.010048: step 99060, loss = 1.57 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:57:13.510980: step 99070, loss = 1.24 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 13:57:23.982371: step 99080, loss = 1.77 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 13:57:34.435724: step 99090, loss = 1.67 (122.4 examples/sec; 1.045 sec/batch)
2018-04-10 13:57:45.138568: step 99100, loss = 1.69 (119.6 examples/sec; 1.070 sec/batch)
2018-04-10 13:57:55.570208: step 99110, loss = 1.44 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 13:58:06.081594: step 99120, loss = 1.43 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 13:58:16.580489: step 99130, loss = 1.43 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 13:58:27.045447: step 99140, loss = 1.57 (122.3 examples/sec; 1.046 sec/batch)
2018-04-10 13:58:37.556527: step 99150, loss = 1.52 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 13:58:48.084521: step 99160, loss = 1.38 (121.6 examples/sec; 1.053 sec/batch)
2018-04-10 13:58:58.578706: step 99170, loss = 1.32 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:59:09.148684: step 99180, loss = 1.50 (121.1 examples/sec; 1.057 sec/batch)
2018-04-10 13:59:19.639501: step 99190, loss = 1.36 (122.0 examples/sec; 1.049 sec/batch)
2018-04-10 13:59:30.359092: step 99200, loss = 1.66 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 13:59:40.817171: step 99210, loss = 1.34 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 13:59:51.268024: step 99220, loss = 1.54 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 14:00:01.737151: step 99230, loss = 1.68 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 14:00:12.272862: step 99240, loss = 1.41 (121.5 examples/sec; 1.054 sec/batch)
2018-04-10 14:00:22.746187: step 99250, loss = 1.40 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 14:00:33.245445: step 99260, loss = 1.42 (121.9 examples/sec; 1.050 sec/batch)
2018-04-10 14:00:43.705704: step 99270, loss = 1.79 (122.4 examples/sec; 1.046 sec/batch)
2018-04-10 14:00:54.428971: step 99280, loss = 1.39 (119.4 examples/sec; 1.072 sec/batch)
2018-04-10 14:01:04.847789: step 99290, loss = 1.63 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 14:01:15.609110: step 99300, loss = 1.52 (118.9 examples/sec; 1.076 sec/batch)
2018-04-10 14:01:26.080848: step 99310, loss = 1.43 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 14:01:36.554535: step 99320, loss = 1.69 (122.2 examples/sec; 1.047 sec/batch)
2018-04-10 14:01:46.990367: step 99330, loss = 1.63 (122.7 examples/sec; 1.044 sec/batch)
2018-04-10 14:01:57.425299: step 99340, loss = 1.52 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 14:02:07.932382: step 99350, loss = 1.60 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 14:02:18.439897: step 99360, loss = 1.55 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 14:02:28.847344: step 99370, loss = 1.55 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 14:02:39.242110: step 99380, loss = 1.38 (123.1 examples/sec; 1.039 sec/batch)
2018-04-10 14:02:49.610617: step 99390, loss = 1.34 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 14:03:00.250691: step 99400, loss = 1.49 (120.3 examples/sec; 1.064 sec/batch)
2018-04-10 14:03:10.694753: step 99410, loss = 1.51 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 14:03:21.122857: step 99420, loss = 1.48 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 14:03:31.522514: step 99430, loss = 1.63 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 14:03:41.915619: step 99440, loss = 1.39 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:03:52.309431: step 99450, loss = 1.55 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:04:02.742973: step 99460, loss = 1.47 (122.7 examples/sec; 1.043 sec/batch)
2018-04-10 14:04:13.192003: step 99470, loss = 1.49 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 14:04:23.636088: step 99480, loss = 1.60 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 14:04:34.024129: step 99490, loss = 1.58 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:04:44.689641: step 99500, loss = 1.39 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 14:04:55.071705: step 99510, loss = 1.56 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 14:05:05.496561: step 99520, loss = 1.46 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 14:05:15.892371: step 99530, loss = 1.41 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 14:05:26.285362: step 99540, loss = 1.49 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:05:36.670714: step 99550, loss = 1.38 (123.3 examples/sec; 1.039 sec/batch)
2018-04-10 14:05:47.064444: step 99560, loss = 1.49 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:05:57.443658: step 99570, loss = 1.39 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 14:06:07.913015: step 99580, loss = 1.52 (122.3 examples/sec; 1.047 sec/batch)
2018-04-10 14:06:18.312415: step 99590, loss = 1.65 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 14:06:28.995813: step 99600, loss = 1.59 (119.8 examples/sec; 1.068 sec/batch)
2018-04-10 14:06:39.398050: step 99610, loss = 1.72 (123.1 examples/sec; 1.040 sec/batch)
2018-04-10 14:06:49.763636: step 99620, loss = 1.40 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 14:07:00.114706: step 99630, loss = 1.62 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 14:07:10.537648: step 99640, loss = 1.57 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 14:07:20.947189: step 99650, loss = 1.60 (123.0 examples/sec; 1.041 sec/batch)
2018-04-10 14:07:31.387921: step 99660, loss = 1.55 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 14:07:41.791541: step 99670, loss = 1.35 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 14:07:52.208872: step 99680, loss = 1.46 (122.9 examples/sec; 1.042 sec/batch)
2018-04-10 14:08:02.654339: step 99690, loss = 1.61 (122.5 examples/sec; 1.045 sec/batch)
2018-04-10 14:08:13.324609: step 99700, loss = 1.49 (120.0 examples/sec; 1.067 sec/batch)
2018-04-10 14:08:23.679991: step 99710, loss = 1.76 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 14:08:34.062568: step 99720, loss = 1.49 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 14:08:44.407833: step 99730, loss = 1.52 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 14:08:54.774655: step 99740, loss = 1.32 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 14:09:05.212680: step 99750, loss = 1.42 (122.6 examples/sec; 1.044 sec/batch)
2018-04-10 14:09:15.615843: step 99760, loss = 1.56 (123.0 examples/sec; 1.040 sec/batch)
2018-04-10 14:09:25.964580: step 99770, loss = 1.76 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 14:09:36.323862: step 99780, loss = 1.65 (123.6 examples/sec; 1.036 sec/batch)
2018-04-10 14:09:46.693836: step 99790, loss = 1.49 (123.4 examples/sec; 1.037 sec/batch)
2018-04-10 14:09:57.387788: step 99800, loss = 1.62 (119.7 examples/sec; 1.069 sec/batch)
2018-04-10 14:10:07.779980: step 99810, loss = 1.60 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:10:18.159860: step 99820, loss = 1.38 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 14:10:28.524276: step 99830, loss = 1.39 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 14:10:38.886539: step 99840, loss = 1.48 (123.5 examples/sec; 1.036 sec/batch)
2018-04-10 14:10:49.264279: step 99850, loss = 1.58 (123.3 examples/sec; 1.038 sec/batch)
2018-04-10 14:10:59.773642: step 99860, loss = 1.51 (121.8 examples/sec; 1.051 sec/batch)
2018-04-10 14:11:10.195061: step 99870, loss = 1.35 (122.8 examples/sec; 1.042 sec/batch)
2018-04-10 14:11:20.587139: step 99880, loss = 1.53 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:11:30.918240: step 99890, loss = 1.71 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 14:11:41.546577: step 99900, loss = 1.59 (120.4 examples/sec; 1.063 sec/batch)
2018-04-10 14:11:51.843826: step 99910, loss = 1.32 (124.3 examples/sec; 1.030 sec/batch)
2018-04-10 14:12:02.190371: step 99920, loss = 1.59 (123.7 examples/sec; 1.035 sec/batch)
2018-04-10 14:12:12.513823: step 99930, loss = 1.69 (124.0 examples/sec; 1.032 sec/batch)
2018-04-10 14:12:22.850174: step 99940, loss = 1.43 (123.8 examples/sec; 1.034 sec/batch)
2018-04-10 14:12:33.215516: step 99950, loss = 1.76 (123.5 examples/sec; 1.037 sec/batch)
2018-04-10 14:12:43.558978: step 99960, loss = 1.51 (123.7 examples/sec; 1.034 sec/batch)
2018-04-10 14:12:53.886878: step 99970, loss = 1.32 (123.9 examples/sec; 1.033 sec/batch)
2018-04-10 14:13:04.279597: step 99980, loss = 1.66 (123.2 examples/sec; 1.039 sec/batch)
2018-04-10 14:13:14.630386: step 99990, loss = 1.53 (123.7 examples/sec; 1.035 sec/batch)
